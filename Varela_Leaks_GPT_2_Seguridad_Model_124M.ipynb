{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Varela Leaks - GPT-2 - Seguridad - Model 124M.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "xU7dzP04W5XZ",
        "colab_type": "code",
        "outputId": "d4d8b79c-b46c-48fe-a9fe-bf825ed91352",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "!git clone https://github.com/nshepperd/gpt-2.git\n",
        "!python gpt-2/download_model.py 355M\n",
        "!python gpt-2/download_model.py 124M"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\rFetching checkpoint:   0%|                                              | 0.00/77.0 [00:00<?, ?it/s]\rFetching checkpoint: 1.00kit [00:00, 852kit/s]                                                      \n",
            "\rFetching encoder.json:   0%|                                           | 0.00/1.04M [00:00<?, ?it/s]\rFetching encoder.json: 1.04Mit [00:00, 41.0Mit/s]                                                   \n",
            "Fetching hparams.json: 1.00kit [00:00, 873kit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001: 498Mit [00:17, 28.2Mit/s]                                  \n",
            "Fetching model.ckpt.index: 6.00kit [00:00, 3.17Mit/s]                                               \n",
            "Fetching model.ckpt.meta: 472kit [00:00, 30.7Mit/s]                                                 \n",
            "Fetching vocab.bpe: 457kit [00:00, 35.3Mit/s]                                                       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOaC3EHZXV4d",
        "colab_type": "code",
        "outputId": "eec9d31a-6395-459a-f17a-86ae15771b88",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 91
        }
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-f15546b4-e372-4162-89a3-e366666348b8\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-f15546b4-e372-4162-89a3-e366666348b8\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving _chat.txt to _chat (1).txt\n",
            "User uploaded file \"_chat.txt\" with length 1318534 bytes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ooS-afCEXXIc",
        "colab_type": "code",
        "outputId": "a0885c23-cfb3-4887-c1c5-a0c108004b2c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 629
        }
      },
      "source": [
        "!pip install -r gpt-2/requirements.txt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting fire>=0.1.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d9/69/faeaae8687f4de0f5973694d02e9d6c3eb827636a009157352d98de1129e/fire-0.2.1.tar.gz (76kB)\n",
            "\r\u001b[K     |████▎                           | 10kB 18.0MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 20kB 3.1MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 30kB 4.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 40kB 2.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 51kB 3.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 61kB 4.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 71kB 4.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 81kB 3.7MB/s \n",
            "\u001b[?25hCollecting regex==2017.4.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/62/c0c0d762ffd4ffaf39f372eb8561b8d491a11ace5a7884610424a8b40f95/regex-2017.04.05.tar.gz (601kB)\n",
            "\u001b[K     |████████████████████████████████| 604kB 14.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests==2.21.0 in /usr/local/lib/python3.6/dist-packages (from -r gpt-2/requirements.txt (line 3)) (2.21.0)\n",
            "Collecting tqdm==4.31.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6c/4b/c38b5144cf167c4f52288517436ccafefe9dc01b8d1c190e18a6b154cd4a/tqdm-4.31.1-py2.py3-none-any.whl (48kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 5.4MB/s \n",
            "\u001b[?25hCollecting toposort==1.5\n",
            "  Downloading https://files.pythonhosted.org/packages/e9/8a/321cd8ea5f4a22a06e3ba30ef31ec33bea11a3443eeb1d89807640ee6ed4/toposort-1.5-py2.py3-none-any.whl\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from fire>=0.1.3->-r gpt-2/requirements.txt (line 1)) (1.12.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from fire>=0.1.3->-r gpt-2/requirements.txt (line 1)) (1.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r gpt-2/requirements.txt (line 3)) (2019.9.11)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r gpt-2/requirements.txt (line 3)) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r gpt-2/requirements.txt (line 3)) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r gpt-2/requirements.txt (line 3)) (2.8)\n",
            "Building wheels for collected packages: fire, regex\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.2.1-py2.py3-none-any.whl size=103527 sha256=62d611b16c198d6f77e28718cc590e4c9eada6fd18255b384c107955e8a69892\n",
            "  Stored in directory: /root/.cache/pip/wheels/31/9c/c0/07b6dc7faf1844bb4688f46b569efe6cafaa2179c95db821da\n",
            "  Building wheel for regex (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for regex: filename=regex-2017.4.5-cp36-cp36m-linux_x86_64.whl size=533183 sha256=8ad1dba8c8aa5aa392efdf62e65c22505a58ca0ccc4e6e1ce6908ce9d8060742\n",
            "  Stored in directory: /root/.cache/pip/wheels/75/07/38/3c16b529d50cb4e0cd3dbc7b75cece8a09c132692c74450b01\n",
            "Successfully built fire regex\n",
            "Installing collected packages: fire, regex, tqdm, toposort\n",
            "  Found existing installation: tqdm 4.28.1\n",
            "    Uninstalling tqdm-4.28.1:\n",
            "      Successfully uninstalled tqdm-4.28.1\n",
            "Successfully installed fire-0.2.1 regex-2017.4.5 toposort-1.5 tqdm-4.31.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tqdm"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mlY2OiiuYDRD",
        "colab_type": "code",
        "outputId": "147c46ed-6135-492c-87ce-664d8d898250",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "!mv gpt-2/encode.py gpt-2/src/\n",
        "!python gpt-2/src/encode.py --model_name 124M _chat.txt _chat.npz\n",
        "#!python gpt-2/src/encode.py --model_name 355M _chat.txt _chat.npz"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading files\n",
            "100% 1/1 [00:03<00:00,  3.04s/it]\n",
            "Writing _chat.npz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBZpcvQmF65P",
        "colab_type": "code",
        "outputId": "5009eb5a-33d9-4756-cb2d-cc5420ac085f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!mv gpt-2/train.py gpt-2/src/\n",
        "!python gpt-2/src/train.py --model_name 124M --dataset _chat.npz --sample_every 500 --sample_num 1 --save_every=250"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mv: cannot stat 'gpt-2/train.py': No such file or directory\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:147: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
            "\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/memory_saving_gradients.py:13: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
            "\n",
            "WARNING:tensorflow:From gpt-2/src/train.py:88: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From gpt-2/src/train.py:91: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "2019-11-14 01:34:33.288664: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
            "2019-11-14 01:34:33.288898: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1483640 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2019-11-14 01:34:33.288935: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2019-11-14 01:34:33.290852: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
            "2019-11-14 01:34:33.365435: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-11-14 01:34:33.366349: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5f63880 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2019-11-14 01:34:33.366403: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7\n",
            "2019-11-14 01:34:33.366621: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-11-14 01:34:33.367279: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-11-14 01:34:33.367648: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-11-14 01:34:33.369160: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
            "2019-11-14 01:34:33.370456: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\n",
            "2019-11-14 01:34:33.370816: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\n",
            "2019-11-14 01:34:33.373586: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2019-11-14 01:34:33.374931: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2019-11-14 01:34:33.378634: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-11-14 01:34:33.378772: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-11-14 01:34:33.379541: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-11-14 01:34:33.380194: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
            "2019-11-14 01:34:33.380262: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-11-14 01:34:33.381805: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-11-14 01:34:33.381839: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \n",
            "2019-11-14 01:34:33.381855: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \n",
            "2019-11-14 01:34:33.382029: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-11-14 01:34:33.382837: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-11-14 01:34:33.383574: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10805 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
            "WARNING:tensorflow:From gpt-2/src/train.py:92: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:148: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:152: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:36: The name tf.rsqrt is deprecated. Please use tf.math.rsqrt instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:166: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:65: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:16: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:70: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.random.categorical` instead.\n",
            "WARNING:tensorflow:From gpt-2/src/train.py:117: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From gpt-2/src/train.py:121: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From gpt-2/src/train.py:144: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
            "\n",
            "WARNING:tensorflow:From gpt-2/src/train.py:147: The name tf.summary.merge is deprecated. Please use tf.compat.v1.summary.merge instead.\n",
            "\n",
            "WARNING:tensorflow:From gpt-2/src/train.py:149: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
            "\n",
            "WARNING:tensorflow:From gpt-2/src/train.py:152: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "WARNING:tensorflow:From gpt-2/src/train.py:156: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
            "\n",
            "Loading checkpoint models/124M/model.ckpt\n",
            "Loading dataset...\n",
            "100% 1/1 [00:00<00:00, 46.41it/s]\n",
            "dataset has 580932 tokens\n",
            "Training...\n",
            "2019-11-14 01:34:54.354198: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
            "[1 | 4.54] loss=2.05 avg=2.05\n",
            "[2 | 5.46] loss=2.97 avg=2.51\n",
            "[3 | 6.40] loss=3.33 avg=2.79\n",
            "[4 | 7.32] loss=2.14 avg=2.62\n",
            "[5 | 8.26] loss=1.94 avg=2.48\n",
            "[6 | 9.17] loss=2.63 avg=2.51\n",
            "[7 | 10.11] loss=2.30 avg=2.48\n",
            "[8 | 11.03] loss=3.07 avg=2.56\n",
            "[9 | 11.97] loss=2.56 avg=2.56\n",
            "[10 | 12.90] loss=2.36 avg=2.54\n",
            "[11 | 13.83] loss=1.91 avg=2.48\n",
            "[12 | 14.75] loss=2.28 avg=2.46\n",
            "[13 | 15.68] loss=2.66 avg=2.48\n",
            "[14 | 16.61] loss=2.13 avg=2.45\n",
            "[15 | 17.53] loss=2.26 avg=2.44\n",
            "[16 | 18.47] loss=1.99 avg=2.40\n",
            "[17 | 19.40] loss=1.86 avg=2.37\n",
            "[18 | 20.33] loss=2.13 avg=2.36\n",
            "[19 | 21.27] loss=1.89 avg=2.33\n",
            "[20 | 22.20] loss=2.22 avg=2.32\n",
            "[21 | 23.14] loss=3.79 avg=2.40\n",
            "[22 | 24.07] loss=1.98 avg=2.38\n",
            "[23 | 25.00] loss=1.46 avg=2.33\n",
            "[24 | 25.93] loss=3.37 avg=2.38\n",
            "[25 | 26.86] loss=2.90 avg=2.41\n",
            "[26 | 27.81] loss=2.46 avg=2.41\n",
            "[27 | 28.74] loss=2.60 avg=2.42\n",
            "[28 | 29.67] loss=2.37 avg=2.41\n",
            "[29 | 30.60] loss=2.43 avg=2.41\n",
            "[30 | 31.53] loss=3.29 avg=2.45\n",
            "[31 | 32.47] loss=1.72 avg=2.42\n",
            "[32 | 33.40] loss=2.14 avg=2.41\n",
            "[33 | 34.34] loss=2.29 avg=2.41\n",
            "[34 | 35.28] loss=2.37 avg=2.40\n",
            "[35 | 36.21] loss=1.73 avg=2.38\n",
            "[36 | 37.13] loss=2.09 avg=2.37\n",
            "[37 | 38.06] loss=2.16 avg=2.37\n",
            "[38 | 39.00] loss=3.16 avg=2.39\n",
            "[39 | 39.93] loss=2.63 avg=2.40\n",
            "[40 | 40.86] loss=2.27 avg=2.39\n",
            "[41 | 41.79] loss=1.91 avg=2.38\n",
            "[42 | 42.72] loss=2.10 avg=2.37\n",
            "[43 | 43.66] loss=1.76 avg=2.35\n",
            "[44 | 44.59] loss=1.76 avg=2.34\n",
            "[45 | 45.52] loss=1.99 avg=2.33\n",
            "[46 | 46.45] loss=2.15 avg=2.32\n",
            "[47 | 47.39] loss=2.15 avg=2.32\n",
            "[48 | 48.32] loss=2.71 avg=2.33\n",
            "[49 | 49.25] loss=2.79 avg=2.34\n",
            "[50 | 50.18] loss=1.91 avg=2.33\n",
            "[51 | 51.11] loss=2.36 avg=2.33\n",
            "[52 | 52.05] loss=2.05 avg=2.32\n",
            "[53 | 52.97] loss=3.98 avg=2.36\n",
            "[54 | 53.90] loss=2.01 avg=2.35\n",
            "[55 | 54.83] loss=1.63 avg=2.34\n",
            "[56 | 55.77] loss=3.73 avg=2.37\n",
            "[57 | 56.70] loss=1.87 avg=2.36\n",
            "[58 | 57.63] loss=1.80 avg=2.35\n",
            "[59 | 58.56] loss=1.86 avg=2.34\n",
            "[60 | 59.50] loss=2.38 avg=2.34\n",
            "[61 | 60.43] loss=2.23 avg=2.33\n",
            "[62 | 61.37] loss=1.78 avg=2.32\n",
            "[63 | 62.29] loss=1.85 avg=2.31\n",
            "[64 | 63.23] loss=1.91 avg=2.30\n",
            "[65 | 64.16] loss=1.98 avg=2.30\n",
            "[66 | 65.09] loss=3.22 avg=2.32\n",
            "[67 | 66.01] loss=1.97 avg=2.31\n",
            "[68 | 66.94] loss=1.46 avg=2.29\n",
            "[69 | 67.87] loss=1.69 avg=2.28\n",
            "[70 | 68.80] loss=1.76 avg=2.27\n",
            "[71 | 69.73] loss=2.03 avg=2.26\n",
            "[72 | 70.65] loss=2.70 avg=2.27\n",
            "[73 | 71.57] loss=3.13 avg=2.29\n",
            "[74 | 72.51] loss=2.06 avg=2.29\n",
            "[75 | 73.44] loss=1.67 avg=2.27\n",
            "[76 | 74.38] loss=2.02 avg=2.27\n",
            "[77 | 75.31] loss=1.93 avg=2.26\n",
            "[78 | 76.24] loss=2.07 avg=2.26\n",
            "[79 | 77.17] loss=2.11 avg=2.26\n",
            "[80 | 78.10] loss=1.93 avg=2.25\n",
            "[81 | 79.03] loss=1.64 avg=2.24\n",
            "[82 | 79.96] loss=2.27 avg=2.24\n",
            "[83 | 80.88] loss=1.96 avg=2.23\n",
            "[84 | 81.80] loss=2.04 avg=2.23\n",
            "[85 | 82.74] loss=1.89 avg=2.23\n",
            "[86 | 83.67] loss=1.94 avg=2.22\n",
            "[87 | 84.61] loss=1.67 avg=2.21\n",
            "[88 | 85.53] loss=1.94 avg=2.21\n",
            "[89 | 86.46] loss=1.63 avg=2.20\n",
            "[90 | 87.39] loss=2.41 avg=2.20\n",
            "[91 | 88.31] loss=1.98 avg=2.20\n",
            "[92 | 89.24] loss=2.21 avg=2.20\n",
            "[93 | 90.18] loss=2.40 avg=2.20\n",
            "[94 | 91.11] loss=2.85 avg=2.21\n",
            "[95 | 92.04] loss=2.30 avg=2.21\n",
            "[96 | 92.97] loss=2.18 avg=2.21\n",
            "[97 | 93.90] loss=1.95 avg=2.21\n",
            "[98 | 94.83] loss=1.75 avg=2.20\n",
            "[99 | 95.76] loss=1.67 avg=2.19\n",
            "[100 | 96.69] loss=1.84 avg=2.19\n",
            "[101 | 97.61] loss=1.76 avg=2.18\n",
            "[102 | 98.55] loss=1.88 avg=2.17\n",
            "[103 | 99.47] loss=1.39 avg=2.16\n",
            "[104 | 100.40] loss=2.49 avg=2.17\n",
            "[105 | 101.34] loss=1.73 avg=2.16\n",
            "[106 | 102.27] loss=1.68 avg=2.15\n",
            "[107 | 103.20] loss=2.17 avg=2.15\n",
            "[108 | 104.12] loss=1.99 avg=2.15\n",
            "[109 | 105.06] loss=2.32 avg=2.15\n",
            "[110 | 105.99] loss=2.04 avg=2.15\n",
            "[111 | 106.92] loss=1.57 avg=2.14\n",
            "[112 | 107.85] loss=1.49 avg=2.13\n",
            "[113 | 108.78] loss=1.94 avg=2.13\n",
            "[114 | 109.71] loss=1.65 avg=2.12\n",
            "[115 | 110.64] loss=1.96 avg=2.12\n",
            "[116 | 111.57] loss=2.35 avg=2.12\n",
            "[117 | 112.49] loss=2.03 avg=2.12\n",
            "[118 | 113.42] loss=1.99 avg=2.12\n",
            "[119 | 114.35] loss=1.71 avg=2.12\n",
            "[120 | 115.28] loss=2.93 avg=2.13\n",
            "[121 | 116.21] loss=2.55 avg=2.13\n",
            "[122 | 117.14] loss=2.18 avg=2.13\n",
            "[123 | 118.08] loss=1.56 avg=2.13\n",
            "[124 | 119.00] loss=1.99 avg=2.12\n",
            "[125 | 119.93] loss=2.10 avg=2.12\n",
            "[126 | 120.87] loss=1.86 avg=2.12\n",
            "[127 | 121.81] loss=3.84 avg=2.14\n",
            "[128 | 122.74] loss=1.50 avg=2.13\n",
            "[129 | 123.67] loss=2.31 avg=2.14\n",
            "[130 | 124.59] loss=1.55 avg=2.13\n",
            "[131 | 125.53] loss=3.63 avg=2.15\n",
            "[132 | 126.46] loss=1.40 avg=2.14\n",
            "[133 | 127.39] loss=1.97 avg=2.14\n",
            "[134 | 128.31] loss=1.75 avg=2.13\n",
            "[135 | 129.23] loss=2.24 avg=2.13\n",
            "[136 | 130.17] loss=2.28 avg=2.14\n",
            "[137 | 131.10] loss=2.59 avg=2.14\n",
            "[138 | 132.04] loss=2.78 avg=2.15\n",
            "[139 | 132.96] loss=1.41 avg=2.14\n",
            "[140 | 133.89] loss=1.83 avg=2.14\n",
            "[141 | 134.81] loss=1.29 avg=2.12\n",
            "[142 | 135.74] loss=1.59 avg=2.12\n",
            "[143 | 136.68] loss=2.23 avg=2.12\n",
            "[144 | 137.62] loss=1.51 avg=2.11\n",
            "[145 | 138.55] loss=1.89 avg=2.11\n",
            "[146 | 139.48] loss=1.57 avg=2.10\n",
            "[147 | 140.41] loss=2.20 avg=2.10\n",
            "[148 | 141.34] loss=1.42 avg=2.09\n",
            "[149 | 142.27] loss=1.47 avg=2.09\n",
            "[150 | 143.19] loss=1.92 avg=2.08\n",
            "[151 | 144.12] loss=2.97 avg=2.09\n",
            "[152 | 145.05] loss=1.96 avg=2.09\n",
            "[153 | 145.98] loss=1.85 avg=2.09\n",
            "[154 | 146.90] loss=2.03 avg=2.09\n",
            "[155 | 147.83] loss=2.52 avg=2.09\n",
            "[156 | 148.74] loss=1.96 avg=2.09\n",
            "[157 | 149.68] loss=2.16 avg=2.09\n",
            "[158 | 150.61] loss=1.95 avg=2.09\n",
            "[159 | 151.54] loss=1.75 avg=2.09\n",
            "[160 | 152.47] loss=2.34 avg=2.09\n",
            "[161 | 153.40] loss=2.30 avg=2.09\n",
            "[162 | 154.33] loss=1.79 avg=2.09\n",
            "[163 | 155.25] loss=1.61 avg=2.08\n",
            "[164 | 156.19] loss=2.92 avg=2.09\n",
            "[165 | 157.12] loss=2.03 avg=2.09\n",
            "[166 | 158.05] loss=1.54 avg=2.09\n",
            "[167 | 158.98] loss=1.63 avg=2.08\n",
            "[168 | 159.90] loss=1.51 avg=2.07\n",
            "[169 | 160.83] loss=1.67 avg=2.07\n",
            "[170 | 161.76] loss=2.14 avg=2.07\n",
            "[171 | 162.68] loss=2.04 avg=2.07\n",
            "[172 | 163.61] loss=2.06 avg=2.07\n",
            "[173 | 164.54] loss=2.37 avg=2.07\n",
            "[174 | 165.46] loss=1.58 avg=2.07\n",
            "[175 | 166.40] loss=1.74 avg=2.06\n",
            "[176 | 167.32] loss=2.02 avg=2.06\n",
            "[177 | 168.24] loss=1.86 avg=2.06\n",
            "[178 | 169.17] loss=2.21 avg=2.06\n",
            "[179 | 170.10] loss=1.18 avg=2.05\n",
            "[180 | 171.02] loss=2.11 avg=2.05\n",
            "[181 | 171.95] loss=2.16 avg=2.05\n",
            "[182 | 172.88] loss=1.71 avg=2.05\n",
            "[183 | 173.81] loss=1.78 avg=2.05\n",
            "[184 | 174.73] loss=1.90 avg=2.04\n",
            "[185 | 175.66] loss=1.94 avg=2.04\n",
            "[186 | 176.59] loss=1.86 avg=2.04\n",
            "[187 | 177.51] loss=3.34 avg=2.06\n",
            "[188 | 178.44] loss=1.65 avg=2.05\n",
            "[189 | 179.36] loss=2.05 avg=2.05\n",
            "[190 | 180.29] loss=1.70 avg=2.05\n",
            "[191 | 181.22] loss=1.70 avg=2.04\n",
            "[192 | 182.14] loss=1.68 avg=2.04\n",
            "[193 | 183.07] loss=1.44 avg=2.03\n",
            "[194 | 184.00] loss=2.11 avg=2.03\n",
            "[195 | 184.94] loss=2.14 avg=2.03\n",
            "[196 | 185.87] loss=2.12 avg=2.04\n",
            "[197 | 186.80] loss=1.90 avg=2.03\n",
            "[198 | 187.73] loss=1.43 avg=2.03\n",
            "[199 | 188.66] loss=1.99 avg=2.03\n",
            "[200 | 189.59] loss=1.83 avg=2.02\n",
            "[201 | 190.51] loss=1.45 avg=2.02\n",
            "[202 | 191.45] loss=1.41 avg=2.01\n",
            "[203 | 192.38] loss=1.84 avg=2.01\n",
            "[204 | 193.31] loss=2.62 avg=2.02\n",
            "[205 | 194.23] loss=2.49 avg=2.02\n",
            "[206 | 195.16] loss=2.37 avg=2.02\n",
            "[207 | 196.10] loss=1.44 avg=2.02\n",
            "[208 | 197.03] loss=1.84 avg=2.02\n",
            "[209 | 197.96] loss=1.45 avg=2.01\n",
            "[210 | 198.89] loss=1.89 avg=2.01\n",
            "[211 | 199.82] loss=2.17 avg=2.01\n",
            "[212 | 200.75] loss=2.16 avg=2.01\n",
            "[213 | 201.67] loss=1.77 avg=2.01\n",
            "[214 | 202.60] loss=1.76 avg=2.01\n",
            "[215 | 203.52] loss=1.65 avg=2.00\n",
            "[216 | 204.45] loss=1.96 avg=2.00\n",
            "[217 | 205.39] loss=1.19 avg=1.99\n",
            "[218 | 206.31] loss=1.84 avg=1.99\n",
            "[219 | 207.25] loss=1.63 avg=1.99\n",
            "[220 | 208.18] loss=1.98 avg=1.99\n",
            "[221 | 209.10] loss=1.41 avg=1.98\n",
            "[222 | 210.03] loss=1.66 avg=1.98\n",
            "[223 | 210.96] loss=1.72 avg=1.97\n",
            "[224 | 211.89] loss=1.78 avg=1.97\n",
            "[225 | 212.82] loss=1.69 avg=1.97\n",
            "[226 | 213.75] loss=1.62 avg=1.96\n",
            "[227 | 214.68] loss=2.33 avg=1.97\n",
            "[228 | 215.61] loss=2.99 avg=1.98\n",
            "[229 | 216.54] loss=2.16 avg=1.98\n",
            "[230 | 217.47] loss=1.67 avg=1.98\n",
            "[231 | 218.40] loss=2.77 avg=1.99\n",
            "[232 | 219.32] loss=1.82 avg=1.99\n",
            "[233 | 220.24] loss=2.17 avg=1.99\n",
            "[234 | 221.17] loss=1.47 avg=1.98\n",
            "[235 | 222.10] loss=1.48 avg=1.98\n",
            "[236 | 223.03] loss=1.59 avg=1.97\n",
            "[237 | 223.96] loss=2.35 avg=1.98\n",
            "[238 | 224.89] loss=1.59 avg=1.97\n",
            "[239 | 225.83] loss=1.79 avg=1.97\n",
            "[240 | 226.76] loss=2.08 avg=1.97\n",
            "[241 | 227.69] loss=1.65 avg=1.97\n",
            "[242 | 228.62] loss=1.54 avg=1.96\n",
            "[243 | 229.54] loss=1.56 avg=1.96\n",
            "[244 | 230.48] loss=1.91 avg=1.96\n",
            "[245 | 231.42] loss=1.80 avg=1.96\n",
            "[246 | 232.34] loss=2.10 avg=1.96\n",
            "[247 | 233.27] loss=1.44 avg=1.95\n",
            "[248 | 234.20] loss=1.63 avg=1.95\n",
            "[249 | 235.13] loss=2.18 avg=1.95\n",
            "[250 | 236.05] loss=1.66 avg=1.95\n",
            "[251 | 236.99] loss=2.30 avg=1.95\n",
            "[252 | 237.92] loss=2.33 avg=1.96\n",
            "[253 | 238.85] loss=2.37 avg=1.96\n",
            "[254 | 239.78] loss=1.98 avg=1.96\n",
            "[255 | 240.72] loss=1.74 avg=1.96\n",
            "[256 | 241.65] loss=1.39 avg=1.95\n",
            "[257 | 242.58] loss=2.33 avg=1.96\n",
            "[258 | 243.51] loss=2.19 avg=1.96\n",
            "[259 | 244.45] loss=2.39 avg=1.96\n",
            "[260 | 245.38] loss=2.30 avg=1.97\n",
            "[261 | 246.31] loss=1.69 avg=1.96\n",
            "[262 | 247.24] loss=1.77 avg=1.96\n",
            "[263 | 248.17] loss=1.63 avg=1.96\n",
            "[264 | 249.11] loss=1.61 avg=1.95\n",
            "[265 | 250.03] loss=2.49 avg=1.96\n",
            "[266 | 250.96] loss=1.66 avg=1.96\n",
            "[267 | 251.89] loss=1.77 avg=1.96\n",
            "[268 | 252.81] loss=1.63 avg=1.95\n",
            "[269 | 253.74] loss=3.15 avg=1.96\n",
            "[270 | 254.67] loss=2.24 avg=1.97\n",
            "[271 | 255.60] loss=3.07 avg=1.98\n",
            "[272 | 256.54] loss=2.17 avg=1.98\n",
            "[273 | 257.47] loss=1.98 avg=1.98\n",
            "[274 | 258.40] loss=1.64 avg=1.98\n",
            "[275 | 259.33] loss=3.31 avg=1.99\n",
            "[276 | 260.27] loss=1.77 avg=1.99\n",
            "[277 | 261.19] loss=1.94 avg=1.99\n",
            "[278 | 262.12] loss=1.94 avg=1.99\n",
            "[279 | 263.05] loss=1.82 avg=1.99\n",
            "[280 | 263.99] loss=1.80 avg=1.98\n",
            "[281 | 264.91] loss=1.59 avg=1.98\n",
            "[282 | 265.84] loss=1.86 avg=1.98\n",
            "[283 | 266.76] loss=1.52 avg=1.97\n",
            "[284 | 267.69] loss=2.86 avg=1.98\n",
            "[285 | 268.62] loss=1.84 avg=1.98\n",
            "[286 | 269.55] loss=1.93 avg=1.98\n",
            "[287 | 270.48] loss=1.79 avg=1.98\n",
            "[288 | 271.40] loss=2.00 avg=1.98\n",
            "[289 | 272.34] loss=2.22 avg=1.98\n",
            "[290 | 273.26] loss=2.71 avg=1.99\n",
            "[291 | 274.19] loss=1.63 avg=1.99\n",
            "[292 | 275.11] loss=1.59 avg=1.98\n",
            "[293 | 276.04] loss=3.12 avg=1.99\n",
            "[294 | 276.97] loss=1.67 avg=1.99\n",
            "[295 | 277.90] loss=2.92 avg=2.00\n",
            "[296 | 278.82] loss=1.96 avg=2.00\n",
            "[297 | 279.76] loss=1.19 avg=1.99\n",
            "[298 | 280.70] loss=1.55 avg=1.99\n",
            "[299 | 281.63] loss=2.02 avg=1.99\n",
            "[300 | 282.55] loss=2.70 avg=1.99\n",
            "[301 | 283.48] loss=1.60 avg=1.99\n",
            "[302 | 284.41] loss=1.77 avg=1.99\n",
            "[303 | 285.34] loss=1.69 avg=1.99\n",
            "[304 | 286.26] loss=1.96 avg=1.98\n",
            "[305 | 287.20] loss=1.58 avg=1.98\n",
            "[306 | 288.13] loss=2.19 avg=1.98\n",
            "[307 | 289.07] loss=1.56 avg=1.98\n",
            "[308 | 289.99] loss=1.87 avg=1.98\n",
            "[309 | 290.92] loss=2.08 avg=1.98\n",
            "[310 | 291.85] loss=1.67 avg=1.98\n",
            "[311 | 292.78] loss=1.37 avg=1.97\n",
            "[312 | 293.71] loss=1.45 avg=1.96\n",
            "[313 | 294.63] loss=1.52 avg=1.96\n",
            "[314 | 295.56] loss=2.04 avg=1.96\n",
            "[315 | 296.50] loss=1.84 avg=1.96\n",
            "[316 | 297.42] loss=2.40 avg=1.96\n",
            "[317 | 298.35] loss=1.86 avg=1.96\n",
            "[318 | 299.27] loss=1.72 avg=1.96\n",
            "[319 | 300.21] loss=1.72 avg=1.96\n",
            "[320 | 301.14] loss=1.28 avg=1.95\n",
            "[321 | 302.07] loss=1.66 avg=1.95\n",
            "[322 | 303.00] loss=1.63 avg=1.94\n",
            "[323 | 303.93] loss=1.25 avg=1.94\n",
            "[324 | 304.85] loss=1.66 avg=1.93\n",
            "[325 | 305.78] loss=1.90 avg=1.93\n",
            "[326 | 306.71] loss=1.53 avg=1.93\n",
            "[327 | 307.64] loss=1.69 avg=1.93\n",
            "[328 | 308.56] loss=1.90 avg=1.93\n",
            "[329 | 309.49] loss=1.58 avg=1.92\n",
            "[330 | 310.42] loss=1.98 avg=1.92\n",
            "[331 | 311.35] loss=1.63 avg=1.92\n",
            "[332 | 312.28] loss=1.55 avg=1.92\n",
            "[333 | 313.20] loss=1.80 avg=1.92\n",
            "[334 | 314.13] loss=2.07 avg=1.92\n",
            "[335 | 315.06] loss=1.51 avg=1.91\n",
            "[336 | 315.99] loss=2.28 avg=1.92\n",
            "[337 | 316.91] loss=2.38 avg=1.92\n",
            "[338 | 317.84] loss=1.77 avg=1.92\n",
            "[339 | 318.77] loss=1.57 avg=1.92\n",
            "[340 | 319.70] loss=1.37 avg=1.91\n",
            "[341 | 320.62] loss=1.83 avg=1.91\n",
            "[342 | 321.55] loss=1.55 avg=1.91\n",
            "[343 | 322.49] loss=2.24 avg=1.91\n",
            "[344 | 323.41] loss=1.25 avg=1.90\n",
            "[345 | 324.35] loss=1.87 avg=1.90\n",
            "[346 | 325.28] loss=1.66 avg=1.90\n",
            "[347 | 326.21] loss=1.78 avg=1.90\n",
            "[348 | 327.14] loss=1.59 avg=1.90\n",
            "[349 | 328.06] loss=1.78 avg=1.89\n",
            "[350 | 328.99] loss=1.49 avg=1.89\n",
            "[351 | 329.92] loss=3.53 avg=1.91\n",
            "[352 | 330.85] loss=1.87 avg=1.91\n",
            "[353 | 331.79] loss=2.25 avg=1.91\n",
            "[354 | 332.72] loss=1.75 avg=1.91\n",
            "[355 | 333.65] loss=1.63 avg=1.91\n",
            "[356 | 334.58] loss=1.47 avg=1.90\n",
            "[357 | 335.51] loss=1.94 avg=1.90\n",
            "[358 | 336.44] loss=1.55 avg=1.90\n",
            "[359 | 337.38] loss=1.51 avg=1.89\n",
            "[360 | 338.31] loss=2.41 avg=1.90\n",
            "[361 | 339.24] loss=2.66 avg=1.91\n",
            "[362 | 340.17] loss=1.73 avg=1.91\n",
            "[363 | 341.10] loss=1.72 avg=1.90\n",
            "[364 | 342.02] loss=1.82 avg=1.90\n",
            "[365 | 342.96] loss=2.21 avg=1.91\n",
            "[366 | 343.88] loss=1.81 avg=1.90\n",
            "[367 | 344.80] loss=1.52 avg=1.90\n",
            "[368 | 345.72] loss=1.37 avg=1.90\n",
            "[369 | 346.64] loss=2.73 avg=1.90\n",
            "[370 | 347.56] loss=2.23 avg=1.91\n",
            "[371 | 348.48] loss=2.07 avg=1.91\n",
            "[372 | 349.40] loss=1.70 avg=1.91\n",
            "[373 | 350.33] loss=2.80 avg=1.92\n",
            "[374 | 351.25] loss=1.58 avg=1.91\n",
            "[375 | 352.17] loss=1.78 avg=1.91\n",
            "[376 | 353.09] loss=1.93 avg=1.91\n",
            "[377 | 354.00] loss=1.40 avg=1.91\n",
            "[378 | 354.93] loss=1.73 avg=1.90\n",
            "[379 | 355.85] loss=2.60 avg=1.91\n",
            "[380 | 356.77] loss=1.63 avg=1.91\n",
            "[381 | 357.69] loss=1.56 avg=1.90\n",
            "[382 | 358.61] loss=1.90 avg=1.90\n",
            "[383 | 359.53] loss=2.14 avg=1.91\n",
            "[384 | 360.45] loss=1.16 avg=1.90\n",
            "[385 | 361.37] loss=1.59 avg=1.90\n",
            "[386 | 362.30] loss=1.88 avg=1.90\n",
            "[387 | 363.21] loss=1.51 avg=1.89\n",
            "[388 | 364.13] loss=1.61 avg=1.89\n",
            "[389 | 365.04] loss=1.33 avg=1.88\n",
            "[390 | 365.96] loss=1.72 avg=1.88\n",
            "[391 | 366.88] loss=1.51 avg=1.88\n",
            "[392 | 367.80] loss=1.74 avg=1.88\n",
            "[393 | 368.72] loss=1.58 avg=1.87\n",
            "[394 | 369.64] loss=1.96 avg=1.87\n",
            "[395 | 370.56] loss=1.99 avg=1.88\n",
            "[396 | 371.46] loss=1.45 avg=1.87\n",
            "[397 | 372.38] loss=1.98 avg=1.87\n",
            "[398 | 373.30] loss=3.34 avg=1.89\n",
            "[399 | 374.22] loss=1.46 avg=1.88\n",
            "[400 | 375.14] loss=1.61 avg=1.88\n",
            "[401 | 376.07] loss=2.18 avg=1.88\n",
            "[402 | 376.98] loss=1.48 avg=1.88\n",
            "[403 | 377.90] loss=1.75 avg=1.88\n",
            "[404 | 378.82] loss=1.92 avg=1.88\n",
            "[405 | 379.74] loss=2.51 avg=1.89\n",
            "[406 | 380.66] loss=2.85 avg=1.89\n",
            "[407 | 381.58] loss=1.65 avg=1.89\n",
            "[408 | 382.50] loss=1.56 avg=1.89\n",
            "[409 | 383.42] loss=1.31 avg=1.88\n",
            "[410 | 384.34] loss=1.56 avg=1.88\n",
            "[411 | 385.27] loss=2.26 avg=1.88\n",
            "[412 | 386.20] loss=2.13 avg=1.89\n",
            "[413 | 387.12] loss=1.59 avg=1.88\n",
            "[414 | 388.04] loss=1.38 avg=1.88\n",
            "[415 | 388.95] loss=2.16 avg=1.88\n",
            "[416 | 389.88] loss=1.27 avg=1.87\n",
            "[417 | 390.80] loss=1.52 avg=1.87\n",
            "[418 | 391.73] loss=2.09 avg=1.87\n",
            "[419 | 392.65] loss=1.37 avg=1.87\n",
            "[420 | 393.57] loss=1.56 avg=1.87\n",
            "[421 | 394.47] loss=1.57 avg=1.86\n",
            "[422 | 395.39] loss=1.48 avg=1.86\n",
            "[423 | 396.32] loss=1.76 avg=1.86\n",
            "[424 | 397.24] loss=1.50 avg=1.85\n",
            "[425 | 398.16] loss=1.45 avg=1.85\n",
            "[426 | 399.09] loss=1.91 avg=1.85\n",
            "[427 | 399.99] loss=1.88 avg=1.85\n",
            "[428 | 400.91] loss=1.43 avg=1.85\n",
            "[429 | 401.82] loss=1.67 avg=1.84\n",
            "[430 | 402.75] loss=2.00 avg=1.85\n",
            "[431 | 403.67] loss=1.93 avg=1.85\n",
            "[432 | 404.59] loss=1.45 avg=1.84\n",
            "[433 | 405.52] loss=1.83 avg=1.84\n",
            "[434 | 406.44] loss=1.37 avg=1.84\n",
            "[435 | 407.35] loss=1.83 avg=1.84\n",
            "[436 | 408.26] loss=1.89 avg=1.84\n",
            "[437 | 409.18] loss=1.90 avg=1.84\n",
            "[438 | 410.11] loss=1.76 avg=1.84\n",
            "[439 | 411.03] loss=2.70 avg=1.85\n",
            "[440 | 411.95] loss=2.05 avg=1.85\n",
            "[441 | 412.87] loss=1.57 avg=1.85\n",
            "[442 | 413.79] loss=2.30 avg=1.85\n",
            "[443 | 414.71] loss=1.47 avg=1.85\n",
            "[444 | 415.64] loss=1.96 avg=1.85\n",
            "[445 | 416.56] loss=1.43 avg=1.84\n",
            "[446 | 417.48] loss=1.66 avg=1.84\n",
            "[447 | 418.42] loss=1.46 avg=1.84\n",
            "[448 | 419.35] loss=1.63 avg=1.84\n",
            "[449 | 420.28] loss=1.80 avg=1.84\n",
            "[450 | 421.20] loss=1.68 avg=1.83\n",
            "[451 | 422.13] loss=1.72 avg=1.83\n",
            "[452 | 423.05] loss=1.55 avg=1.83\n",
            "[453 | 423.98] loss=1.60 avg=1.83\n",
            "[454 | 424.92] loss=2.08 avg=1.83\n",
            "[455 | 425.84] loss=1.60 avg=1.83\n",
            "[456 | 426.77] loss=1.05 avg=1.82\n",
            "[457 | 427.69] loss=2.02 avg=1.82\n",
            "[458 | 428.63] loss=1.96 avg=1.82\n",
            "[459 | 429.55] loss=1.96 avg=1.82\n",
            "[460 | 430.48] loss=2.07 avg=1.83\n",
            "[461 | 431.41] loss=2.00 avg=1.83\n",
            "[462 | 432.33] loss=2.56 avg=1.84\n",
            "[463 | 433.26] loss=1.56 avg=1.83\n",
            "[464 | 434.20] loss=3.08 avg=1.85\n",
            "[465 | 435.13] loss=1.83 avg=1.85\n",
            "[466 | 436.05] loss=1.86 avg=1.85\n",
            "[467 | 436.98] loss=1.89 avg=1.85\n",
            "[468 | 437.91] loss=2.11 avg=1.85\n",
            "[469 | 438.83] loss=2.14 avg=1.85\n",
            "[470 | 439.76] loss=1.78 avg=1.85\n",
            "[471 | 440.69] loss=1.37 avg=1.85\n",
            "[472 | 441.61] loss=2.71 avg=1.86\n",
            "[473 | 442.54] loss=1.91 avg=1.86\n",
            "[474 | 443.45] loss=1.38 avg=1.85\n",
            "[475 | 444.38] loss=1.78 avg=1.85\n",
            "[476 | 445.31] loss=2.12 avg=1.85\n",
            "[477 | 446.23] loss=1.80 avg=1.85\n",
            "[478 | 447.17] loss=2.13 avg=1.86\n",
            "[479 | 448.09] loss=1.49 avg=1.85\n",
            "[480 | 449.03] loss=1.39 avg=1.85\n",
            "[481 | 449.95] loss=2.00 avg=1.85\n",
            "[482 | 450.88] loss=2.21 avg=1.85\n",
            "[483 | 451.81] loss=1.84 avg=1.85\n",
            "[484 | 452.74] loss=1.66 avg=1.85\n",
            "[485 | 453.66] loss=1.41 avg=1.85\n",
            "[486 | 454.60] loss=1.58 avg=1.84\n",
            "[487 | 455.52] loss=1.56 avg=1.84\n",
            "[488 | 456.45] loss=1.55 avg=1.84\n",
            "[489 | 457.37] loss=2.16 avg=1.84\n",
            "[490 | 458.30] loss=2.16 avg=1.84\n",
            "[491 | 459.23] loss=1.44 avg=1.84\n",
            "[492 | 460.16] loss=1.90 avg=1.84\n",
            "[493 | 461.08] loss=1.68 avg=1.84\n",
            "[494 | 462.02] loss=1.58 avg=1.84\n",
            "[495 | 462.95] loss=1.83 avg=1.84\n",
            "[496 | 463.86] loss=1.38 avg=1.83\n",
            "[497 | 464.79] loss=3.12 avg=1.84\n",
            "[498 | 465.70] loss=1.35 avg=1.84\n",
            "[499 | 466.63] loss=1.67 avg=1.84\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "ia de las veces muchas bien: con este que todo. Está caso estamentos los tuscanos y tocasan de todo y tocómano que estoy poco una toca q no es la entiende se mejor una tráfica y si los poder al cerrado a la toca como lunes. Muy gente a los tres ellos una crea así. Conocuntó como el conogedo mejor q no se todo.\n",
            "[04/04/18 12:57:12 p.m.] Jj: Jajaa\n",
            "[04/04/18 1:48:53 p.m.] Jj: Y que puedan se estoy también\n",
            "[04/04/18 1:48:57 p.m.] Jj: Voy a hejo\n",
            "[04/04/18 1:49:14 p.m.] Jj: Lo vaya\n",
            "[04/04/18 1:49:31 p.m.] Jj: Jajaa\n",
            "[04/04/18 1:49:51 p.m.] Jj: Voy a los señores\n",
            "[04/04/18 1:50:00 p.m.] Jj: Y a señor los ciudad del su salud\n",
            "[04/04/18 1:50:34 p.m.] Jj: Y a los ciudad tomar\n",
            "[04/04/18 1:50:44 p.m.] Jj: Jajaajaa\n",
            "[04/04/18 1:51:04 p.m.] Jj: Viernes\n",
            "[04/04/18 1:51:14 p.m.] Jj: Jajaapana\n",
            "[04/04/18 1:51:31 p.m.] Jj: Lo puede una término\n",
            "[04/04/18 1:51:45 p.m.] Jj: Este lo vaya\n",
            "[04/04/18 1:51:52 p.m.] Jj: Me vaya\n",
            "[04/04/18 1:52:15 p.m.] Jj: Vaya la jaja\n",
            "[04/04/18 1:52:49 p.m.] Jj: Conoceto\n",
            "[04/04/18 1:52:56 p.m.] Jj: Conicado\n",
            "[04/04/18 1:52:59 p.m.] Jj: Por favor\n",
            "[04/04/18 1:57:00 p.m.] Jj: Oje un adalecidad\n",
            "[04/04/18 2:00:02 p.m.] Kenia Porcell Privado: Así diciendo le digo\n",
            "[04/04/18 2:00:16 p.m.] Kenia Porcell Privado: Y q lo no se le casa de la poder al toca dolores. Por favor y la toca.\n",
            "[04/04/18 2:00:37 p.m.] Kenia Porcell Privado: Y al toca comandar\n",
            "[04/04/18 2:01:01 p.m.] Jj: Vaya\n",
            "[04/04/18 2:01:23 p.m.] Jj: Oedo fue una tener en el diferencia\n",
            "[04/04/18 2:01:35 p.m.] Jj: Que vez el cambio y se luego y lo cajaos\n",
            "[04/04/18 2:02:01 p.m.] Jj: Jajaa\n",
            "[04/04/18 2:02:18 p.m.] Jj: Asi especion\n",
            "[04/04/18 2:02:29 p.m.] Jj: Como ver las vaya que todaban el cambio\n",
            "[04/04/18 2:02:52 p.m.] Jj: Y a las cambio de su salud\n",
            "[04/04/18 2:03:14 p.m.] Jj: Conocetario\n",
            "[04/04/18 2:04:25 p.m.] Jj: Estoy la cambio\n",
            "[04/04/18 2:04:29 p.m.] Jj: Es puedo\n",
            "[04/04/18 2:04:34 p.m.] Jj: Que vez les cuestiones que le técnica por mi recede y que le más todo la corte\n",
            "[04/04/18 2:\n",
            "\n",
            "[500 | 485.65] loss=2.38 avg=1.84\n",
            "[501 | 486.58] loss=1.69 avg=1.84\n",
            "[502 | 487.50] loss=1.78 avg=1.84\n",
            "[503 | 488.42] loss=1.60 avg=1.84\n",
            "[504 | 489.33] loss=2.07 avg=1.84\n",
            "[505 | 490.25] loss=1.55 avg=1.84\n",
            "[506 | 491.17] loss=1.38 avg=1.83\n",
            "[507 | 492.09] loss=1.93 avg=1.83\n",
            "[508 | 493.00] loss=1.83 avg=1.83\n",
            "[509 | 493.93] loss=1.57 avg=1.83\n",
            "[510 | 494.85] loss=1.71 avg=1.83\n",
            "[511 | 495.76] loss=1.28 avg=1.82\n",
            "[512 | 496.68] loss=1.84 avg=1.82\n",
            "[513 | 497.60] loss=1.58 avg=1.82\n",
            "[514 | 498.52] loss=1.78 avg=1.82\n",
            "[515 | 499.45] loss=1.51 avg=1.82\n",
            "[516 | 500.37] loss=1.97 avg=1.82\n",
            "[517 | 501.28] loss=1.58 avg=1.82\n",
            "[518 | 502.21] loss=1.31 avg=1.81\n",
            "[519 | 503.13] loss=2.01 avg=1.81\n",
            "[520 | 504.05] loss=1.47 avg=1.81\n",
            "[521 | 504.98] loss=2.20 avg=1.82\n",
            "[522 | 505.90] loss=1.46 avg=1.81\n",
            "[523 | 506.83] loss=3.36 avg=1.83\n",
            "[524 | 507.75] loss=1.36 avg=1.82\n",
            "[525 | 508.66] loss=1.66 avg=1.82\n",
            "[526 | 509.58] loss=1.43 avg=1.82\n",
            "[527 | 510.50] loss=1.93 avg=1.82\n",
            "[528 | 511.42] loss=2.35 avg=1.82\n",
            "[529 | 512.34] loss=2.03 avg=1.83\n",
            "[530 | 513.24] loss=1.40 avg=1.82\n",
            "[531 | 514.17] loss=1.76 avg=1.82\n",
            "[532 | 515.09] loss=1.75 avg=1.82\n",
            "[533 | 516.01] loss=1.35 avg=1.82\n",
            "[534 | 516.92] loss=1.78 avg=1.81\n",
            "[535 | 517.85] loss=1.97 avg=1.82\n",
            "[536 | 518.76] loss=1.85 avg=1.82\n",
            "[537 | 519.68] loss=1.86 avg=1.82\n",
            "[538 | 520.60] loss=1.54 avg=1.81\n",
            "[539 | 521.53] loss=1.54 avg=1.81\n",
            "[540 | 522.45] loss=1.82 avg=1.81\n",
            "[541 | 523.37] loss=1.63 avg=1.81\n",
            "[542 | 524.29] loss=1.74 avg=1.81\n",
            "[543 | 525.21] loss=1.64 avg=1.81\n",
            "[544 | 526.13] loss=1.62 avg=1.81\n",
            "[545 | 527.05] loss=2.15 avg=1.81\n",
            "[546 | 527.98] loss=2.23 avg=1.81\n",
            "[547 | 528.91] loss=1.83 avg=1.81\n",
            "[548 | 529.84] loss=1.62 avg=1.81\n",
            "[549 | 530.78] loss=1.38 avg=1.81\n",
            "[550 | 531.71] loss=1.65 avg=1.81\n",
            "[551 | 532.64] loss=2.01 avg=1.81\n",
            "[552 | 533.57] loss=1.81 avg=1.81\n",
            "[553 | 534.50] loss=1.93 avg=1.81\n",
            "[554 | 535.43] loss=1.54 avg=1.81\n",
            "[555 | 536.36] loss=1.30 avg=1.80\n",
            "[556 | 537.29] loss=1.54 avg=1.80\n",
            "[557 | 538.21] loss=2.03 avg=1.80\n",
            "[558 | 539.14] loss=1.21 avg=1.79\n",
            "[559 | 540.08] loss=2.08 avg=1.80\n",
            "[560 | 541.00] loss=1.62 avg=1.80\n",
            "[561 | 541.93] loss=1.75 avg=1.80\n",
            "[562 | 542.86] loss=1.80 avg=1.80\n",
            "[563 | 543.78] loss=2.03 avg=1.80\n",
            "[564 | 544.71] loss=1.52 avg=1.80\n",
            "[565 | 545.64] loss=2.51 avg=1.80\n",
            "[566 | 546.57] loss=1.58 avg=1.80\n",
            "[567 | 547.50] loss=1.85 avg=1.80\n",
            "[568 | 548.43] loss=1.61 avg=1.80\n",
            "[569 | 549.37] loss=1.77 avg=1.80\n",
            "[570 | 550.30] loss=1.84 avg=1.80\n",
            "[571 | 551.22] loss=2.47 avg=1.81\n",
            "[572 | 552.16] loss=1.87 avg=1.81\n",
            "[573 | 553.09] loss=1.30 avg=1.80\n",
            "[574 | 554.01] loss=1.66 avg=1.80\n",
            "[575 | 554.94] loss=2.13 avg=1.80\n",
            "[576 | 555.88] loss=3.07 avg=1.82\n",
            "[577 | 556.81] loss=2.53 avg=1.82\n",
            "[578 | 557.73] loss=1.79 avg=1.82\n",
            "[579 | 558.66] loss=1.48 avg=1.82\n",
            "[580 | 559.59] loss=3.13 avg=1.83\n",
            "[581 | 560.52] loss=1.61 avg=1.83\n",
            "[582 | 561.45] loss=2.34 avg=1.84\n",
            "[583 | 562.37] loss=1.79 avg=1.83\n",
            "[584 | 563.31] loss=1.70 avg=1.83\n",
            "[585 | 564.23] loss=1.48 avg=1.83\n",
            "[586 | 565.16] loss=1.65 avg=1.83\n",
            "[587 | 566.08] loss=2.23 avg=1.83\n",
            "[588 | 567.02] loss=1.75 avg=1.83\n",
            "[589 | 567.95] loss=1.78 avg=1.83\n",
            "[590 | 568.88] loss=1.71 avg=1.83\n",
            "[591 | 569.80] loss=2.05 avg=1.83\n",
            "[592 | 570.74] loss=1.85 avg=1.83\n",
            "[593 | 571.66] loss=1.42 avg=1.83\n",
            "[594 | 572.59] loss=1.73 avg=1.83\n",
            "[595 | 573.52] loss=1.42 avg=1.82\n",
            "[596 | 574.45] loss=1.39 avg=1.82\n",
            "[597 | 575.38] loss=1.78 avg=1.82\n",
            "[598 | 576.32] loss=1.45 avg=1.81\n",
            "[599 | 577.24] loss=1.65 avg=1.81\n",
            "[600 | 578.17] loss=2.08 avg=1.82\n",
            "[601 | 579.09] loss=2.21 avg=1.82\n",
            "[602 | 580.02] loss=1.26 avg=1.81\n",
            "[603 | 580.95] loss=1.81 avg=1.81\n",
            "[604 | 581.88] loss=2.52 avg=1.82\n",
            "[605 | 582.80] loss=1.40 avg=1.82\n",
            "[606 | 583.72] loss=1.45 avg=1.81\n",
            "[607 | 584.65] loss=1.66 avg=1.81\n",
            "[608 | 585.58] loss=2.71 avg=1.82\n",
            "[609 | 586.51] loss=0.97 avg=1.81\n",
            "[610 | 587.43] loss=1.63 avg=1.81\n",
            "[611 | 588.36] loss=2.75 avg=1.82\n",
            "[612 | 589.29] loss=2.20 avg=1.82\n",
            "[613 | 590.22] loss=2.67 avg=1.83\n",
            "[614 | 591.15] loss=1.67 avg=1.83\n",
            "[615 | 592.07] loss=1.71 avg=1.83\n",
            "[616 | 593.00] loss=2.10 avg=1.83\n",
            "[617 | 593.92] loss=1.70 avg=1.83\n",
            "[618 | 594.85] loss=1.79 avg=1.83\n",
            "[619 | 595.77] loss=1.82 avg=1.83\n",
            "[620 | 596.71] loss=1.64 avg=1.83\n",
            "[621 | 597.63] loss=1.93 avg=1.83\n",
            "[622 | 598.55] loss=1.76 avg=1.83\n",
            "[623 | 599.48] loss=2.08 avg=1.83\n",
            "[624 | 600.40] loss=1.76 avg=1.83\n",
            "[625 | 601.33] loss=1.57 avg=1.83\n",
            "[626 | 602.26] loss=1.74 avg=1.83\n",
            "[627 | 603.18] loss=1.63 avg=1.82\n",
            "[628 | 604.11] loss=2.04 avg=1.83\n",
            "[629 | 605.04] loss=1.57 avg=1.82\n",
            "[630 | 605.96] loss=1.62 avg=1.82\n",
            "[631 | 606.89] loss=1.69 avg=1.82\n",
            "[632 | 607.82] loss=1.50 avg=1.82\n",
            "[633 | 608.76] loss=1.56 avg=1.81\n",
            "[634 | 609.69] loss=1.36 avg=1.81\n",
            "[635 | 610.60] loss=1.43 avg=1.81\n",
            "[636 | 611.54] loss=1.16 avg=1.80\n",
            "[637 | 612.48] loss=1.77 avg=1.80\n",
            "[638 | 613.41] loss=1.29 avg=1.79\n",
            "[639 | 614.33] loss=2.17 avg=1.80\n",
            "[640 | 615.25] loss=1.61 avg=1.80\n",
            "[641 | 616.18] loss=1.71 avg=1.80\n",
            "[642 | 617.12] loss=1.24 avg=1.79\n",
            "[643 | 618.05] loss=1.43 avg=1.79\n",
            "[644 | 618.98] loss=2.26 avg=1.79\n",
            "[645 | 619.90] loss=1.90 avg=1.79\n",
            "[646 | 620.83] loss=2.04 avg=1.79\n",
            "[647 | 621.76] loss=1.65 avg=1.79\n",
            "[648 | 622.69] loss=1.84 avg=1.79\n",
            "[649 | 623.61] loss=1.94 avg=1.80\n",
            "[650 | 624.55] loss=1.93 avg=1.80\n",
            "[651 | 625.47] loss=1.87 avg=1.80\n",
            "[652 | 626.40] loss=2.09 avg=1.80\n",
            "[653 | 627.32] loss=1.28 avg=1.80\n",
            "[654 | 628.24] loss=1.76 avg=1.79\n",
            "[655 | 629.18] loss=1.66 avg=1.79\n",
            "[656 | 630.10] loss=1.57 avg=1.79\n",
            "[657 | 631.02] loss=1.78 avg=1.79\n",
            "[658 | 631.95] loss=3.18 avg=1.80\n",
            "[659 | 632.88] loss=1.20 avg=1.80\n",
            "[660 | 633.81] loss=1.68 avg=1.80\n",
            "[661 | 634.74] loss=2.06 avg=1.80\n",
            "[662 | 635.67] loss=2.09 avg=1.80\n",
            "[663 | 636.60] loss=1.75 avg=1.80\n",
            "[664 | 637.52] loss=1.59 avg=1.80\n",
            "[665 | 638.45] loss=1.73 avg=1.80\n",
            "[666 | 639.38] loss=2.03 avg=1.80\n",
            "[667 | 640.31] loss=1.77 avg=1.80\n",
            "[668 | 641.24] loss=3.10 avg=1.81\n",
            "[669 | 642.17] loss=1.53 avg=1.81\n",
            "[670 | 643.10] loss=2.17 avg=1.82\n",
            "[671 | 644.03] loss=1.76 avg=1.82\n",
            "[672 | 644.96] loss=1.52 avg=1.81\n",
            "[673 | 645.88] loss=1.68 avg=1.81\n",
            "[674 | 646.82] loss=1.71 avg=1.81\n",
            "[675 | 647.75] loss=1.65 avg=1.81\n",
            "[676 | 648.68] loss=1.54 avg=1.81\n",
            "[677 | 649.60] loss=1.32 avg=1.80\n",
            "[678 | 650.53] loss=1.32 avg=1.80\n",
            "[679 | 651.46] loss=1.45 avg=1.79\n",
            "[680 | 652.39] loss=1.52 avg=1.79\n",
            "[681 | 653.31] loss=1.44 avg=1.79\n",
            "[682 | 654.24] loss=1.65 avg=1.78\n",
            "[683 | 655.17] loss=1.83 avg=1.79\n",
            "[684 | 656.10] loss=1.57 avg=1.78\n",
            "[685 | 657.03] loss=1.65 avg=1.78\n",
            "[686 | 657.96] loss=1.86 avg=1.78\n",
            "[687 | 658.89] loss=1.91 avg=1.78\n",
            "[688 | 659.82] loss=1.89 avg=1.78\n",
            "[689 | 660.75] loss=1.89 avg=1.79\n",
            "[690 | 661.68] loss=1.43 avg=1.78\n",
            "[691 | 662.61] loss=1.49 avg=1.78\n",
            "[692 | 663.54] loss=1.51 avg=1.78\n",
            "[693 | 664.46] loss=1.17 avg=1.77\n",
            "[694 | 665.39] loss=1.57 avg=1.77\n",
            "[695 | 666.32] loss=1.84 avg=1.77\n",
            "[696 | 667.25] loss=1.77 avg=1.77\n",
            "[697 | 668.17] loss=1.98 avg=1.77\n",
            "[698 | 669.11] loss=1.66 avg=1.77\n",
            "[699 | 670.05] loss=1.71 avg=1.77\n",
            "[700 | 670.98] loss=1.87 avg=1.77\n",
            "[701 | 671.90] loss=1.17 avg=1.76\n",
            "[702 | 672.83] loss=1.68 avg=1.76\n",
            "[703 | 673.76] loss=1.69 avg=1.76\n",
            "[704 | 674.68] loss=2.74 avg=1.77\n",
            "[705 | 675.61] loss=2.60 avg=1.78\n",
            "[706 | 676.53] loss=1.51 avg=1.78\n",
            "[707 | 677.46] loss=1.56 avg=1.78\n",
            "[708 | 678.38] loss=2.76 avg=1.79\n",
            "[709 | 679.31] loss=1.87 avg=1.79\n",
            "[710 | 680.23] loss=1.47 avg=1.78\n",
            "[711 | 681.16] loss=1.50 avg=1.78\n",
            "[712 | 682.09] loss=1.87 avg=1.78\n",
            "[713 | 683.01] loss=1.60 avg=1.78\n",
            "[714 | 683.94] loss=1.16 avg=1.77\n",
            "[715 | 684.88] loss=1.42 avg=1.77\n",
            "[716 | 685.81] loss=2.99 avg=1.78\n",
            "[717 | 686.72] loss=1.83 avg=1.78\n",
            "[718 | 687.64] loss=2.36 avg=1.79\n",
            "[719 | 688.57] loss=1.80 avg=1.79\n",
            "[720 | 689.49] loss=2.11 avg=1.79\n",
            "[721 | 690.42] loss=1.71 avg=1.79\n",
            "[722 | 691.35] loss=1.92 avg=1.79\n",
            "[723 | 692.29] loss=1.63 avg=1.79\n",
            "[724 | 693.21] loss=1.50 avg=1.79\n",
            "[725 | 694.14] loss=1.37 avg=1.78\n",
            "[726 | 695.06] loss=1.97 avg=1.79\n",
            "[727 | 696.00] loss=1.48 avg=1.78\n",
            "[728 | 696.92] loss=1.75 avg=1.78\n",
            "[729 | 697.85] loss=1.60 avg=1.78\n",
            "[730 | 698.78] loss=1.56 avg=1.78\n",
            "[731 | 699.71] loss=2.45 avg=1.79\n",
            "[732 | 700.64] loss=2.14 avg=1.79\n",
            "[733 | 701.57] loss=1.95 avg=1.79\n",
            "[734 | 702.49] loss=1.74 avg=1.79\n",
            "[735 | 703.43] loss=1.32 avg=1.79\n",
            "[736 | 704.36] loss=2.96 avg=1.80\n",
            "[737 | 705.29] loss=2.28 avg=1.80\n",
            "[738 | 706.21] loss=1.69 avg=1.80\n",
            "[739 | 707.15] loss=1.66 avg=1.80\n",
            "[740 | 708.07] loss=1.88 avg=1.80\n",
            "[741 | 709.00] loss=2.10 avg=1.80\n",
            "[742 | 709.93] loss=1.48 avg=1.80\n",
            "[743 | 710.86] loss=1.64 avg=1.80\n",
            "[744 | 711.79] loss=1.81 avg=1.80\n",
            "[745 | 712.71] loss=1.56 avg=1.80\n",
            "[746 | 713.63] loss=2.17 avg=1.80\n",
            "[747 | 714.57] loss=1.91 avg=1.80\n",
            "[748 | 715.49] loss=1.79 avg=1.80\n",
            "[749 | 716.42] loss=1.51 avg=1.80\n",
            "[750 | 717.35] loss=1.65 avg=1.80\n",
            "[751 | 718.28] loss=2.79 avg=1.81\n",
            "[752 | 719.21] loss=1.42 avg=1.80\n",
            "[753 | 720.13] loss=1.97 avg=1.80\n",
            "[754 | 721.06] loss=1.67 avg=1.80\n",
            "[755 | 721.99] loss=1.90 avg=1.80\n",
            "[756 | 722.92] loss=1.77 avg=1.80\n",
            "[757 | 723.84] loss=1.45 avg=1.80\n",
            "[758 | 724.75] loss=1.58 avg=1.80\n",
            "[759 | 725.69] loss=1.73 avg=1.80\n",
            "[760 | 726.62] loss=1.48 avg=1.79\n",
            "[761 | 727.55] loss=1.90 avg=1.79\n",
            "[762 | 728.48] loss=1.63 avg=1.79\n",
            "[763 | 729.41] loss=1.48 avg=1.79\n",
            "[764 | 730.34] loss=1.29 avg=1.78\n",
            "[765 | 731.27] loss=2.46 avg=1.79\n",
            "[766 | 732.19] loss=1.48 avg=1.79\n",
            "[767 | 733.13] loss=1.51 avg=1.79\n",
            "[768 | 734.05] loss=1.79 avg=1.79\n",
            "[769 | 734.98] loss=1.65 avg=1.78\n",
            "[770 | 735.91] loss=1.45 avg=1.78\n",
            "[771 | 736.83] loss=1.50 avg=1.78\n",
            "[772 | 737.76] loss=1.76 avg=1.78\n",
            "[773 | 738.68] loss=1.44 avg=1.77\n",
            "[774 | 739.61] loss=1.57 avg=1.77\n",
            "[775 | 740.54] loss=1.44 avg=1.77\n",
            "[776 | 741.46] loss=1.42 avg=1.77\n",
            "[777 | 742.39] loss=1.97 avg=1.77\n",
            "[778 | 743.32] loss=1.58 avg=1.77\n",
            "[779 | 744.26] loss=1.47 avg=1.76\n",
            "[780 | 745.19] loss=1.80 avg=1.76\n",
            "[781 | 746.11] loss=1.33 avg=1.76\n",
            "[782 | 747.04] loss=1.17 avg=1.75\n",
            "[783 | 747.98] loss=1.74 avg=1.75\n",
            "[784 | 748.91] loss=1.58 avg=1.75\n",
            "[785 | 749.83] loss=2.73 avg=1.76\n",
            "[786 | 750.77] loss=1.16 avg=1.76\n",
            "[787 | 751.70] loss=1.77 avg=1.76\n",
            "[788 | 752.63] loss=1.36 avg=1.75\n",
            "[789 | 753.55] loss=1.96 avg=1.75\n",
            "[790 | 754.48] loss=1.34 avg=1.75\n",
            "[791 | 755.42] loss=1.44 avg=1.75\n",
            "[792 | 756.34] loss=2.02 avg=1.75\n",
            "[793 | 757.28] loss=1.62 avg=1.75\n",
            "[794 | 758.20] loss=1.59 avg=1.75\n",
            "[795 | 759.14] loss=1.68 avg=1.75\n",
            "[796 | 760.07] loss=1.84 avg=1.75\n",
            "[797 | 760.99] loss=1.63 avg=1.75\n",
            "[798 | 761.92] loss=1.77 avg=1.75\n",
            "[799 | 762.85] loss=2.13 avg=1.75\n",
            "[800 | 763.78] loss=1.80 avg=1.75\n",
            "[801 | 764.71] loss=1.63 avg=1.75\n",
            "[802 | 765.64] loss=1.65 avg=1.75\n",
            "[803 | 766.56] loss=1.66 avg=1.75\n",
            "[804 | 767.49] loss=1.23 avg=1.74\n",
            "[805 | 768.41] loss=2.40 avg=1.75\n",
            "[806 | 769.34] loss=1.54 avg=1.75\n",
            "[807 | 770.27] loss=1.49 avg=1.74\n",
            "[808 | 771.20] loss=2.00 avg=1.75\n",
            "[809 | 772.12] loss=1.28 avg=1.74\n",
            "[810 | 773.05] loss=1.84 avg=1.74\n",
            "[811 | 773.99] loss=1.64 avg=1.74\n",
            "[812 | 774.92] loss=1.74 avg=1.74\n",
            "[813 | 775.84] loss=1.52 avg=1.74\n",
            "[814 | 776.78] loss=2.63 avg=1.75\n",
            "[815 | 777.71] loss=1.49 avg=1.75\n",
            "[816 | 778.64] loss=2.16 avg=1.75\n",
            "[817 | 779.56] loss=1.70 avg=1.75\n",
            "[818 | 780.50] loss=1.76 avg=1.75\n",
            "[819 | 781.43] loss=1.41 avg=1.75\n",
            "[820 | 782.35] loss=1.88 avg=1.75\n",
            "[821 | 783.28] loss=1.55 avg=1.75\n",
            "[822 | 784.21] loss=1.55 avg=1.74\n",
            "[823 | 785.14] loss=1.29 avg=1.74\n",
            "[824 | 786.08] loss=1.35 avg=1.73\n",
            "[825 | 786.99] loss=1.62 avg=1.73\n",
            "[826 | 787.92] loss=1.66 avg=1.73\n",
            "[827 | 788.86] loss=1.90 avg=1.73\n",
            "[828 | 789.78] loss=1.60 avg=1.73\n",
            "[829 | 790.71] loss=1.64 avg=1.73\n",
            "[830 | 791.65] loss=1.80 avg=1.73\n",
            "[831 | 792.58] loss=1.45 avg=1.73\n",
            "[832 | 793.50] loss=1.92 avg=1.73\n",
            "[833 | 794.43] loss=1.50 avg=1.73\n",
            "[834 | 795.36] loss=1.62 avg=1.73\n",
            "[835 | 796.30] loss=1.72 avg=1.73\n",
            "[836 | 797.23] loss=2.11 avg=1.73\n",
            "[837 | 798.16] loss=1.29 avg=1.73\n",
            "[838 | 799.09] loss=1.77 avg=1.73\n",
            "[839 | 800.02] loss=1.73 avg=1.73\n",
            "[840 | 800.95] loss=1.53 avg=1.73\n",
            "[841 | 801.88] loss=1.22 avg=1.72\n",
            "[842 | 802.81] loss=1.39 avg=1.72\n",
            "[843 | 803.74] loss=1.88 avg=1.72\n",
            "[844 | 804.67] loss=1.67 avg=1.72\n",
            "[845 | 805.60] loss=1.70 avg=1.72\n",
            "[846 | 806.53] loss=2.77 avg=1.73\n",
            "[847 | 807.47] loss=1.56 avg=1.73\n",
            "[848 | 808.39] loss=2.41 avg=1.73\n",
            "[849 | 809.32] loss=2.07 avg=1.74\n",
            "[850 | 810.25] loss=1.94 avg=1.74\n",
            "[851 | 811.18] loss=1.84 avg=1.74\n",
            "[852 | 812.11] loss=1.62 avg=1.74\n",
            "[853 | 813.04] loss=2.59 avg=1.75\n",
            "[854 | 813.97] loss=1.74 avg=1.75\n",
            "[855 | 814.90] loss=1.37 avg=1.74\n",
            "[856 | 815.83] loss=1.29 avg=1.74\n",
            "[857 | 816.76] loss=1.78 avg=1.74\n",
            "[858 | 817.69] loss=1.66 avg=1.74\n",
            "[859 | 818.62] loss=1.70 avg=1.74\n",
            "[860 | 819.55] loss=1.77 avg=1.74\n",
            "[861 | 820.48] loss=1.72 avg=1.74\n",
            "[862 | 821.41] loss=1.40 avg=1.74\n",
            "[863 | 822.33] loss=1.99 avg=1.74\n",
            "[864 | 823.26] loss=1.79 avg=1.74\n",
            "[865 | 824.19] loss=1.39 avg=1.74\n",
            "[866 | 825.11] loss=1.57 avg=1.73\n",
            "[867 | 826.04] loss=1.53 avg=1.73\n",
            "[868 | 826.97] loss=1.69 avg=1.73\n",
            "[869 | 827.90] loss=1.42 avg=1.73\n",
            "[870 | 828.83] loss=2.38 avg=1.73\n",
            "[871 | 829.75] loss=1.36 avg=1.73\n",
            "[872 | 830.69] loss=1.57 avg=1.73\n",
            "[873 | 831.62] loss=2.02 avg=1.73\n",
            "[874 | 832.56] loss=3.59 avg=1.75\n",
            "[875 | 833.49] loss=1.68 avg=1.75\n",
            "[876 | 834.42] loss=1.72 avg=1.75\n",
            "[877 | 835.34] loss=1.77 avg=1.75\n",
            "[878 | 836.27] loss=1.82 avg=1.75\n",
            "[879 | 837.21] loss=1.54 avg=1.75\n",
            "[880 | 838.14] loss=1.25 avg=1.74\n",
            "[881 | 839.07] loss=1.64 avg=1.74\n",
            "[882 | 840.00] loss=1.20 avg=1.74\n",
            "[883 | 840.94] loss=1.21 avg=1.73\n",
            "[884 | 841.86] loss=1.51 avg=1.73\n",
            "[885 | 842.80] loss=1.39 avg=1.73\n",
            "[886 | 843.73] loss=2.10 avg=1.73\n",
            "[887 | 844.66] loss=1.25 avg=1.73\n",
            "[888 | 845.60] loss=1.55 avg=1.72\n",
            "[889 | 846.52] loss=1.78 avg=1.72\n",
            "[890 | 847.44] loss=1.27 avg=1.72\n",
            "[891 | 848.37] loss=1.49 avg=1.72\n",
            "[892 | 849.30] loss=1.52 avg=1.72\n",
            "[893 | 850.24] loss=1.61 avg=1.71\n",
            "[894 | 851.17] loss=1.89 avg=1.72\n",
            "[895 | 852.09] loss=2.66 avg=1.73\n",
            "[896 | 853.03] loss=1.66 avg=1.72\n",
            "[897 | 853.96] loss=1.66 avg=1.72\n",
            "[898 | 854.88] loss=1.84 avg=1.73\n",
            "[899 | 855.80] loss=1.52 avg=1.72\n",
            "[900 | 856.73] loss=1.47 avg=1.72\n",
            "[901 | 857.65] loss=2.51 avg=1.73\n",
            "[902 | 858.59] loss=1.45 avg=1.73\n",
            "[903 | 859.52] loss=1.38 avg=1.72\n",
            "[904 | 860.45] loss=1.62 avg=1.72\n",
            "[905 | 861.38] loss=2.10 avg=1.73\n",
            "[906 | 862.31] loss=1.76 avg=1.73\n",
            "[907 | 863.24] loss=1.87 avg=1.73\n",
            "[908 | 864.18] loss=1.43 avg=1.72\n",
            "[909 | 865.11] loss=1.91 avg=1.73\n",
            "[910 | 866.04] loss=2.22 avg=1.73\n",
            "[911 | 866.97] loss=1.16 avg=1.73\n",
            "[912 | 867.90] loss=2.03 avg=1.73\n",
            "[913 | 868.84] loss=2.30 avg=1.73\n",
            "[914 | 869.76] loss=1.49 avg=1.73\n",
            "[915 | 870.68] loss=1.40 avg=1.73\n",
            "[916 | 871.62] loss=1.31 avg=1.72\n",
            "[917 | 872.55] loss=1.80 avg=1.72\n",
            "[918 | 873.47] loss=1.22 avg=1.72\n",
            "[919 | 874.40] loss=1.45 avg=1.72\n",
            "[920 | 875.32] loss=1.61 avg=1.72\n",
            "[921 | 876.26] loss=1.68 avg=1.72\n",
            "[922 | 877.18] loss=1.91 avg=1.72\n",
            "[923 | 878.11] loss=1.94 avg=1.72\n",
            "[924 | 879.03] loss=1.60 avg=1.72\n",
            "[925 | 879.97] loss=1.61 avg=1.72\n",
            "[926 | 880.90] loss=1.44 avg=1.71\n",
            "[927 | 881.84] loss=2.31 avg=1.72\n",
            "[928 | 882.77] loss=1.70 avg=1.72\n",
            "[929 | 883.70] loss=1.32 avg=1.72\n",
            "[930 | 884.63] loss=1.44 avg=1.71\n",
            "[931 | 885.56] loss=1.60 avg=1.71\n",
            "[932 | 886.48] loss=1.24 avg=1.71\n",
            "[933 | 887.42] loss=1.53 avg=1.71\n",
            "[934 | 888.34] loss=1.89 avg=1.71\n",
            "[935 | 889.27] loss=1.83 avg=1.71\n",
            "[936 | 890.20] loss=1.54 avg=1.71\n",
            "[937 | 891.13] loss=1.39 avg=1.70\n",
            "[938 | 892.06] loss=1.13 avg=1.70\n",
            "[939 | 892.98] loss=1.97 avg=1.70\n",
            "[940 | 893.91] loss=1.48 avg=1.70\n",
            "[941 | 894.85] loss=1.58 avg=1.70\n",
            "[942 | 895.77] loss=1.93 avg=1.70\n",
            "[943 | 896.70] loss=1.43 avg=1.70\n",
            "[944 | 897.62] loss=2.42 avg=1.70\n",
            "[945 | 898.55] loss=1.58 avg=1.70\n",
            "[946 | 899.48] loss=1.85 avg=1.70\n",
            "[947 | 900.40] loss=2.35 avg=1.71\n",
            "[948 | 901.33] loss=1.54 avg=1.71\n",
            "[949 | 902.27] loss=2.08 avg=1.71\n",
            "[950 | 903.20] loss=1.45 avg=1.71\n",
            "[951 | 904.12] loss=1.86 avg=1.71\n",
            "[952 | 905.04] loss=1.44 avg=1.71\n",
            "[953 | 905.98] loss=1.50 avg=1.71\n",
            "[954 | 906.92] loss=2.46 avg=1.71\n",
            "[955 | 907.84] loss=1.33 avg=1.71\n",
            "[956 | 908.77] loss=1.66 avg=1.71\n",
            "[957 | 909.71] loss=2.09 avg=1.71\n",
            "[958 | 910.63] loss=1.53 avg=1.71\n",
            "[959 | 911.56] loss=1.47 avg=1.71\n",
            "[960 | 912.48] loss=1.85 avg=1.71\n",
            "[961 | 913.42] loss=1.75 avg=1.71\n",
            "[962 | 914.34] loss=1.39 avg=1.71\n",
            "[963 | 915.27] loss=1.31 avg=1.70\n",
            "[964 | 916.20] loss=1.85 avg=1.71\n",
            "[965 | 917.14] loss=1.61 avg=1.71\n",
            "[966 | 918.07] loss=1.18 avg=1.70\n",
            "[967 | 918.98] loss=1.51 avg=1.70\n",
            "[968 | 919.91] loss=1.58 avg=1.70\n",
            "[969 | 920.84] loss=1.28 avg=1.69\n",
            "[970 | 921.77] loss=1.95 avg=1.70\n",
            "[971 | 922.69] loss=1.59 avg=1.69\n",
            "[972 | 923.62] loss=2.24 avg=1.70\n",
            "[973 | 924.54] loss=1.90 avg=1.70\n",
            "[974 | 925.48] loss=1.89 avg=1.70\n",
            "[975 | 926.41] loss=1.07 avg=1.70\n",
            "[976 | 927.33] loss=1.51 avg=1.70\n",
            "[977 | 928.26] loss=1.66 avg=1.69\n",
            "[978 | 929.20] loss=1.97 avg=1.70\n",
            "[979 | 930.13] loss=1.57 avg=1.70\n",
            "[980 | 931.06] loss=1.00 avg=1.69\n",
            "[981 | 931.99] loss=1.87 avg=1.69\n",
            "[982 | 932.92] loss=1.54 avg=1.69\n",
            "[983 | 933.85] loss=1.50 avg=1.69\n",
            "[984 | 934.78] loss=1.60 avg=1.69\n",
            "[985 | 935.71] loss=1.64 avg=1.69\n",
            "[986 | 936.65] loss=1.70 avg=1.69\n",
            "[987 | 937.58] loss=1.99 avg=1.69\n",
            "[988 | 938.51] loss=1.74 avg=1.69\n",
            "[989 | 939.44] loss=1.51 avg=1.69\n",
            "[990 | 940.38] loss=3.64 avg=1.71\n",
            "[991 | 941.30] loss=1.82 avg=1.71\n",
            "[992 | 942.22] loss=1.98 avg=1.71\n",
            "[993 | 943.14] loss=1.34 avg=1.71\n",
            "[994 | 944.07] loss=1.41 avg=1.70\n",
            "[995 | 945.00] loss=2.31 avg=1.71\n",
            "[996 | 945.92] loss=1.52 avg=1.71\n",
            "[997 | 946.85] loss=1.27 avg=1.70\n",
            "[998 | 947.78] loss=2.51 avg=1.71\n",
            "[999 | 948.71] loss=1.68 avg=1.71\n",
            "Saving checkpoint/run1/model-1000\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "12:59 a.m.] Kenia Porcell Privado: Jefe el gobierno del iba de miembrica de iba de la primera periodista\n",
            "[03/10/18 10:48:12 a.m.] Kenia Porcell Privado: Y me habia bien\n",
            "[03/10/18 10:48:19 a.m.] Kenia Porcell Privado: Y haya a un nombre es nuestras una tete enfrentaciones\n",
            "[03/10/18 11:04:00 a.m.] Jj: Asi es\n",
            "[03/10/18 11:05:11 a.m.] Jj: En la fuese orgla de la familia hacerlo\n",
            "[03/10/18 12:23:10 p.m.] Kenia Porcell Privado: Mire la vista. La fuese es buenigosas nivel de juntas\n",
            "[03/10/18 12:22:30 p.m.] Kenia Porcell Privado: Pero lo ser mensaje, le dije y envió\n",
            "[03/10/18 12:22:39 p.m.] Kenia Porcell Privado: Solo estradatos xq fuerte es importante\n",
            "[03/10/18 12:23:08 p.m.] Kenia Porcell Privado: Sii\n",
            "[03/10/18 1:05:27 p.m.] Jj: Ese dejar a la nombre\n",
            "[03/10/18 2:10:43 p.m.] Kenia Porcell Privado: Así es\n",
            "[03/10/18 2:28:59 p,06)\n",
            "[03/10/18 2:29:08 p,22) QUIT: Le dije de su país, q hablan debe afectar al policio\n",
            "[03/10/18 2:30:09 p,22) QUIT: Y el dije al policio\n",
            "[03/10/18 4:04:17 p.,01)\n",
            "Sensio de la República de Panamá\n",
            "[03/10/18 4:47:35 p.,04)\n",
            "\n",
            "Sensio de la República de Panamá\n",
            "[03/10/18 4:47:35 p.,02) LE DIGITALE:pdf\n",
            "[03/10/18 4:48:23 p.,09)\n",
            "EL AUNCTICA: De llega eso\n",
            "[03/10/18 4:50:37 p.,17)\n",
            "\n",
            "Español:\n",
            "[03/10/18 4:50:48 p.,17)\n",
            "[03/10/18 4:51:37 p.,15)\n",
            "Si le dije\n",
            "[03/10/18 4:52:39 p.,17)\n",
            "[04/01/18 8:12:17 a.m.] Kenia Porcell Privado: Entonces mi verdad, piensas\n",
            "[04/01/18 8:13:01 a.m.] Kenia Porcell Privado: Quiero en q vamos al frente y q le diferente a la fuese orglas pública en otros días\n",
            "[04/01/18 8:13:07 a.m.] Kenia Porcell Privado: Todo es mire q le diferente, quiero de esos comercias\n",
            "[04/01/18 8:13:12 a.m.] Kenia Porcell Privado: El mp y no sabe un comisionado de mi área\n",
            "[04/01/18 8:14:02 a.m.] Kenia Porcell Privado: El bacerita en q se hablando\n",
            "[04/01/18 8:15:03 a.m.] Kenia Porcell Privado: Lo q hacer q todos los temas\n",
            "[04/01/18 8:15:31 a.m.] Kenia Porcell Privado: Estoy la fuese que en nueva la comisionado.\n",
            "[04/01/18 8:17:13 a.m.] Kenia Porcell Privado: Mejor una pidié q señor las públicas\n",
            "[04/01/18 8:18:18 a.m.] Kenia Porcell Privado: Si q el de la fuese orgla, le dejercía me estoy en alguien\n",
            "[04/01/18 8:19:12 a.m.] Jj: Quedo con eso\n",
            "\n",
            "[1000 | 969.73] loss=1.60 avg=1.71\n",
            "[1001 | 970.66] loss=1.69 avg=1.71\n",
            "[1002 | 971.58] loss=1.53 avg=1.71\n",
            "[1003 | 972.51] loss=1.74 avg=1.71\n",
            "[1004 | 973.44] loss=1.38 avg=1.71\n",
            "[1005 | 974.37] loss=1.87 avg=1.71\n",
            "[1006 | 975.30] loss=2.11 avg=1.71\n",
            "[1007 | 976.23] loss=1.60 avg=1.71\n",
            "[1008 | 977.17] loss=1.13 avg=1.70\n",
            "[1009 | 978.09] loss=1.82 avg=1.71\n",
            "[1010 | 979.02] loss=1.81 avg=1.71\n",
            "[1011 | 979.95] loss=1.75 avg=1.71\n",
            "[1012 | 980.88] loss=2.16 avg=1.71\n",
            "[1013 | 981.80] loss=1.76 avg=1.71\n",
            "[1014 | 982.73] loss=1.65 avg=1.71\n",
            "[1015 | 983.66] loss=1.72 avg=1.71\n",
            "[1016 | 984.60] loss=1.68 avg=1.71\n",
            "[1017 | 985.53] loss=1.60 avg=1.71\n",
            "[1018 | 986.46] loss=2.08 avg=1.71\n",
            "[1019 | 987.40] loss=1.76 avg=1.71\n",
            "[1020 | 988.33] loss=1.76 avg=1.72\n",
            "[1021 | 989.26] loss=1.30 avg=1.71\n",
            "[1022 | 990.19] loss=1.74 avg=1.71\n",
            "[1023 | 991.12] loss=1.41 avg=1.71\n",
            "[1024 | 992.05] loss=1.69 avg=1.71\n",
            "[1025 | 992.98] loss=1.57 avg=1.71\n",
            "[1026 | 993.91] loss=1.65 avg=1.71\n",
            "[1027 | 994.83] loss=1.48 avg=1.70\n",
            "[1028 | 995.77] loss=1.72 avg=1.70\n",
            "[1029 | 996.71] loss=1.88 avg=1.71\n",
            "[1030 | 997.64] loss=1.66 avg=1.71\n",
            "[1031 | 998.57] loss=1.76 avg=1.71\n",
            "[1032 | 999.50] loss=2.14 avg=1.71\n",
            "[1033 | 1000.43] loss=1.93 avg=1.71\n",
            "[1034 | 1001.36] loss=1.70 avg=1.71\n",
            "[1035 | 1002.28] loss=1.61 avg=1.71\n",
            "[1036 | 1003.21] loss=1.29 avg=1.71\n",
            "[1037 | 1004.14] loss=1.49 avg=1.71\n",
            "[1038 | 1005.07] loss=1.87 avg=1.71\n",
            "[1039 | 1006.00] loss=1.30 avg=1.70\n",
            "[1040 | 1006.93] loss=1.49 avg=1.70\n",
            "[1041 | 1007.86] loss=1.30 avg=1.70\n",
            "[1042 | 1008.79] loss=1.49 avg=1.69\n",
            "[1043 | 1009.71] loss=1.34 avg=1.69\n",
            "[1044 | 1010.64] loss=1.26 avg=1.69\n",
            "[1045 | 1011.56] loss=1.25 avg=1.68\n",
            "[1046 | 1012.49] loss=1.49 avg=1.68\n",
            "[1047 | 1013.40] loss=1.38 avg=1.68\n",
            "[1048 | 1014.34] loss=1.36 avg=1.67\n",
            "[1049 | 1015.28] loss=1.27 avg=1.67\n",
            "[1050 | 1016.21] loss=1.45 avg=1.67\n",
            "[1051 | 1017.13] loss=1.97 avg=1.67\n",
            "[1052 | 1018.05] loss=1.51 avg=1.67\n",
            "[1053 | 1018.99] loss=1.41 avg=1.67\n",
            "[1054 | 1019.91] loss=1.28 avg=1.66\n",
            "[1055 | 1020.85] loss=1.06 avg=1.66\n",
            "[1056 | 1021.77] loss=3.50 avg=1.68\n",
            "[1057 | 1022.71] loss=1.43 avg=1.67\n",
            "[1058 | 1023.63] loss=1.32 avg=1.67\n",
            "[1059 | 1024.56] loss=1.99 avg=1.67\n",
            "[1060 | 1025.48] loss=1.83 avg=1.67\n",
            "[1061 | 1026.42] loss=1.50 avg=1.67\n",
            "[1062 | 1027.33] loss=1.34 avg=1.67\n",
            "[1063 | 1028.27] loss=1.41 avg=1.67\n",
            "[1064 | 1029.20] loss=1.19 avg=1.66\n",
            "[1065 | 1030.13] loss=1.49 avg=1.66\n",
            "[1066 | 1031.06] loss=1.74 avg=1.66\n",
            "[1067 | 1032.00] loss=1.81 avg=1.66\n",
            "[1068 | 1032.92] loss=1.93 avg=1.66\n",
            "[1069 | 1033.85] loss=1.26 avg=1.66\n",
            "[1070 | 1034.78] loss=1.74 avg=1.66\n",
            "[1071 | 1035.70] loss=1.27 avg=1.66\n",
            "[1072 | 1036.63] loss=1.63 avg=1.66\n",
            "[1073 | 1037.56] loss=1.50 avg=1.66\n",
            "[1074 | 1038.49] loss=1.54 avg=1.65\n",
            "[1075 | 1039.42] loss=1.55 avg=1.65\n",
            "[1076 | 1040.34] loss=1.53 avg=1.65\n",
            "[1077 | 1041.28] loss=1.49 avg=1.65\n",
            "[1078 | 1042.20] loss=1.48 avg=1.65\n",
            "[1079 | 1043.13] loss=0.95 avg=1.64\n",
            "[1080 | 1044.06] loss=1.82 avg=1.64\n",
            "[1081 | 1045.00] loss=1.72 avg=1.64\n",
            "[1082 | 1045.92] loss=2.39 avg=1.65\n",
            "[1083 | 1046.85] loss=1.56 avg=1.65\n",
            "[1084 | 1047.78] loss=1.59 avg=1.65\n",
            "[1085 | 1048.70] loss=2.76 avg=1.66\n",
            "[1086 | 1049.63] loss=1.53 avg=1.66\n",
            "[1087 | 1050.56] loss=1.74 avg=1.66\n",
            "[1088 | 1051.50] loss=2.10 avg=1.67\n",
            "[1089 | 1052.43] loss=1.52 avg=1.66\n",
            "[1090 | 1053.36] loss=1.42 avg=1.66\n",
            "[1091 | 1054.28] loss=1.32 avg=1.66\n",
            "[1092 | 1055.21] loss=1.68 avg=1.66\n",
            "[1093 | 1056.14] loss=1.53 avg=1.66\n",
            "[1094 | 1057.07] loss=1.34 avg=1.65\n",
            "[1095 | 1058.00] loss=1.34 avg=1.65\n",
            "[1096 | 1058.93] loss=1.45 avg=1.65\n",
            "[1097 | 1059.86] loss=1.49 avg=1.65\n",
            "[1098 | 1060.79] loss=1.56 avg=1.65\n",
            "[1099 | 1061.71] loss=1.34 avg=1.64\n",
            "[1100 | 1062.64] loss=1.48 avg=1.64\n",
            "[1101 | 1063.57] loss=1.60 avg=1.64\n",
            "[1102 | 1064.50] loss=1.65 avg=1.64\n",
            "[1103 | 1065.43] loss=1.63 avg=1.64\n",
            "[1104 | 1066.36] loss=1.31 avg=1.64\n",
            "[1105 | 1067.30] loss=1.43 avg=1.64\n",
            "[1106 | 1068.23] loss=1.87 avg=1.64\n",
            "[1107 | 1069.17] loss=2.04 avg=1.64\n",
            "[1108 | 1070.11] loss=1.70 avg=1.64\n",
            "[1109 | 1071.04] loss=1.34 avg=1.64\n",
            "[1110 | 1071.97] loss=1.50 avg=1.64\n",
            "[1111 | 1072.90] loss=1.52 avg=1.64\n",
            "[1112 | 1073.83] loss=1.64 avg=1.64\n",
            "[1113 | 1074.75] loss=1.21 avg=1.63\n",
            "[1114 | 1075.68] loss=1.79 avg=1.63\n",
            "[1115 | 1076.61] loss=1.98 avg=1.64\n",
            "[1116 | 1077.53] loss=1.27 avg=1.63\n",
            "[1117 | 1078.47] loss=1.48 avg=1.63\n",
            "[1118 | 1079.39] loss=1.93 avg=1.64\n",
            "[1119 | 1080.32] loss=1.88 avg=1.64\n",
            "[1120 | 1081.24] loss=1.49 avg=1.64\n",
            "[1121 | 1082.17] loss=1.49 avg=1.64\n",
            "[1122 | 1083.10] loss=1.89 avg=1.64\n",
            "[1123 | 1084.04] loss=1.40 avg=1.64\n",
            "[1124 | 1084.96] loss=1.14 avg=1.63\n",
            "[1125 | 1085.90] loss=1.66 avg=1.63\n",
            "[1126 | 1086.83] loss=1.39 avg=1.63\n",
            "[1127 | 1087.76] loss=2.16 avg=1.63\n",
            "[1128 | 1088.69] loss=2.12 avg=1.64\n",
            "[1129 | 1089.62] loss=1.36 avg=1.64\n",
            "[1130 | 1090.55] loss=1.56 avg=1.63\n",
            "[1131 | 1091.48] loss=1.24 avg=1.63\n",
            "[1132 | 1092.41] loss=1.56 avg=1.63\n",
            "[1133 | 1093.34] loss=2.25 avg=1.64\n",
            "[1134 | 1094.27] loss=1.58 avg=1.64\n",
            "[1135 | 1095.20] loss=1.83 avg=1.64\n",
            "[1136 | 1096.12] loss=1.80 avg=1.64\n",
            "[1137 | 1097.06] loss=1.61 avg=1.64\n",
            "[1138 | 1097.99] loss=1.38 avg=1.64\n",
            "[1139 | 1098.92] loss=1.40 avg=1.63\n",
            "[1140 | 1099.84] loss=1.19 avg=1.63\n",
            "[1141 | 1100.78] loss=1.46 avg=1.63\n",
            "[1142 | 1101.71] loss=1.72 avg=1.63\n",
            "[1143 | 1102.63] loss=1.50 avg=1.63\n",
            "[1144 | 1103.55] loss=3.40 avg=1.65\n",
            "[1145 | 1104.48] loss=1.11 avg=1.64\n",
            "[1146 | 1105.41] loss=1.51 avg=1.64\n",
            "[1147 | 1106.34] loss=1.58 avg=1.64\n",
            "[1148 | 1107.27] loss=2.56 avg=1.65\n",
            "[1149 | 1108.21] loss=1.51 avg=1.65\n",
            "[1150 | 1109.13] loss=1.29 avg=1.64\n",
            "[1151 | 1110.06] loss=1.31 avg=1.64\n",
            "[1152 | 1110.98] loss=1.55 avg=1.64\n",
            "[1153 | 1111.92] loss=1.74 avg=1.64\n",
            "[1154 | 1112.84] loss=2.34 avg=1.65\n",
            "[1155 | 1113.77] loss=1.35 avg=1.64\n",
            "[1156 | 1114.70] loss=1.79 avg=1.64\n",
            "[1157 | 1115.63] loss=1.18 avg=1.64\n",
            "[1158 | 1116.56] loss=1.96 avg=1.64\n",
            "[1159 | 1117.49] loss=2.25 avg=1.65\n",
            "[1160 | 1118.42] loss=1.40 avg=1.65\n",
            "[1161 | 1119.34] loss=1.53 avg=1.65\n",
            "[1162 | 1120.27] loss=2.65 avg=1.66\n",
            "[1163 | 1121.20] loss=1.92 avg=1.66\n",
            "[1164 | 1122.13] loss=2.32 avg=1.67\n",
            "[1165 | 1123.07] loss=2.14 avg=1.67\n",
            "[1166 | 1124.00] loss=1.34 avg=1.67\n",
            "[1167 | 1124.93] loss=1.66 avg=1.67\n",
            "[1168 | 1125.85] loss=2.15 avg=1.67\n",
            "[1169 | 1126.79] loss=1.43 avg=1.67\n",
            "[1170 | 1127.71] loss=1.38 avg=1.67\n",
            "[1171 | 1128.64] loss=2.66 avg=1.68\n",
            "[1172 | 1129.57] loss=1.38 avg=1.67\n",
            "[1173 | 1130.50] loss=1.75 avg=1.67\n",
            "[1174 | 1131.43] loss=1.31 avg=1.67\n",
            "[1175 | 1132.35] loss=2.10 avg=1.67\n",
            "[1176 | 1133.28] loss=1.27 avg=1.67\n",
            "[1177 | 1134.20] loss=1.86 avg=1.67\n",
            "[1178 | 1135.13] loss=1.45 avg=1.67\n",
            "[1179 | 1136.06] loss=2.62 avg=1.68\n",
            "[1180 | 1136.99] loss=1.93 avg=1.68\n",
            "[1181 | 1137.92] loss=1.56 avg=1.68\n",
            "[1182 | 1138.85] loss=1.39 avg=1.68\n",
            "[1183 | 1139.77] loss=1.96 avg=1.68\n",
            "[1184 | 1140.70] loss=1.45 avg=1.68\n",
            "[1185 | 1141.63] loss=2.14 avg=1.68\n",
            "[1186 | 1142.56] loss=1.81 avg=1.68\n",
            "[1187 | 1143.50] loss=1.70 avg=1.68\n",
            "[1188 | 1144.43] loss=1.46 avg=1.68\n",
            "[1189 | 1145.35] loss=1.44 avg=1.68\n",
            "[1190 | 1146.28] loss=1.32 avg=1.68\n",
            "[1191 | 1147.21] loss=1.80 avg=1.68\n",
            "[1192 | 1148.13] loss=1.35 avg=1.67\n",
            "[1193 | 1149.06] loss=1.65 avg=1.67\n",
            "[1194 | 1150.00] loss=1.86 avg=1.68\n",
            "[1195 | 1150.92] loss=1.72 avg=1.68\n",
            "[1196 | 1151.85] loss=2.30 avg=1.68\n",
            "[1197 | 1152.78] loss=1.53 avg=1.68\n",
            "[1198 | 1153.71] loss=1.59 avg=1.68\n",
            "[1199 | 1154.64] loss=1.74 avg=1.68\n",
            "[1200 | 1155.57] loss=1.82 avg=1.68\n",
            "[1201 | 1156.50] loss=2.23 avg=1.69\n",
            "[1202 | 1157.43] loss=2.06 avg=1.69\n",
            "[1203 | 1158.36] loss=2.05 avg=1.69\n",
            "[1204 | 1159.29] loss=1.60 avg=1.69\n",
            "[1205 | 1160.21] loss=1.60 avg=1.69\n",
            "[1206 | 1161.15] loss=3.28 avg=1.71\n",
            "[1207 | 1162.07] loss=2.96 avg=1.72\n",
            "[1208 | 1163.00] loss=1.55 avg=1.72\n",
            "[1209 | 1163.93] loss=1.39 avg=1.72\n",
            "[1210 | 1164.86] loss=1.40 avg=1.71\n",
            "[1211 | 1165.79] loss=1.87 avg=1.71\n",
            "[1212 | 1166.72] loss=1.52 avg=1.71\n",
            "[1213 | 1167.64] loss=1.77 avg=1.71\n",
            "[1214 | 1168.58] loss=1.30 avg=1.71\n",
            "[1215 | 1169.50] loss=1.30 avg=1.71\n",
            "[1216 | 1170.43] loss=1.60 avg=1.70\n",
            "[1217 | 1171.36] loss=1.48 avg=1.70\n",
            "[1218 | 1172.30] loss=1.52 avg=1.70\n",
            "[1219 | 1173.23] loss=1.37 avg=1.70\n",
            "[1220 | 1174.16] loss=1.35 avg=1.69\n",
            "[1221 | 1175.09] loss=1.53 avg=1.69\n",
            "[1222 | 1176.03] loss=1.67 avg=1.69\n",
            "[1223 | 1176.95] loss=2.04 avg=1.70\n",
            "[1224 | 1177.89] loss=1.21 avg=1.69\n",
            "[1225 | 1178.81] loss=1.55 avg=1.69\n",
            "[1226 | 1179.75] loss=1.93 avg=1.69\n",
            "[1227 | 1180.68] loss=1.29 avg=1.69\n",
            "[1228 | 1181.61] loss=1.51 avg=1.69\n",
            "[1229 | 1182.54] loss=1.32 avg=1.68\n",
            "[1230 | 1183.47] loss=1.83 avg=1.68\n",
            "[1231 | 1184.39] loss=1.14 avg=1.68\n",
            "[1232 | 1185.32] loss=1.49 avg=1.68\n",
            "[1233 | 1186.24] loss=1.22 avg=1.67\n",
            "[1234 | 1187.18] loss=1.48 avg=1.67\n",
            "[1235 | 1188.10] loss=1.56 avg=1.67\n",
            "[1236 | 1189.03] loss=2.72 avg=1.68\n",
            "[1237 | 1189.96] loss=1.56 avg=1.68\n",
            "[1238 | 1190.89] loss=1.38 avg=1.67\n",
            "[1239 | 1191.82] loss=2.01 avg=1.68\n",
            "[1240 | 1192.75] loss=1.26 avg=1.67\n",
            "[1241 | 1193.68] loss=1.04 avg=1.67\n",
            "[1242 | 1194.62] loss=1.49 avg=1.67\n",
            "[1243 | 1195.54] loss=1.63 avg=1.67\n",
            "[1244 | 1196.48] loss=1.63 avg=1.67\n",
            "[1245 | 1197.40] loss=1.82 avg=1.67\n",
            "[1246 | 1198.34] loss=1.33 avg=1.66\n",
            "[1247 | 1199.27] loss=1.42 avg=1.66\n",
            "[1248 | 1200.20] loss=1.67 avg=1.66\n",
            "[1249 | 1201.13] loss=1.31 avg=1.66\n",
            "[1250 | 1202.06] loss=1.58 avg=1.66\n",
            "[1251 | 1202.99] loss=1.53 avg=1.66\n",
            "[1252 | 1203.92] loss=2.02 avg=1.66\n",
            "[1253 | 1204.85] loss=1.56 avg=1.66\n",
            "[1254 | 1205.78] loss=1.84 avg=1.66\n",
            "[1255 | 1206.71] loss=1.33 avg=1.66\n",
            "[1256 | 1207.65] loss=1.66 avg=1.66\n",
            "[1257 | 1208.57] loss=1.44 avg=1.65\n",
            "[1258 | 1209.50] loss=1.46 avg=1.65\n",
            "[1259 | 1210.43] loss=1.63 avg=1.65\n",
            "[1260 | 1211.36] loss=2.07 avg=1.66\n",
            "[1261 | 1212.29] loss=1.97 avg=1.66\n",
            "[1262 | 1213.21] loss=1.20 avg=1.66\n",
            "[1263 | 1214.15] loss=1.22 avg=1.65\n",
            "[1264 | 1215.07] loss=1.51 avg=1.65\n",
            "[1265 | 1215.99] loss=1.35 avg=1.65\n",
            "[1266 | 1216.91] loss=1.88 avg=1.65\n",
            "[1267 | 1217.85] loss=1.71 avg=1.65\n",
            "[1268 | 1218.78] loss=2.92 avg=1.66\n",
            "[1269 | 1219.71] loss=1.49 avg=1.66\n",
            "[1270 | 1220.64] loss=2.14 avg=1.67\n",
            "[1271 | 1221.57] loss=1.54 avg=1.66\n",
            "[1272 | 1222.51] loss=1.41 avg=1.66\n",
            "[1273 | 1223.43] loss=1.85 avg=1.66\n",
            "[1274 | 1224.37] loss=1.63 avg=1.66\n",
            "[1275 | 1225.30] loss=1.43 avg=1.66\n",
            "[1276 | 1226.23] loss=1.71 avg=1.66\n",
            "[1277 | 1227.16] loss=2.37 avg=1.67\n",
            "[1278 | 1228.10] loss=1.69 avg=1.67\n",
            "[1279 | 1229.03] loss=1.37 avg=1.67\n",
            "[1280 | 1229.96] loss=1.53 avg=1.66\n",
            "[1281 | 1230.88] loss=1.61 avg=1.66\n",
            "[1282 | 1231.81] loss=2.00 avg=1.67\n",
            "[1283 | 1232.74] loss=1.55 avg=1.67\n",
            "[1284 | 1233.67] loss=1.43 avg=1.66\n",
            "[1285 | 1234.59] loss=1.96 avg=1.67\n",
            "[1286 | 1235.52] loss=1.12 avg=1.66\n",
            "[1287 | 1236.44] loss=1.57 avg=1.66\n",
            "[1288 | 1237.38] loss=1.64 avg=1.66\n",
            "[1289 | 1238.30] loss=1.64 avg=1.66\n",
            "[1290 | 1239.23] loss=1.46 avg=1.66\n",
            "[1291 | 1240.15] loss=2.12 avg=1.66\n",
            "[1292 | 1241.08] loss=1.38 avg=1.66\n",
            "[1293 | 1242.01] loss=1.54 avg=1.66\n",
            "[1294 | 1242.93] loss=2.17 avg=1.66\n",
            "[1295 | 1243.86] loss=1.46 avg=1.66\n",
            "[1296 | 1244.79] loss=1.99 avg=1.66\n",
            "[1297 | 1245.72] loss=1.30 avg=1.66\n",
            "[1298 | 1246.65] loss=2.05 avg=1.66\n",
            "[1299 | 1247.57] loss=1.26 avg=1.66\n",
            "[1300 | 1248.49] loss=1.51 avg=1.66\n",
            "[1301 | 1249.42] loss=1.42 avg=1.66\n",
            "[1302 | 1250.35] loss=1.54 avg=1.66\n",
            "[1303 | 1251.28] loss=1.39 avg=1.65\n",
            "[1304 | 1252.21] loss=1.31 avg=1.65\n",
            "[1305 | 1253.14] loss=1.99 avg=1.65\n",
            "[1306 | 1254.07] loss=1.58 avg=1.65\n",
            "[1307 | 1255.00] loss=1.63 avg=1.65\n",
            "[1308 | 1255.93] loss=1.63 avg=1.65\n",
            "[1309 | 1256.86] loss=1.19 avg=1.65\n",
            "[1310 | 1257.79] loss=2.00 avg=1.65\n",
            "[1311 | 1258.72] loss=1.21 avg=1.65\n",
            "[1312 | 1259.65] loss=1.84 avg=1.65\n",
            "[1313 | 1260.58] loss=1.55 avg=1.65\n",
            "[1314 | 1261.50] loss=1.31 avg=1.64\n",
            "[1315 | 1262.43] loss=1.39 avg=1.64\n",
            "[1316 | 1263.35] loss=1.60 avg=1.64\n",
            "[1317 | 1264.28] loss=1.41 avg=1.64\n",
            "[1318 | 1265.22] loss=1.35 avg=1.64\n",
            "[1319 | 1266.15] loss=2.05 avg=1.64\n",
            "[1320 | 1267.08] loss=1.45 avg=1.64\n",
            "[1321 | 1268.02] loss=1.55 avg=1.64\n",
            "[1322 | 1268.94] loss=1.74 avg=1.64\n",
            "[1323 | 1269.88] loss=1.55 avg=1.64\n",
            "[1324 | 1270.80] loss=2.01 avg=1.64\n",
            "[1325 | 1271.73] loss=1.65 avg=1.64\n",
            "[1326 | 1272.66] loss=1.28 avg=1.64\n",
            "[1327 | 1273.59] loss=1.43 avg=1.64\n",
            "[1328 | 1274.52] loss=2.41 avg=1.64\n",
            "[1329 | 1275.44] loss=1.48 avg=1.64\n",
            "[1330 | 1276.37] loss=1.66 avg=1.64\n",
            "[1331 | 1277.30] loss=1.30 avg=1.64\n",
            "[1332 | 1278.23] loss=1.21 avg=1.63\n",
            "[1333 | 1279.16] loss=2.02 avg=1.64\n",
            "[1334 | 1280.09] loss=1.47 avg=1.64\n",
            "[1335 | 1281.01] loss=1.80 avg=1.64\n",
            "[1336 | 1281.95] loss=1.70 avg=1.64\n",
            "[1337 | 1282.87] loss=1.63 avg=1.64\n",
            "[1338 | 1283.79] loss=2.52 avg=1.65\n",
            "[1339 | 1284.71] loss=1.60 avg=1.65\n",
            "[1340 | 1285.65] loss=1.44 avg=1.64\n",
            "[1341 | 1286.58] loss=1.11 avg=1.64\n",
            "[1342 | 1287.51] loss=2.33 avg=1.65\n",
            "[1343 | 1288.44] loss=1.51 avg=1.64\n",
            "[1344 | 1289.37] loss=2.36 avg=1.65\n",
            "[1345 | 1290.30] loss=2.12 avg=1.66\n",
            "[1346 | 1291.23] loss=1.71 avg=1.66\n",
            "[1347 | 1292.16] loss=2.38 avg=1.66\n",
            "[1348 | 1293.09] loss=1.42 avg=1.66\n",
            "[1349 | 1294.02] loss=2.01 avg=1.67\n",
            "[1350 | 1294.95] loss=1.56 avg=1.66\n",
            "[1351 | 1295.88] loss=1.39 avg=1.66\n",
            "[1352 | 1296.81] loss=2.05 avg=1.67\n",
            "[1353 | 1297.74] loss=1.52 avg=1.66\n",
            "[1354 | 1298.67] loss=1.37 avg=1.66\n",
            "[1355 | 1299.60] loss=1.37 avg=1.66\n",
            "[1356 | 1300.52] loss=1.47 avg=1.66\n",
            "[1357 | 1301.46] loss=1.44 avg=1.65\n",
            "[1358 | 1302.39] loss=2.37 avg=1.66\n",
            "[1359 | 1303.31] loss=1.44 avg=1.66\n",
            "[1360 | 1304.23] loss=1.01 avg=1.65\n",
            "[1361 | 1305.16] loss=1.72 avg=1.65\n",
            "[1362 | 1306.09] loss=1.85 avg=1.66\n",
            "[1363 | 1307.01] loss=1.21 avg=1.65\n",
            "[1364 | 1307.93] loss=1.69 avg=1.65\n",
            "[1365 | 1308.85] loss=1.39 avg=1.65\n",
            "[1366 | 1309.77] loss=1.67 avg=1.65\n",
            "[1367 | 1310.70] loss=1.40 avg=1.65\n",
            "[1368 | 1311.62] loss=1.42 avg=1.64\n",
            "[1369 | 1312.55] loss=1.10 avg=1.64\n",
            "[1370 | 1313.47] loss=1.56 avg=1.64\n",
            "[1371 | 1314.39] loss=1.62 avg=1.64\n",
            "[1372 | 1315.31] loss=1.58 avg=1.64\n",
            "[1373 | 1316.24] loss=1.44 avg=1.63\n",
            "[1374 | 1317.17] loss=1.52 avg=1.63\n",
            "[1375 | 1318.08] loss=3.12 avg=1.65\n",
            "[1376 | 1319.00] loss=1.43 avg=1.65\n",
            "[1377 | 1319.93] loss=1.34 avg=1.64\n",
            "[1378 | 1320.87] loss=2.10 avg=1.65\n",
            "[1379 | 1321.79] loss=1.27 avg=1.64\n",
            "[1380 | 1322.71] loss=1.52 avg=1.64\n",
            "[1381 | 1323.64] loss=1.56 avg=1.64\n",
            "[1382 | 1324.56] loss=1.47 avg=1.64\n",
            "[1383 | 1325.49] loss=1.29 avg=1.64\n",
            "[1384 | 1326.41] loss=1.91 avg=1.64\n",
            "[1385 | 1327.33] loss=2.25 avg=1.65\n",
            "[1386 | 1328.26] loss=1.43 avg=1.64\n",
            "[1387 | 1329.18] loss=1.60 avg=1.64\n",
            "[1388 | 1330.10] loss=1.44 avg=1.64\n",
            "[1389 | 1331.03] loss=1.39 avg=1.64\n",
            "[1390 | 1331.96] loss=1.85 avg=1.64\n",
            "[1391 | 1332.88] loss=1.50 avg=1.64\n",
            "[1392 | 1333.81] loss=1.55 avg=1.64\n",
            "[1393 | 1334.75] loss=1.83 avg=1.64\n",
            "[1394 | 1335.68] loss=1.46 avg=1.64\n",
            "[1395 | 1336.61] loss=1.83 avg=1.64\n",
            "[1396 | 1337.54] loss=1.86 avg=1.64\n",
            "[1397 | 1338.47] loss=1.66 avg=1.64\n",
            "[1398 | 1339.39] loss=1.44 avg=1.64\n",
            "[1399 | 1340.33] loss=1.62 avg=1.64\n",
            "[1400 | 1341.25] loss=1.38 avg=1.64\n",
            "[1401 | 1342.19] loss=0.88 avg=1.63\n",
            "[1402 | 1343.12] loss=1.75 avg=1.63\n",
            "[1403 | 1344.05] loss=2.85 avg=1.64\n",
            "[1404 | 1344.99] loss=1.77 avg=1.65\n",
            "[1405 | 1345.91] loss=1.32 avg=1.64\n",
            "[1406 | 1346.85] loss=2.58 avg=1.65\n",
            "[1407 | 1347.79] loss=2.13 avg=1.66\n",
            "[1408 | 1348.72] loss=1.40 avg=1.65\n",
            "[1409 | 1349.64] loss=1.62 avg=1.65\n",
            "[1410 | 1350.57] loss=2.01 avg=1.66\n",
            "[1411 | 1351.51] loss=1.81 avg=1.66\n",
            "[1412 | 1352.44] loss=1.98 avg=1.66\n",
            "[1413 | 1353.36] loss=1.96 avg=1.66\n",
            "[1414 | 1354.29] loss=1.32 avg=1.66\n",
            "[1415 | 1355.22] loss=1.76 avg=1.66\n",
            "[1416 | 1356.15] loss=1.35 avg=1.66\n",
            "[1417 | 1357.07] loss=1.54 avg=1.66\n",
            "[1418 | 1358.00] loss=1.45 avg=1.66\n",
            "[1419 | 1358.93] loss=1.84 avg=1.66\n",
            "[1420 | 1359.86] loss=1.59 avg=1.66\n",
            "[1421 | 1360.78] loss=1.23 avg=1.65\n",
            "[1422 | 1361.70] loss=1.49 avg=1.65\n",
            "[1423 | 1362.63] loss=1.73 avg=1.65\n",
            "[1424 | 1363.57] loss=1.55 avg=1.65\n",
            "[1425 | 1364.50] loss=1.73 avg=1.65\n",
            "[1426 | 1365.42] loss=1.56 avg=1.65\n",
            "[1427 | 1366.35] loss=1.52 avg=1.65\n",
            "[1428 | 1367.27] loss=1.14 avg=1.64\n",
            "[1429 | 1368.20] loss=1.59 avg=1.64\n",
            "[1430 | 1369.12] loss=1.85 avg=1.65\n",
            "[1431 | 1370.04] loss=1.56 avg=1.64\n",
            "[1432 | 1370.97] loss=1.31 avg=1.64\n",
            "[1433 | 1371.90] loss=1.77 avg=1.64\n",
            "[1434 | 1372.82] loss=1.31 avg=1.64\n",
            "[1435 | 1373.74] loss=2.06 avg=1.64\n",
            "[1436 | 1374.68] loss=1.39 avg=1.64\n",
            "[1437 | 1375.60] loss=1.20 avg=1.64\n",
            "[1438 | 1376.53] loss=1.54 avg=1.64\n",
            "[1439 | 1377.46] loss=1.59 avg=1.64\n",
            "[1440 | 1378.38] loss=2.27 avg=1.64\n",
            "[1441 | 1379.30] loss=1.27 avg=1.64\n",
            "[1442 | 1380.23] loss=2.25 avg=1.64\n",
            "[1443 | 1381.16] loss=1.35 avg=1.64\n",
            "[1444 | 1382.09] loss=1.64 avg=1.64\n",
            "[1445 | 1383.03] loss=1.67 avg=1.64\n",
            "[1446 | 1383.94] loss=1.53 avg=1.64\n",
            "[1447 | 1384.87] loss=1.53 avg=1.64\n",
            "[1448 | 1385.80] loss=1.49 avg=1.64\n",
            "[1449 | 1386.73] loss=1.36 avg=1.63\n",
            "[1450 | 1387.66] loss=1.98 avg=1.64\n",
            "[1451 | 1388.58] loss=1.37 avg=1.64\n",
            "[1452 | 1389.52] loss=1.55 avg=1.63\n",
            "[1453 | 1390.44] loss=1.84 avg=1.64\n",
            "[1454 | 1391.37] loss=2.25 avg=1.64\n",
            "[1455 | 1392.30] loss=1.55 avg=1.64\n",
            "[1456 | 1393.24] loss=1.42 avg=1.64\n",
            "[1457 | 1394.15] loss=2.56 avg=1.65\n",
            "[1458 | 1395.08] loss=1.93 avg=1.65\n",
            "[1459 | 1396.00] loss=3.21 avg=1.67\n",
            "[1460 | 1396.93] loss=1.34 avg=1.66\n",
            "[1461 | 1397.85] loss=1.76 avg=1.66\n",
            "[1462 | 1398.77] loss=2.25 avg=1.67\n",
            "[1463 | 1399.70] loss=1.36 avg=1.67\n",
            "[1464 | 1400.63] loss=1.25 avg=1.66\n",
            "[1465 | 1401.56] loss=1.91 avg=1.67\n",
            "[1466 | 1402.49] loss=1.96 avg=1.67\n",
            "[1467 | 1403.39] loss=1.36 avg=1.67\n",
            "[1468 | 1404.32] loss=1.60 avg=1.67\n",
            "[1469 | 1405.26] loss=1.24 avg=1.66\n",
            "[1470 | 1406.18] loss=2.42 avg=1.67\n",
            "[1471 | 1407.11] loss=1.86 avg=1.67\n",
            "[1472 | 1408.02] loss=1.35 avg=1.67\n",
            "[1473 | 1408.95] loss=1.40 avg=1.66\n",
            "[1474 | 1409.88] loss=1.29 avg=1.66\n",
            "[1475 | 1410.82] loss=1.28 avg=1.66\n",
            "[1476 | 1411.74] loss=1.76 avg=1.66\n",
            "[1477 | 1412.67] loss=1.97 avg=1.66\n",
            "[1478 | 1413.59] loss=1.76 avg=1.66\n",
            "[1479 | 1414.52] loss=1.33 avg=1.66\n",
            "[1480 | 1415.45] loss=1.21 avg=1.65\n",
            "[1481 | 1416.38] loss=1.41 avg=1.65\n",
            "[1482 | 1417.30] loss=1.66 avg=1.65\n",
            "[1483 | 1418.23] loss=1.23 avg=1.65\n",
            "[1484 | 1419.16] loss=2.24 avg=1.65\n",
            "[1485 | 1420.08] loss=1.48 avg=1.65\n",
            "[1486 | 1421.01] loss=1.44 avg=1.65\n",
            "[1487 | 1421.94] loss=1.46 avg=1.65\n",
            "[1488 | 1422.87] loss=1.45 avg=1.65\n",
            "[1489 | 1423.79] loss=1.43 avg=1.64\n",
            "[1490 | 1424.72] loss=1.94 avg=1.65\n",
            "[1491 | 1425.65] loss=1.24 avg=1.64\n",
            "[1492 | 1426.58] loss=1.39 avg=1.64\n",
            "[1493 | 1427.51] loss=1.68 avg=1.64\n",
            "[1494 | 1428.43] loss=2.94 avg=1.65\n",
            "[1495 | 1429.37] loss=1.49 avg=1.65\n",
            "[1496 | 1430.29] loss=1.88 avg=1.65\n",
            "[1497 | 1431.22] loss=1.27 avg=1.65\n",
            "[1498 | 1432.15] loss=1.28 avg=1.65\n",
            "[1499 | 1433.08] loss=1.81 avg=1.65\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "os para q todo el mundo xq mejor.\n",
            "\n",
            "Hola y si así lo de la csj para q el mundo y todo el mundo del mundo .\n",
            "[10/02/18 8:41:16 a.m.] Kenia Porcell Privado: En el hubo de su comprendo .\n",
            "[10/02/18 8:41:20 a.m.] Kenia Porcell Privado: Usted?\n",
            "[10/02/18 8:56:19 a.m.] Jj: El mundo muy profesional, el mundo es una comida fue un gabinete\n",
            "[10/02/18 8:56:35 a.m.] Jj: Y es una casa de estas casos\n",
            "[10/02/18 8:58:13 a.m.] Kenia Porcell Privado: Le gusta\n",
            "[10/02/18 8:58:20 a.m.] Jj: La casi casita\n",
            "[10/02/18 8:58:27 a.m.] Jj: Ellos a la casa de unidad\n",
            "[10/02/18 8:58:38 a.m.] Kenia Porcell Privado: Lo q ayer no se han atención con todo el mundo y le fue en mi esposo\n",
            "[10/02/18 8:59:02 a.m.] Jj: Jajaja\n",
            "[10/02/18 8:59:29 a.m.] Jj: Los pueblo\n",
            "[10/02/18 8:59:36 a.m.] Jj: Se llegué a una comida\n",
            "[10/02/18 8:59:45 a.m.] Jj: Y siii\n",
            "[10/02/18 8:59:54 a.m.] Kenia Porcell Privado: Le gusta se la mamada. Pero sistema hoy de línea\n",
            "[10/02/18 8:59:59 a.m.] Kenia Porcell Privado: Usted. Si, usted es que le hablé\n",
            "[10/02/18 9:00:02 a.m.] Jj: Si se acaba y el ministro unas faltas\n",
            "[10/02/18 9:00:24 a.m.] Jj: Eso, tengo así\n",
            "[10/02/18 9:00:35 a.m.] Kenia Porcell Privado: Y q estara es un hizo de los abogados de la seguridad y seguía mas hombres comunicado\n",
            "[10/02/18 9:00:52 a.m.] Jj: Estancia y su posicion\n",
            "[10/02/18 9:01:25 a.m.] Jj: Si el juez si habla\n",
            "[10/02/18 9:01:43 a.m.] Kenia Porcell Privado: Cero, cómo usted, q no dieron x denuncias q se hablé\n",
            "[10/02/18 9:01:50 a.m.] Jj: El mundo también que ya no se han el de los aportes\n",
            "[10/02/18 9:02:10 a.m.] Jj: Y tiene con el ministro a los de las fonseca\n",
            "[10/02/18 9:02:16 a.m.] Kenia Porcell Privado: Sii. Cómo vivir xq el ministró q me dijo está muy importante y no es el lunes. No es que los aportes hoy si lo quiero q me habido con la casa de uní\n",
            "[10/02/18 9:02:27 a.m.] Jj: Solo a cita, no es, nada la seguridad y no es su hijo\n",
            "[10/02/18 9:02:38 a.m.] Jj: Me alegro pargo de todo\n",
            "[10/02/18 9:02:46 a.m.] Kenia Porcell Privado: Ya se lo más, más.\n",
            "[10/02/18 9:02:51 a.m.] Jj: Buenas tardes\n",
            "[10/02/18 9:03:04 a.m.] Jj: Si el hizo, bueno, de las fonsecas, que la casa de unidad\n",
            "[10/02/18 9:03:14 a.m.] Kenia Por\n",
            "\n",
            "[1500 | 1451.17] loss=1.09 avg=1.64\n",
            "[1501 | 1452.10] loss=1.51 avg=1.64\n",
            "[1502 | 1453.02] loss=1.14 avg=1.64\n",
            "[1503 | 1453.95] loss=1.43 avg=1.63\n",
            "[1504 | 1454.87] loss=1.80 avg=1.64\n",
            "[1505 | 1455.81] loss=1.74 avg=1.64\n",
            "[1506 | 1456.73] loss=1.89 avg=1.64\n",
            "[1507 | 1457.65] loss=2.48 avg=1.65\n",
            "[1508 | 1458.58] loss=1.59 avg=1.65\n",
            "[1509 | 1459.52] loss=1.58 avg=1.65\n",
            "[1510 | 1460.45] loss=1.62 avg=1.65\n",
            "[1511 | 1461.37] loss=1.97 avg=1.65\n",
            "[1512 | 1462.30] loss=1.72 avg=1.65\n",
            "[1513 | 1463.23] loss=1.36 avg=1.65\n",
            "[1514 | 1464.16] loss=1.36 avg=1.64\n",
            "[1515 | 1465.09] loss=1.04 avg=1.64\n",
            "[1516 | 1466.01] loss=1.21 avg=1.63\n",
            "[1517 | 1466.93] loss=1.84 avg=1.64\n",
            "[1518 | 1467.87] loss=1.78 avg=1.64\n",
            "[1519 | 1468.80] loss=1.18 avg=1.63\n",
            "[1520 | 1469.73] loss=1.73 avg=1.63\n",
            "[1521 | 1470.65] loss=1.12 avg=1.63\n",
            "[1522 | 1471.58] loss=1.36 avg=1.63\n",
            "[1523 | 1472.51] loss=2.54 avg=1.64\n",
            "[1524 | 1473.43] loss=1.48 avg=1.63\n",
            "[1525 | 1474.35] loss=1.45 avg=1.63\n",
            "[1526 | 1475.29] loss=1.33 avg=1.63\n",
            "[1527 | 1476.22] loss=1.51 avg=1.63\n",
            "[1528 | 1477.14] loss=1.21 avg=1.62\n",
            "[1529 | 1478.08] loss=3.04 avg=1.64\n",
            "[1530 | 1478.99] loss=1.84 avg=1.64\n",
            "[1531 | 1479.92] loss=1.72 avg=1.64\n",
            "[1532 | 1480.85] loss=1.49 avg=1.64\n",
            "[1533 | 1481.78] loss=1.47 avg=1.64\n",
            "[1534 | 1482.70] loss=1.15 avg=1.63\n",
            "[1535 | 1483.64] loss=2.67 avg=1.64\n",
            "[1536 | 1484.57] loss=1.62 avg=1.64\n",
            "[1537 | 1485.49] loss=1.73 avg=1.64\n",
            "[1538 | 1486.42] loss=1.95 avg=1.65\n",
            "[1539 | 1487.35] loss=1.52 avg=1.64\n",
            "[1540 | 1488.28] loss=1.54 avg=1.64\n",
            "[1541 | 1489.20] loss=1.57 avg=1.64\n",
            "[1542 | 1490.13] loss=1.32 avg=1.64\n",
            "[1543 | 1491.06] loss=1.55 avg=1.64\n",
            "[1544 | 1491.99] loss=1.45 avg=1.64\n",
            "[1545 | 1492.91] loss=2.49 avg=1.65\n",
            "[1546 | 1493.84] loss=1.65 avg=1.65\n",
            "[1547 | 1494.77] loss=1.59 avg=1.65\n",
            "[1548 | 1495.71] loss=1.90 avg=1.65\n",
            "[1549 | 1496.63] loss=1.33 avg=1.64\n",
            "[1550 | 1497.56] loss=1.44 avg=1.64\n",
            "[1551 | 1498.49] loss=1.36 avg=1.64\n",
            "[1552 | 1499.43] loss=1.58 avg=1.64\n",
            "[1553 | 1500.35] loss=1.59 avg=1.64\n",
            "[1554 | 1501.28] loss=1.31 avg=1.64\n",
            "[1555 | 1502.20] loss=1.44 avg=1.63\n",
            "[1556 | 1503.13] loss=1.86 avg=1.64\n",
            "[1557 | 1504.06] loss=1.18 avg=1.63\n",
            "[1558 | 1504.98] loss=1.41 avg=1.63\n",
            "[1559 | 1505.92] loss=1.31 avg=1.63\n",
            "[1560 | 1506.85] loss=1.51 avg=1.62\n",
            "[1561 | 1507.78] loss=1.90 avg=1.63\n",
            "[1562 | 1508.70] loss=1.31 avg=1.62\n",
            "[1563 | 1509.63] loss=1.49 avg=1.62\n",
            "[1564 | 1510.56] loss=1.48 avg=1.62\n",
            "[1565 | 1511.49] loss=1.67 avg=1.62\n",
            "[1566 | 1512.42] loss=1.76 avg=1.62\n",
            "[1567 | 1513.35] loss=1.37 avg=1.62\n",
            "[1568 | 1514.29] loss=1.53 avg=1.62\n",
            "[1569 | 1515.21] loss=1.43 avg=1.62\n",
            "[1570 | 1516.13] loss=1.44 avg=1.62\n",
            "[1571 | 1517.06] loss=1.50 avg=1.62\n",
            "[1572 | 1518.00] loss=1.20 avg=1.61\n",
            "[1573 | 1518.92] loss=1.31 avg=1.61\n",
            "[1574 | 1519.85] loss=1.68 avg=1.61\n",
            "[1575 | 1520.78] loss=1.87 avg=1.61\n",
            "[1576 | 1521.71] loss=2.21 avg=1.62\n",
            "[1577 | 1522.64] loss=1.51 avg=1.62\n",
            "[1578 | 1523.56] loss=1.46 avg=1.61\n",
            "[1579 | 1524.50] loss=1.71 avg=1.62\n",
            "[1580 | 1525.42] loss=1.46 avg=1.61\n",
            "[1581 | 1526.34] loss=1.45 avg=1.61\n",
            "[1582 | 1527.27] loss=1.35 avg=1.61\n",
            "[1583 | 1528.20] loss=1.36 avg=1.61\n",
            "[1584 | 1529.12] loss=1.56 avg=1.61\n",
            "[1585 | 1530.05] loss=1.72 avg=1.61\n",
            "[1586 | 1530.98] loss=1.26 avg=1.60\n",
            "[1587 | 1531.91] loss=1.53 avg=1.60\n",
            "[1588 | 1532.84] loss=1.65 avg=1.60\n",
            "[1589 | 1533.77] loss=1.69 avg=1.60\n",
            "[1590 | 1534.71] loss=1.62 avg=1.61\n",
            "[1591 | 1535.64] loss=1.50 avg=1.60\n",
            "[1592 | 1536.57] loss=1.96 avg=1.61\n",
            "[1593 | 1537.50] loss=1.56 avg=1.61\n",
            "[1594 | 1538.43] loss=1.16 avg=1.60\n",
            "[1595 | 1539.36] loss=2.38 avg=1.61\n",
            "[1596 | 1540.28] loss=1.90 avg=1.61\n",
            "[1597 | 1541.21] loss=1.51 avg=1.61\n",
            "[1598 | 1542.14] loss=1.60 avg=1.61\n",
            "[1599 | 1543.06] loss=1.60 avg=1.61\n",
            "[1600 | 1543.99] loss=1.18 avg=1.61\n",
            "[1601 | 1544.92] loss=2.17 avg=1.61\n",
            "[1602 | 1545.85] loss=1.34 avg=1.61\n",
            "[1603 | 1546.78] loss=1.34 avg=1.61\n",
            "[1604 | 1547.72] loss=1.75 avg=1.61\n",
            "[1605 | 1548.65] loss=1.37 avg=1.61\n",
            "[1606 | 1549.57] loss=1.70 avg=1.61\n",
            "[1607 | 1550.50] loss=1.61 avg=1.61\n",
            "[1608 | 1551.44] loss=1.63 avg=1.61\n",
            "[1609 | 1552.36] loss=1.34 avg=1.61\n",
            "[1610 | 1553.28] loss=1.55 avg=1.61\n",
            "[1611 | 1554.20] loss=2.71 avg=1.62\n",
            "[1612 | 1555.14] loss=1.45 avg=1.61\n",
            "[1613 | 1556.06] loss=1.43 avg=1.61\n",
            "[1614 | 1556.99] loss=1.65 avg=1.61\n",
            "[1615 | 1557.91] loss=2.11 avg=1.62\n",
            "[1616 | 1558.84] loss=1.11 avg=1.61\n",
            "[1617 | 1559.76] loss=1.77 avg=1.61\n",
            "[1618 | 1560.68] loss=1.99 avg=1.62\n",
            "[1619 | 1561.62] loss=1.78 avg=1.62\n",
            "[1620 | 1562.55] loss=1.26 avg=1.62\n",
            "[1621 | 1563.47] loss=1.36 avg=1.61\n",
            "[1622 | 1564.40] loss=1.38 avg=1.61\n",
            "[1623 | 1565.32] loss=1.53 avg=1.61\n",
            "[1624 | 1566.25] loss=1.66 avg=1.61\n",
            "[1625 | 1567.17] loss=1.43 avg=1.61\n",
            "[1626 | 1568.10] loss=2.00 avg=1.61\n",
            "[1627 | 1569.02] loss=1.63 avg=1.61\n",
            "[1628 | 1569.96] loss=1.81 avg=1.62\n",
            "[1629 | 1570.88] loss=1.39 avg=1.61\n",
            "[1630 | 1571.81] loss=1.21 avg=1.61\n",
            "[1631 | 1572.73] loss=1.78 avg=1.61\n",
            "[1632 | 1573.67] loss=1.70 avg=1.61\n",
            "[1633 | 1574.60] loss=1.38 avg=1.61\n",
            "[1634 | 1575.52] loss=1.78 avg=1.61\n",
            "[1635 | 1576.44] loss=1.65 avg=1.61\n",
            "[1636 | 1577.36] loss=1.32 avg=1.61\n",
            "[1637 | 1578.29] loss=1.52 avg=1.61\n",
            "[1638 | 1579.22] loss=1.37 avg=1.61\n",
            "[1639 | 1580.14] loss=1.50 avg=1.60\n",
            "[1640 | 1581.07] loss=1.75 avg=1.61\n",
            "[1641 | 1582.00] loss=1.48 avg=1.60\n",
            "[1642 | 1582.93] loss=1.47 avg=1.60\n",
            "[1643 | 1583.85] loss=1.61 avg=1.60\n",
            "[1644 | 1584.78] loss=1.53 avg=1.60\n",
            "[1645 | 1585.71] loss=1.40 avg=1.60\n",
            "[1646 | 1586.64] loss=1.53 avg=1.60\n",
            "[1647 | 1587.57] loss=1.83 avg=1.60\n",
            "[1648 | 1588.50] loss=1.72 avg=1.60\n",
            "[1649 | 1589.44] loss=1.27 avg=1.60\n",
            "[1650 | 1590.36] loss=1.35 avg=1.60\n",
            "[1651 | 1591.29] loss=1.61 avg=1.60\n",
            "[1652 | 1592.22] loss=1.74 avg=1.60\n",
            "[1653 | 1593.14] loss=2.03 avg=1.60\n",
            "[1654 | 1594.08] loss=1.67 avg=1.60\n",
            "[1655 | 1595.00] loss=1.51 avg=1.60\n",
            "[1656 | 1595.93] loss=1.49 avg=1.60\n",
            "[1657 | 1596.85] loss=2.23 avg=1.61\n",
            "[1658 | 1597.78] loss=1.79 avg=1.61\n",
            "[1659 | 1598.71] loss=1.54 avg=1.61\n",
            "[1660 | 1599.64] loss=1.05 avg=1.60\n",
            "[1661 | 1600.57] loss=1.81 avg=1.61\n",
            "[1662 | 1601.50] loss=1.49 avg=1.60\n",
            "[1663 | 1602.43] loss=1.59 avg=1.60\n",
            "[1664 | 1603.35] loss=1.45 avg=1.60\n",
            "[1665 | 1604.28] loss=1.62 avg=1.60\n",
            "[1666 | 1605.21] loss=1.53 avg=1.60\n",
            "[1667 | 1606.14] loss=1.32 avg=1.60\n",
            "[1668 | 1607.07] loss=1.59 avg=1.60\n",
            "[1669 | 1607.99] loss=1.77 avg=1.60\n",
            "[1670 | 1608.92] loss=1.40 avg=1.60\n",
            "[1671 | 1609.85] loss=1.48 avg=1.60\n",
            "[1672 | 1610.78] loss=1.36 avg=1.60\n",
            "[1673 | 1611.69] loss=1.37 avg=1.59\n",
            "[1674 | 1612.61] loss=1.36 avg=1.59\n",
            "[1675 | 1613.54] loss=1.68 avg=1.59\n",
            "[1676 | 1614.47] loss=1.26 avg=1.59\n",
            "[1677 | 1615.39] loss=1.41 avg=1.59\n",
            "[1678 | 1616.32] loss=1.59 avg=1.59\n",
            "[1679 | 1617.25] loss=1.56 avg=1.59\n",
            "[1680 | 1618.18] loss=3.27 avg=1.60\n",
            "[1681 | 1619.10] loss=1.33 avg=1.60\n",
            "[1682 | 1620.03] loss=1.46 avg=1.60\n",
            "[1683 | 1620.97] loss=1.72 avg=1.60\n",
            "[1684 | 1621.90] loss=1.36 avg=1.60\n",
            "[1685 | 1622.82] loss=1.15 avg=1.59\n",
            "[1686 | 1623.74] loss=1.24 avg=1.59\n",
            "[1687 | 1624.67] loss=1.64 avg=1.59\n",
            "[1688 | 1625.59] loss=1.39 avg=1.59\n",
            "[1689 | 1626.52] loss=1.65 avg=1.59\n",
            "[1690 | 1627.45] loss=1.31 avg=1.59\n",
            "[1691 | 1628.38] loss=1.49 avg=1.59\n",
            "[1692 | 1629.31] loss=1.44 avg=1.58\n",
            "[1693 | 1630.23] loss=1.64 avg=1.58\n",
            "[1694 | 1631.17] loss=1.41 avg=1.58\n",
            "[1695 | 1632.08] loss=1.44 avg=1.58\n",
            "[1696 | 1633.01] loss=1.51 avg=1.58\n",
            "[1697 | 1633.94] loss=1.57 avg=1.58\n",
            "[1698 | 1634.87] loss=1.50 avg=1.58\n",
            "[1699 | 1635.80] loss=1.28 avg=1.58\n",
            "[1700 | 1636.73] loss=1.20 avg=1.57\n",
            "[1701 | 1637.65] loss=1.61 avg=1.57\n",
            "[1702 | 1638.59] loss=1.22 avg=1.57\n",
            "[1703 | 1639.52] loss=2.16 avg=1.58\n",
            "[1704 | 1640.44] loss=1.22 avg=1.57\n",
            "[1705 | 1641.37] loss=1.60 avg=1.57\n",
            "[1706 | 1642.31] loss=1.68 avg=1.57\n",
            "[1707 | 1643.24] loss=1.37 avg=1.57\n",
            "[1708 | 1644.17] loss=1.34 avg=1.57\n",
            "[1709 | 1645.09] loss=1.69 avg=1.57\n",
            "[1710 | 1646.02] loss=1.37 avg=1.57\n",
            "[1711 | 1646.95] loss=1.88 avg=1.57\n",
            "[1712 | 1647.88] loss=1.47 avg=1.57\n",
            "[1713 | 1648.81] loss=1.46 avg=1.57\n",
            "[1714 | 1649.74] loss=2.86 avg=1.58\n",
            "[1715 | 1650.66] loss=1.69 avg=1.58\n",
            "[1716 | 1651.59] loss=1.36 avg=1.58\n",
            "[1717 | 1652.52] loss=1.67 avg=1.58\n",
            "[1718 | 1653.46] loss=1.83 avg=1.58\n",
            "[1719 | 1654.39] loss=1.75 avg=1.59\n",
            "[1720 | 1655.32] loss=1.55 avg=1.59\n",
            "[1721 | 1656.25] loss=1.64 avg=1.59\n",
            "[1722 | 1657.18] loss=1.51 avg=1.59\n",
            "[1723 | 1658.11] loss=1.41 avg=1.58\n",
            "[1724 | 1659.03] loss=1.68 avg=1.58\n",
            "[1725 | 1659.95] loss=1.58 avg=1.58\n",
            "[1726 | 1660.88] loss=2.14 avg=1.59\n",
            "[1727 | 1661.81] loss=1.34 avg=1.59\n",
            "[1728 | 1662.73] loss=1.92 avg=1.59\n",
            "[1729 | 1663.64] loss=1.48 avg=1.59\n",
            "[1730 | 1664.58] loss=1.53 avg=1.59\n",
            "[1731 | 1665.51] loss=1.55 avg=1.59\n",
            "[1732 | 1666.43] loss=1.88 avg=1.59\n",
            "[1733 | 1667.36] loss=2.65 avg=1.60\n",
            "[1734 | 1668.28] loss=1.29 avg=1.60\n",
            "[1735 | 1669.21] loss=1.63 avg=1.60\n",
            "[1736 | 1670.15] loss=1.32 avg=1.60\n",
            "[1737 | 1671.07] loss=1.55 avg=1.60\n",
            "[1738 | 1671.99] loss=1.85 avg=1.60\n",
            "[1739 | 1672.92] loss=1.76 avg=1.60\n",
            "[1740 | 1673.85] loss=2.59 avg=1.61\n",
            "[1741 | 1674.78] loss=1.72 avg=1.61\n",
            "[1742 | 1675.70] loss=2.30 avg=1.62\n",
            "[1743 | 1676.63] loss=2.33 avg=1.63\n",
            "[1744 | 1677.56] loss=1.92 avg=1.63\n",
            "[1745 | 1678.49] loss=1.33 avg=1.63\n",
            "[1746 | 1679.42] loss=1.67 avg=1.63\n",
            "[1747 | 1680.34] loss=1.55 avg=1.63\n",
            "[1748 | 1681.28] loss=1.68 avg=1.63\n",
            "[1749 | 1682.20] loss=1.36 avg=1.62\n",
            "[1750 | 1683.13] loss=1.33 avg=1.62\n",
            "[1751 | 1684.05] loss=2.48 avg=1.63\n",
            "[1752 | 1684.98] loss=1.44 avg=1.63\n",
            "[1753 | 1685.92] loss=1.39 avg=1.62\n",
            "[1754 | 1686.85] loss=1.42 avg=1.62\n",
            "[1755 | 1687.76] loss=1.58 avg=1.62\n",
            "[1756 | 1688.69] loss=1.44 avg=1.62\n",
            "[1757 | 1689.61] loss=2.23 avg=1.63\n",
            "[1758 | 1690.54] loss=1.37 avg=1.62\n",
            "[1759 | 1691.46] loss=2.22 avg=1.63\n",
            "[1760 | 1692.39] loss=1.27 avg=1.63\n",
            "[1761 | 1693.32] loss=1.06 avg=1.62\n",
            "[1762 | 1694.25] loss=1.98 avg=1.62\n",
            "[1763 | 1695.18] loss=1.28 avg=1.62\n",
            "[1764 | 1696.10] loss=2.26 avg=1.63\n",
            "[1765 | 1697.03] loss=1.50 avg=1.63\n",
            "[1766 | 1697.95] loss=1.35 avg=1.62\n",
            "[1767 | 1698.88] loss=1.49 avg=1.62\n",
            "[1768 | 1699.81] loss=1.73 avg=1.62\n",
            "[1769 | 1700.73] loss=1.35 avg=1.62\n",
            "[1770 | 1701.66] loss=1.38 avg=1.62\n",
            "[1771 | 1702.60] loss=1.93 avg=1.62\n",
            "[1772 | 1703.53] loss=1.76 avg=1.62\n",
            "[1773 | 1704.46] loss=1.30 avg=1.62\n",
            "[1774 | 1705.38] loss=1.49 avg=1.62\n",
            "[1775 | 1706.32] loss=1.53 avg=1.62\n",
            "[1776 | 1707.24] loss=1.33 avg=1.61\n",
            "[1777 | 1708.17] loss=2.25 avg=1.62\n",
            "[1778 | 1709.10] loss=1.48 avg=1.62\n",
            "[1779 | 1710.03] loss=1.62 avg=1.62\n",
            "[1780 | 1710.96] loss=1.42 avg=1.62\n",
            "[1781 | 1711.89] loss=1.35 avg=1.61\n",
            "[1782 | 1712.82] loss=1.61 avg=1.61\n",
            "[1783 | 1713.75] loss=1.42 avg=1.61\n",
            "[1784 | 1714.67] loss=0.95 avg=1.61\n",
            "[1785 | 1715.61] loss=1.41 avg=1.60\n",
            "[1786 | 1716.54] loss=1.68 avg=1.60\n",
            "[1787 | 1717.46] loss=1.21 avg=1.60\n",
            "[1788 | 1718.39] loss=2.88 avg=1.61\n",
            "[1789 | 1719.32] loss=1.45 avg=1.61\n",
            "[1790 | 1720.24] loss=1.31 avg=1.61\n",
            "[1791 | 1721.17] loss=2.32 avg=1.62\n",
            "[1792 | 1722.10] loss=1.77 avg=1.62\n",
            "[1793 | 1723.03] loss=1.41 avg=1.62\n",
            "[1794 | 1723.97] loss=1.67 avg=1.62\n",
            "[1795 | 1724.90] loss=1.45 avg=1.61\n",
            "[1796 | 1725.83] loss=1.43 avg=1.61\n",
            "[1797 | 1726.75] loss=1.57 avg=1.61\n",
            "[1798 | 1727.68] loss=1.48 avg=1.61\n",
            "[1799 | 1728.61] loss=1.46 avg=1.61\n",
            "[1800 | 1729.54] loss=1.16 avg=1.60\n",
            "[1801 | 1730.45] loss=1.43 avg=1.60\n",
            "[1802 | 1731.37] loss=2.49 avg=1.61\n",
            "[1803 | 1732.29] loss=2.33 avg=1.62\n",
            "[1804 | 1733.21] loss=1.20 avg=1.61\n",
            "[1805 | 1734.12] loss=1.73 avg=1.62\n",
            "[1806 | 1735.04] loss=1.54 avg=1.62\n",
            "[1807 | 1735.97] loss=1.79 avg=1.62\n",
            "[1808 | 1736.89] loss=1.61 avg=1.62\n",
            "[1809 | 1737.81] loss=1.49 avg=1.62\n",
            "[1810 | 1738.73] loss=1.35 avg=1.61\n",
            "[1811 | 1739.66] loss=2.17 avg=1.62\n",
            "[1812 | 1740.59] loss=2.10 avg=1.62\n",
            "[1813 | 1741.51] loss=1.67 avg=1.62\n",
            "[1814 | 1742.43] loss=1.86 avg=1.63\n",
            "[1815 | 1743.36] loss=1.38 avg=1.62\n",
            "[1816 | 1744.28] loss=1.41 avg=1.62\n",
            "[1817 | 1745.21] loss=2.54 avg=1.63\n",
            "[1818 | 1746.13] loss=1.95 avg=1.63\n",
            "[1819 | 1747.06] loss=1.42 avg=1.63\n",
            "[1820 | 1747.98] loss=1.66 avg=1.63\n",
            "[1821 | 1748.92] loss=1.89 avg=1.63\n",
            "[1822 | 1749.85] loss=1.38 avg=1.63\n",
            "[1823 | 1750.78] loss=1.34 avg=1.63\n",
            "[1824 | 1751.70] loss=1.53 avg=1.63\n",
            "[1825 | 1752.64] loss=1.48 avg=1.63\n",
            "[1826 | 1753.56] loss=1.59 avg=1.63\n",
            "[1827 | 1754.49] loss=1.05 avg=1.62\n",
            "[1828 | 1755.42] loss=1.51 avg=1.62\n",
            "[1829 | 1756.34] loss=1.68 avg=1.62\n",
            "[1830 | 1757.28] loss=1.44 avg=1.62\n",
            "[1831 | 1758.22] loss=1.42 avg=1.62\n",
            "[1832 | 1759.14] loss=1.46 avg=1.61\n",
            "[1833 | 1760.06] loss=1.54 avg=1.61\n",
            "[1834 | 1760.99] loss=2.31 avg=1.62\n",
            "[1835 | 1761.92] loss=1.69 avg=1.62\n",
            "[1836 | 1762.85] loss=1.35 avg=1.62\n",
            "[1837 | 1763.77] loss=1.83 avg=1.62\n",
            "[1838 | 1764.70] loss=1.38 avg=1.62\n",
            "[1839 | 1765.63] loss=1.82 avg=1.62\n",
            "[1840 | 1766.56] loss=1.57 avg=1.62\n",
            "[1841 | 1767.48] loss=1.69 avg=1.62\n",
            "[1842 | 1768.41] loss=1.63 avg=1.62\n",
            "[1843 | 1769.34] loss=1.48 avg=1.62\n",
            "[1844 | 1770.26] loss=1.39 avg=1.62\n",
            "[1845 | 1771.18] loss=2.79 avg=1.63\n",
            "[1846 | 1772.11] loss=2.90 avg=1.64\n",
            "[1847 | 1773.05] loss=1.44 avg=1.64\n",
            "[1848 | 1773.98] loss=2.03 avg=1.64\n",
            "[1849 | 1774.91] loss=1.39 avg=1.64\n",
            "[1850 | 1775.84] loss=1.97 avg=1.64\n",
            "[1851 | 1776.77] loss=1.57 avg=1.64\n",
            "[1852 | 1777.69] loss=1.35 avg=1.64\n",
            "[1853 | 1778.62] loss=1.26 avg=1.64\n",
            "[1854 | 1779.54] loss=1.64 avg=1.64\n",
            "[1855 | 1780.47] loss=0.98 avg=1.63\n",
            "[1856 | 1781.40] loss=0.84 avg=1.62\n",
            "[1857 | 1782.32] loss=1.21 avg=1.62\n",
            "[1858 | 1783.25] loss=0.97 avg=1.61\n",
            "[1859 | 1784.18] loss=1.91 avg=1.61\n",
            "[1860 | 1785.11] loss=1.55 avg=1.61\n",
            "[1861 | 1786.03] loss=1.44 avg=1.61\n",
            "[1862 | 1786.96] loss=1.50 avg=1.61\n",
            "[1863 | 1787.88] loss=1.89 avg=1.61\n",
            "[1864 | 1788.81] loss=1.39 avg=1.61\n",
            "[1865 | 1789.73] loss=1.56 avg=1.61\n",
            "[1866 | 1790.66] loss=1.49 avg=1.61\n",
            "[1867 | 1791.59] loss=1.26 avg=1.61\n",
            "[1868 | 1792.53] loss=1.24 avg=1.60\n",
            "[1869 | 1793.45] loss=1.35 avg=1.60\n",
            "[1870 | 1794.37] loss=1.13 avg=1.60\n",
            "[1871 | 1795.29] loss=1.12 avg=1.59\n",
            "[1872 | 1796.22] loss=1.53 avg=1.59\n",
            "[1873 | 1797.15] loss=1.22 avg=1.59\n",
            "[1874 | 1798.07] loss=1.83 avg=1.59\n",
            "[1875 | 1798.98] loss=1.35 avg=1.59\n",
            "[1876 | 1799.91] loss=1.92 avg=1.59\n",
            "[1877 | 1800.84] loss=1.41 avg=1.59\n",
            "[1878 | 1801.76] loss=0.92 avg=1.58\n",
            "[1879 | 1802.69] loss=1.58 avg=1.58\n",
            "[1880 | 1803.61] loss=1.35 avg=1.58\n",
            "[1881 | 1804.54] loss=1.08 avg=1.57\n",
            "[1882 | 1805.47] loss=1.79 avg=1.58\n",
            "[1883 | 1806.39] loss=1.30 avg=1.57\n",
            "[1884 | 1807.32] loss=1.43 avg=1.57\n",
            "[1885 | 1808.26] loss=1.44 avg=1.57\n",
            "[1886 | 1809.19] loss=1.54 avg=1.57\n",
            "[1887 | 1810.11] loss=1.67 avg=1.57\n",
            "[1888 | 1811.04] loss=1.74 avg=1.57\n",
            "[1889 | 1811.98] loss=1.36 avg=1.57\n",
            "[1890 | 1812.90] loss=1.80 avg=1.57\n",
            "[1891 | 1813.82] loss=1.39 avg=1.57\n",
            "[1892 | 1814.76] loss=1.27 avg=1.57\n",
            "[1893 | 1815.67] loss=1.57 avg=1.57\n",
            "[1894 | 1816.60] loss=1.76 avg=1.57\n",
            "[1895 | 1817.53] loss=1.32 avg=1.57\n",
            "[1896 | 1818.45] loss=1.16 avg=1.56\n",
            "[1897 | 1819.38] loss=1.43 avg=1.56\n",
            "[1898 | 1820.31] loss=1.38 avg=1.56\n",
            "[1899 | 1821.24] loss=1.51 avg=1.56\n",
            "[1900 | 1822.16] loss=1.10 avg=1.56\n",
            "[1901 | 1823.09] loss=1.30 avg=1.55\n",
            "[1902 | 1824.03] loss=2.02 avg=1.56\n",
            "[1903 | 1824.96] loss=1.38 avg=1.56\n",
            "[1904 | 1825.88] loss=1.61 avg=1.56\n",
            "[1905 | 1826.81] loss=1.89 avg=1.56\n",
            "[1906 | 1827.74] loss=2.51 avg=1.57\n",
            "[1907 | 1828.67] loss=1.63 avg=1.57\n",
            "[1908 | 1829.59] loss=1.75 avg=1.57\n",
            "[1909 | 1830.52] loss=1.57 avg=1.57\n",
            "[1910 | 1831.45] loss=2.13 avg=1.58\n",
            "[1911 | 1832.37] loss=1.88 avg=1.58\n",
            "[1912 | 1833.31] loss=3.17 avg=1.60\n",
            "[1913 | 1834.24] loss=1.54 avg=1.60\n",
            "[1914 | 1835.17] loss=1.50 avg=1.59\n",
            "[1915 | 1836.09] loss=1.61 avg=1.59\n",
            "[1916 | 1837.02] loss=1.52 avg=1.59\n",
            "[1917 | 1837.96] loss=1.82 avg=1.60\n",
            "[1918 | 1838.89] loss=0.91 avg=1.59\n",
            "[1919 | 1839.82] loss=2.13 avg=1.59\n",
            "[1920 | 1840.74] loss=1.50 avg=1.59\n",
            "[1921 | 1841.68] loss=2.28 avg=1.60\n",
            "[1922 | 1842.60] loss=1.33 avg=1.60\n",
            "[1923 | 1843.53] loss=1.04 avg=1.59\n",
            "[1924 | 1844.46] loss=1.46 avg=1.59\n",
            "[1925 | 1845.39] loss=1.29 avg=1.59\n",
            "[1926 | 1846.31] loss=2.61 avg=1.60\n",
            "[1927 | 1847.25] loss=1.26 avg=1.60\n",
            "[1928 | 1848.18] loss=1.38 avg=1.59\n",
            "[1929 | 1849.10] loss=1.66 avg=1.59\n",
            "[1930 | 1850.03] loss=1.86 avg=1.60\n",
            "[1931 | 1850.96] loss=1.25 avg=1.59\n",
            "[1932 | 1851.88] loss=1.54 avg=1.59\n",
            "[1933 | 1852.82] loss=2.08 avg=1.60\n",
            "[1934 | 1853.74] loss=1.64 avg=1.60\n",
            "[1935 | 1854.67] loss=1.32 avg=1.59\n",
            "[1936 | 1855.60] loss=1.48 avg=1.59\n",
            "[1937 | 1856.53] loss=1.46 avg=1.59\n",
            "[1938 | 1857.45] loss=1.20 avg=1.59\n",
            "[1939 | 1858.38] loss=1.57 avg=1.59\n",
            "[1940 | 1859.31] loss=1.72 avg=1.59\n",
            "[1941 | 1860.24] loss=1.42 avg=1.59\n",
            "[1942 | 1861.18] loss=2.08 avg=1.59\n",
            "[1943 | 1862.12] loss=1.45 avg=1.59\n",
            "[1944 | 1863.04] loss=1.74 avg=1.59\n",
            "[1945 | 1863.97] loss=2.12 avg=1.60\n",
            "[1946 | 1864.90] loss=1.44 avg=1.60\n",
            "[1947 | 1865.83] loss=1.38 avg=1.59\n",
            "[1948 | 1866.75] loss=1.21 avg=1.59\n",
            "[1949 | 1867.68] loss=1.33 avg=1.59\n",
            "[1950 | 1868.61] loss=1.38 avg=1.59\n",
            "[1951 | 1869.55] loss=1.77 avg=1.59\n",
            "[1952 | 1870.46] loss=1.26 avg=1.58\n",
            "[1953 | 1871.39] loss=1.35 avg=1.58\n",
            "[1954 | 1872.31] loss=1.28 avg=1.58\n",
            "[1955 | 1873.24] loss=1.44 avg=1.58\n",
            "[1956 | 1874.17] loss=1.68 avg=1.58\n",
            "[1957 | 1875.10] loss=1.78 avg=1.58\n",
            "[1958 | 1876.02] loss=1.42 avg=1.58\n",
            "[1959 | 1876.96] loss=1.60 avg=1.58\n",
            "[1960 | 1877.89] loss=1.38 avg=1.58\n",
            "[1961 | 1878.81] loss=1.55 avg=1.58\n",
            "[1962 | 1879.75] loss=1.54 avg=1.58\n",
            "[1963 | 1880.68] loss=1.63 avg=1.58\n",
            "[1964 | 1881.60] loss=1.39 avg=1.58\n",
            "[1965 | 1882.53] loss=1.60 avg=1.58\n",
            "[1966 | 1883.46] loss=1.39 avg=1.57\n",
            "[1967 | 1884.39] loss=1.90 avg=1.58\n",
            "[1968 | 1885.32] loss=1.40 avg=1.58\n",
            "[1969 | 1886.25] loss=1.47 avg=1.57\n",
            "[1970 | 1887.17] loss=2.35 avg=1.58\n",
            "[1971 | 1888.11] loss=1.70 avg=1.58\n",
            "[1972 | 1889.03] loss=1.60 avg=1.58\n",
            "[1973 | 1889.96] loss=1.45 avg=1.58\n",
            "[1974 | 1890.90] loss=1.85 avg=1.58\n",
            "[1975 | 1891.83] loss=1.34 avg=1.58\n",
            "[1976 | 1892.76] loss=1.35 avg=1.58\n",
            "[1977 | 1893.68] loss=1.97 avg=1.58\n",
            "[1978 | 1894.61] loss=1.76 avg=1.59\n",
            "[1979 | 1895.54] loss=1.86 avg=1.59\n",
            "[1980 | 1896.48] loss=1.93 avg=1.59\n",
            "[1981 | 1897.40] loss=1.19 avg=1.59\n",
            "[1982 | 1898.33] loss=1.30 avg=1.58\n",
            "[1983 | 1899.26] loss=1.34 avg=1.58\n",
            "[1984 | 1900.18] loss=1.75 avg=1.58\n",
            "[1985 | 1901.11] loss=1.94 avg=1.59\n",
            "[1986 | 1902.04] loss=1.07 avg=1.58\n",
            "[1987 | 1902.97] loss=1.53 avg=1.58\n",
            "[1988 | 1903.89] loss=1.79 avg=1.58\n",
            "[1989 | 1904.82] loss=1.35 avg=1.58\n",
            "[1990 | 1905.75] loss=1.77 avg=1.58\n",
            "[1991 | 1906.68] loss=1.88 avg=1.59\n",
            "[1992 | 1907.60] loss=3.05 avg=1.60\n",
            "[1993 | 1908.53] loss=1.40 avg=1.60\n",
            "[1994 | 1909.45] loss=0.99 avg=1.59\n",
            "[1995 | 1910.39] loss=1.83 avg=1.60\n",
            "[1996 | 1911.32] loss=1.38 avg=1.59\n",
            "[1997 | 1912.24] loss=1.60 avg=1.59\n",
            "[1998 | 1913.17] loss=1.19 avg=1.59\n",
            "[1999 | 1914.11] loss=1.23 avg=1.59\n",
            "Saving checkpoint/run1/model-2000\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " del Privado: Hernándole y las fuerzas\n",
            "[08/10/18 9:51:37 p.m.] Jj: Lo llevo a nadie por al nuevo\n",
            "[08/10/18 9:51:55 p.m.] Kenia Porcell Privado: Ese es lo q me dijo: con lo q me dijo me fue\n",
            "[08/10/18 9:52:16 p.m.] Kenia Porcell Privado: En el q ha dicen\n",
            "[08/10/18 9:52:20 p.m.] Kenia Porcell Privado: Y lo menos quien\n",
            "[08/10/18 9:52:29 p.m.] Jj: La llamado esa españada que los hacer estos\n",
            "[08/10/18 9:52:31 p.m.] Jj: En este pais por este apoyo\n",
            "[08/10/18 9:52:38 p.m.] Jj: Yo lo mande a especializa\n",
            "[08/10/18 9:52:40 p.m.] Jj: Le toca estar como uno tienen un tema\n",
            "[08/10/18 9:52:48 p.m.] Kenia Porcell Privado: Yo lo mongó\n",
            "[08/10/18 9:53:03 p.m.] Jj: Es bueno me toco debe el\n",
            "[08/10/18 9:53:12 p.m.] Kenia Porcell Privado: Y le pida de ver así?\n",
            "[08/10/18 9:53:14 p.m.] Jj: Es bueno es una espero\n",
            "[08/10/18 9:53:22 p.m.] Jj: Es la llamado el que le dice q se le llamo tan mensaje\n",
            "[08/10/18 9:53:35 p.m.] Jj: Lo voy a trabajar las muerte en tanta\n",
            "[08/10/18 9:53:38 p.m.] Jj: Es mi responsabilidad la entrevista\n",
            "[08/10/18 9:53:44 p.m.] Kenia Porcell Privado: Si\n",
            "[08/10/18 9:53:49 p.m.] Kenia Porcell Privado: Me da no le preocupa \n",
            "[08/10/18 9:54:04 p.m.] Jj: Tambien un tema que el prd\n",
            "[08/10/18 9:54:23 p.m.] Jj: Vamos a todos\n",
            "[08/10/18 9:54:21 p.m.] Jj: Es ella\n",
            "[08/10/18 9:54:27 p.m.] Jj: Y el trabajando con todo y se tocjan\n",
            "‎[08/10/18 9:53:44 p.m.] Jj: ‎<adjunto: 00010145-PHOTO-2018-08-10-09-53-44.jpg>\n",
            "[08/10/18 9:53:35 p.m.] Jj: No le dia\n",
            "‎[08/10/18 9:52:36 p.m.] Jj: ‎<adjunto: 00010156-PHOTO-2018-08-10-09-52-36.jpg>\n",
            "[08/10/18 10:04:17 p.m.] Jj: Se lo agradezco de tanta\n",
            "[08/10/18 10:04:33 p.m.] Jj: Para los bien\n",
            "[08/10/18 10:04:48 p.m.] Jj: Por dado dura\n",
            "[08/10/18 10:04:54 p.m.] Jj: No quiero ser\n",
            "[08/10/18 10:05:28 p.m.] Jj: Le escribo los casos de tanta\n",
            "[08/10/18 11:06:00 p.m.] Jj: Cuando es bien se va a hacer\n",
            "[08/10/18 1:12:10 p.m.] Kenia Porcell Privado: Sr , con mi gente es. Dicen q no le ha dura y x eso q pasa es parecemos como muy aguantado\n",
            "[08/10/18 1:12:26 p.m.] Kenia Porcell Privado: Mire \n",
            "‎[08/10/18 1:\n",
            "\n",
            "[2000 | 1934.39] loss=1.67 avg=1.59\n",
            "[2001 | 1935.33] loss=1.38 avg=1.58\n",
            "[2002 | 1936.25] loss=1.81 avg=1.59\n",
            "[2003 | 1937.18] loss=1.14 avg=1.58\n",
            "[2004 | 1938.10] loss=1.19 avg=1.58\n",
            "[2005 | 1939.04] loss=1.28 avg=1.58\n",
            "[2006 | 1939.96] loss=1.83 avg=1.58\n",
            "[2007 | 1940.88] loss=1.49 avg=1.58\n",
            "[2008 | 1941.81] loss=1.78 avg=1.58\n",
            "[2009 | 1942.74] loss=1.86 avg=1.58\n",
            "[2010 | 1943.67] loss=1.77 avg=1.58\n",
            "[2011 | 1944.60] loss=1.23 avg=1.58\n",
            "[2012 | 1945.51] loss=1.56 avg=1.58\n",
            "[2013 | 1946.44] loss=1.71 avg=1.58\n",
            "[2014 | 1947.37] loss=1.07 avg=1.58\n",
            "[2015 | 1948.30] loss=1.33 avg=1.57\n",
            "[2016 | 1949.21] loss=1.79 avg=1.58\n",
            "[2017 | 1950.15] loss=1.31 avg=1.57\n",
            "[2018 | 1951.08] loss=1.68 avg=1.57\n",
            "[2019 | 1952.01] loss=1.88 avg=1.58\n",
            "[2020 | 1952.94] loss=1.53 avg=1.58\n",
            "[2021 | 1953.87] loss=1.33 avg=1.57\n",
            "[2022 | 1954.80] loss=2.21 avg=1.58\n",
            "[2023 | 1955.73] loss=1.57 avg=1.58\n",
            "[2024 | 1956.66] loss=1.27 avg=1.58\n",
            "[2025 | 1957.59] loss=1.49 avg=1.58\n",
            "[2026 | 1958.52] loss=1.65 avg=1.58\n",
            "[2027 | 1959.45] loss=1.11 avg=1.57\n",
            "[2028 | 1960.37] loss=1.79 avg=1.57\n",
            "[2029 | 1961.31] loss=1.61 avg=1.57\n",
            "[2030 | 1962.23] loss=1.42 avg=1.57\n",
            "[2031 | 1963.16] loss=2.17 avg=1.58\n",
            "[2032 | 1964.09] loss=1.66 avg=1.58\n",
            "[2033 | 1965.03] loss=1.98 avg=1.58\n",
            "[2034 | 1965.95] loss=1.39 avg=1.58\n",
            "[2035 | 1966.88] loss=1.11 avg=1.58\n",
            "[2036 | 1967.80] loss=1.48 avg=1.58\n",
            "[2037 | 1968.73] loss=1.21 avg=1.57\n",
            "[2038 | 1969.66] loss=2.31 avg=1.58\n",
            "[2039 | 1970.60] loss=1.69 avg=1.58\n",
            "[2040 | 1971.52] loss=1.54 avg=1.58\n",
            "[2041 | 1972.45] loss=1.54 avg=1.58\n",
            "[2042 | 1973.38] loss=1.38 avg=1.58\n",
            "[2043 | 1974.31] loss=1.34 avg=1.58\n",
            "[2044 | 1975.23] loss=1.47 avg=1.58\n",
            "[2045 | 1976.17] loss=1.26 avg=1.57\n",
            "[2046 | 1977.11] loss=1.40 avg=1.57\n",
            "[2047 | 1978.03] loss=1.73 avg=1.57\n",
            "[2048 | 1978.96] loss=1.23 avg=1.57\n",
            "[2049 | 1979.88] loss=1.65 avg=1.57\n",
            "[2050 | 1980.80] loss=1.28 avg=1.57\n",
            "[2051 | 1981.73] loss=1.59 avg=1.57\n",
            "[2052 | 1982.66] loss=1.63 avg=1.57\n",
            "[2053 | 1983.58] loss=1.47 avg=1.57\n",
            "[2054 | 1984.52] loss=1.61 avg=1.57\n",
            "[2055 | 1985.44] loss=1.54 avg=1.57\n",
            "[2056 | 1986.37] loss=1.61 avg=1.57\n",
            "[2057 | 1987.29] loss=1.65 avg=1.57\n",
            "[2058 | 1988.22] loss=1.32 avg=1.57\n",
            "[2059 | 1989.14] loss=1.63 avg=1.57\n",
            "[2060 | 1990.07] loss=1.54 avg=1.57\n",
            "[2061 | 1991.00] loss=1.40 avg=1.56\n",
            "[2062 | 1991.94] loss=1.50 avg=1.56\n",
            "[2063 | 1992.86] loss=1.27 avg=1.56\n",
            "[2064 | 1993.79] loss=1.47 avg=1.56\n",
            "[2065 | 1994.72] loss=1.45 avg=1.56\n",
            "[2066 | 1995.64] loss=1.71 avg=1.56\n",
            "[2067 | 1996.57] loss=1.30 avg=1.56\n",
            "[2068 | 1997.50] loss=0.94 avg=1.55\n",
            "[2069 | 1998.43] loss=1.01 avg=1.55\n",
            "[2070 | 1999.35] loss=1.28 avg=1.54\n",
            "[2071 | 2000.29] loss=1.61 avg=1.54\n",
            "[2072 | 2001.22] loss=1.65 avg=1.54\n",
            "[2073 | 2002.16] loss=1.63 avg=1.55\n",
            "[2074 | 2003.08] loss=1.74 avg=1.55\n",
            "[2075 | 2004.02] loss=1.38 avg=1.55\n",
            "[2076 | 2004.94] loss=1.26 avg=1.54\n",
            "[2077 | 2005.87] loss=1.38 avg=1.54\n",
            "[2078 | 2006.79] loss=1.29 avg=1.54\n",
            "[2079 | 2007.72] loss=1.84 avg=1.54\n",
            "[2080 | 2008.66] loss=1.84 avg=1.55\n",
            "[2081 | 2009.58] loss=1.51 avg=1.54\n",
            "[2082 | 2010.50] loss=1.40 avg=1.54\n",
            "[2083 | 2011.43] loss=1.54 avg=1.54\n",
            "[2084 | 2012.36] loss=1.36 avg=1.54\n",
            "[2085 | 2013.30] loss=1.40 avg=1.54\n",
            "[2086 | 2014.22] loss=1.51 avg=1.54\n",
            "[2087 | 2015.15] loss=1.43 avg=1.54\n",
            "[2088 | 2016.09] loss=2.69 avg=1.55\n",
            "[2089 | 2017.02] loss=1.34 avg=1.55\n",
            "[2090 | 2017.94] loss=1.73 avg=1.55\n",
            "[2091 | 2018.86] loss=1.69 avg=1.55\n",
            "[2092 | 2019.78] loss=1.55 avg=1.55\n",
            "[2093 | 2020.71] loss=1.48 avg=1.55\n",
            "[2094 | 2021.64] loss=1.27 avg=1.55\n",
            "[2095 | 2022.56] loss=1.44 avg=1.55\n",
            "[2096 | 2023.48] loss=1.68 avg=1.55\n",
            "[2097 | 2024.41] loss=1.31 avg=1.55\n",
            "[2098 | 2025.33] loss=1.51 avg=1.55\n",
            "[2099 | 2026.26] loss=1.22 avg=1.54\n",
            "[2100 | 2027.19] loss=1.56 avg=1.54\n",
            "[2101 | 2028.11] loss=1.22 avg=1.54\n",
            "[2102 | 2029.04] loss=1.45 avg=1.54\n",
            "[2103 | 2029.97] loss=1.52 avg=1.54\n",
            "[2104 | 2030.90] loss=1.98 avg=1.54\n",
            "[2105 | 2031.82] loss=1.63 avg=1.54\n",
            "[2106 | 2032.74] loss=1.88 avg=1.55\n",
            "[2107 | 2033.67] loss=1.43 avg=1.55\n",
            "[2108 | 2034.60] loss=1.70 avg=1.55\n",
            "[2109 | 2035.53] loss=1.37 avg=1.54\n",
            "[2110 | 2036.46] loss=1.69 avg=1.55\n",
            "[2111 | 2037.38] loss=1.51 avg=1.55\n",
            "[2112 | 2038.32] loss=1.51 avg=1.55\n",
            "[2113 | 2039.24] loss=1.43 avg=1.54\n",
            "[2114 | 2040.18] loss=1.39 avg=1.54\n",
            "[2115 | 2041.10] loss=1.45 avg=1.54\n",
            "[2116 | 2042.04] loss=1.49 avg=1.54\n",
            "[2117 | 2042.96] loss=1.78 avg=1.54\n",
            "[2118 | 2043.88] loss=1.19 avg=1.54\n",
            "[2119 | 2044.80] loss=1.46 avg=1.54\n",
            "[2120 | 2045.73] loss=1.48 avg=1.54\n",
            "[2121 | 2046.66] loss=1.46 avg=1.54\n",
            "[2122 | 2047.59] loss=1.61 avg=1.54\n",
            "[2123 | 2048.52] loss=2.05 avg=1.54\n",
            "[2124 | 2049.45] loss=1.83 avg=1.55\n",
            "[2125 | 2050.37] loss=1.26 avg=1.54\n",
            "[2126 | 2051.30] loss=1.18 avg=1.54\n",
            "[2127 | 2052.23] loss=1.57 avg=1.54\n",
            "[2128 | 2053.16] loss=1.54 avg=1.54\n",
            "[2129 | 2054.08] loss=1.47 avg=1.54\n",
            "[2130 | 2055.00] loss=1.82 avg=1.54\n",
            "[2131 | 2055.92] loss=1.19 avg=1.54\n",
            "[2132 | 2056.83] loss=1.85 avg=1.54\n",
            "[2133 | 2057.74] loss=1.35 avg=1.54\n",
            "[2134 | 2058.67] loss=2.42 avg=1.55\n",
            "[2135 | 2059.60] loss=1.34 avg=1.55\n",
            "[2136 | 2060.53] loss=1.21 avg=1.54\n",
            "[2137 | 2061.46] loss=1.34 avg=1.54\n",
            "[2138 | 2062.38] loss=1.40 avg=1.54\n",
            "[2139 | 2063.32] loss=1.41 avg=1.54\n",
            "[2140 | 2064.26] loss=2.29 avg=1.55\n",
            "[2141 | 2065.18] loss=2.06 avg=1.55\n",
            "[2142 | 2066.11] loss=2.36 avg=1.56\n",
            "[2143 | 2067.04] loss=1.23 avg=1.56\n",
            "[2144 | 2067.98] loss=1.38 avg=1.55\n",
            "[2145 | 2068.90] loss=2.28 avg=1.56\n",
            "[2146 | 2069.82] loss=1.36 avg=1.56\n",
            "[2147 | 2070.75] loss=1.50 avg=1.56\n",
            "[2148 | 2071.68] loss=1.14 avg=1.56\n",
            "[2149 | 2072.61] loss=1.44 avg=1.55\n",
            "[2150 | 2073.53] loss=1.93 avg=1.56\n",
            "[2151 | 2074.47] loss=1.85 avg=1.56\n",
            "[2152 | 2075.40] loss=1.28 avg=1.56\n",
            "[2153 | 2076.33] loss=1.56 avg=1.56\n",
            "[2154 | 2077.26] loss=1.28 avg=1.56\n",
            "[2155 | 2078.19] loss=1.35 avg=1.55\n",
            "[2156 | 2079.11] loss=1.35 avg=1.55\n",
            "[2157 | 2080.03] loss=1.17 avg=1.55\n",
            "[2158 | 2080.96] loss=1.68 avg=1.55\n",
            "[2159 | 2081.89] loss=1.29 avg=1.55\n",
            "[2160 | 2082.83] loss=1.32 avg=1.54\n",
            "[2161 | 2083.75] loss=1.44 avg=1.54\n",
            "[2162 | 2084.68] loss=1.26 avg=1.54\n",
            "[2163 | 2085.60] loss=2.09 avg=1.55\n",
            "[2164 | 2086.54] loss=1.76 avg=1.55\n",
            "[2165 | 2087.47] loss=1.36 avg=1.55\n",
            "[2166 | 2088.39] loss=1.26 avg=1.54\n",
            "[2167 | 2089.32] loss=1.34 avg=1.54\n",
            "[2168 | 2090.25] loss=1.45 avg=1.54\n",
            "[2169 | 2091.18] loss=1.31 avg=1.54\n",
            "[2170 | 2092.10] loss=1.63 avg=1.54\n",
            "[2171 | 2093.03] loss=1.13 avg=1.53\n",
            "[2172 | 2093.94] loss=1.78 avg=1.54\n",
            "[2173 | 2094.86] loss=1.51 avg=1.54\n",
            "[2174 | 2095.78] loss=1.35 avg=1.54\n",
            "[2175 | 2096.70] loss=0.88 avg=1.53\n",
            "[2176 | 2097.62] loss=1.43 avg=1.53\n",
            "[2177 | 2098.54] loss=1.58 avg=1.53\n",
            "[2178 | 2099.46] loss=1.17 avg=1.52\n",
            "[2179 | 2100.37] loss=2.15 avg=1.53\n",
            "[2180 | 2101.30] loss=1.67 avg=1.53\n",
            "[2181 | 2102.22] loss=1.40 avg=1.53\n",
            "[2182 | 2103.14] loss=1.91 avg=1.53\n",
            "[2183 | 2104.07] loss=1.59 avg=1.54\n",
            "[2184 | 2104.99] loss=1.81 avg=1.54\n",
            "[2185 | 2105.91] loss=2.36 avg=1.55\n",
            "[2186 | 2106.83] loss=1.21 avg=1.54\n",
            "[2187 | 2107.75] loss=1.96 avg=1.55\n",
            "[2188 | 2108.67] loss=1.07 avg=1.54\n",
            "[2189 | 2109.59] loss=1.25 avg=1.54\n",
            "[2190 | 2110.52] loss=1.52 avg=1.54\n",
            "[2191 | 2111.44] loss=1.41 avg=1.54\n",
            "[2192 | 2112.37] loss=1.51 avg=1.54\n",
            "[2193 | 2113.30] loss=1.59 avg=1.54\n",
            "[2194 | 2114.22] loss=1.55 avg=1.54\n",
            "[2195 | 2115.14] loss=1.33 avg=1.54\n",
            "[2196 | 2116.06] loss=1.15 avg=1.53\n",
            "[2197 | 2116.98] loss=1.21 avg=1.53\n",
            "[2198 | 2117.90] loss=1.20 avg=1.53\n",
            "[2199 | 2118.82] loss=1.36 avg=1.52\n",
            "[2200 | 2119.75] loss=1.23 avg=1.52\n",
            "[2201 | 2120.67] loss=1.00 avg=1.52\n",
            "[2202 | 2121.59] loss=1.25 avg=1.51\n",
            "[2203 | 2122.52] loss=1.50 avg=1.51\n",
            "[2204 | 2123.44] loss=2.14 avg=1.52\n",
            "[2205 | 2124.36] loss=1.49 avg=1.52\n",
            "[2206 | 2125.28] loss=1.10 avg=1.52\n",
            "[2207 | 2126.20] loss=1.58 avg=1.52\n",
            "[2208 | 2127.12] loss=1.38 avg=1.51\n",
            "[2209 | 2128.04] loss=1.34 avg=1.51\n",
            "[2210 | 2128.96] loss=1.49 avg=1.51\n",
            "[2211 | 2129.87] loss=1.81 avg=1.52\n",
            "[2212 | 2130.79] loss=1.83 avg=1.52\n",
            "[2213 | 2131.69] loss=1.47 avg=1.52\n",
            "[2214 | 2132.61] loss=1.48 avg=1.52\n",
            "[2215 | 2133.53] loss=1.50 avg=1.52\n",
            "[2216 | 2134.45] loss=2.71 avg=1.53\n",
            "[2217 | 2135.38] loss=1.61 avg=1.53\n",
            "[2218 | 2136.29] loss=1.41 avg=1.53\n",
            "[2219 | 2137.21] loss=1.15 avg=1.53\n",
            "[2220 | 2138.14] loss=1.43 avg=1.52\n",
            "[2221 | 2139.08] loss=1.63 avg=1.53\n",
            "[2222 | 2140.00] loss=1.68 avg=1.53\n",
            "[2223 | 2140.92] loss=1.59 avg=1.53\n",
            "[2224 | 2141.84] loss=1.67 avg=1.53\n",
            "[2225 | 2142.76] loss=2.35 avg=1.54\n",
            "[2226 | 2143.68] loss=1.55 avg=1.54\n",
            "[2227 | 2144.61] loss=1.61 avg=1.54\n",
            "[2228 | 2145.53] loss=1.32 avg=1.54\n",
            "[2229 | 2146.45] loss=1.50 avg=1.54\n",
            "[2230 | 2147.37] loss=1.55 avg=1.54\n",
            "[2231 | 2148.29] loss=1.25 avg=1.53\n",
            "[2232 | 2149.20] loss=1.95 avg=1.54\n",
            "[2233 | 2150.13] loss=1.36 avg=1.54\n",
            "[2234 | 2151.05] loss=1.72 avg=1.54\n",
            "[2235 | 2151.97] loss=1.01 avg=1.53\n",
            "[2236 | 2152.90] loss=1.77 avg=1.53\n",
            "[2237 | 2153.81] loss=1.62 avg=1.54\n",
            "[2238 | 2154.73] loss=0.88 avg=1.53\n",
            "[2239 | 2155.66] loss=1.74 avg=1.53\n",
            "[2240 | 2156.59] loss=1.33 avg=1.53\n",
            "[2241 | 2157.50] loss=1.81 avg=1.53\n",
            "[2242 | 2158.41] loss=1.36 avg=1.53\n",
            "[2243 | 2159.33] loss=1.59 avg=1.53\n",
            "[2244 | 2160.25] loss=1.11 avg=1.53\n",
            "[2245 | 2161.17] loss=1.64 avg=1.53\n",
            "[2246 | 2162.09] loss=1.76 avg=1.53\n",
            "[2247 | 2163.01] loss=1.40 avg=1.53\n",
            "[2248 | 2163.93] loss=2.57 avg=1.54\n",
            "[2249 | 2164.84] loss=1.60 avg=1.54\n",
            "[2250 | 2165.76] loss=2.27 avg=1.55\n",
            "[2251 | 2166.68] loss=1.33 avg=1.54\n",
            "[2252 | 2167.59] loss=1.33 avg=1.54\n",
            "[2253 | 2168.52] loss=1.44 avg=1.54\n",
            "[2254 | 2169.45] loss=1.02 avg=1.54\n",
            "[2255 | 2170.36] loss=1.29 avg=1.53\n",
            "[2256 | 2171.28] loss=1.37 avg=1.53\n",
            "[2257 | 2172.20] loss=1.48 avg=1.53\n",
            "[2258 | 2173.12] loss=1.32 avg=1.53\n",
            "[2259 | 2174.04] loss=1.45 avg=1.53\n",
            "[2260 | 2174.96] loss=1.51 avg=1.53\n",
            "[2261 | 2175.88] loss=1.82 avg=1.53\n",
            "[2262 | 2176.79] loss=1.15 avg=1.53\n",
            "[2263 | 2177.70] loss=1.40 avg=1.53\n",
            "[2264 | 2178.62] loss=1.30 avg=1.52\n",
            "[2265 | 2179.54] loss=1.33 avg=1.52\n",
            "[2266 | 2180.46] loss=1.76 avg=1.52\n",
            "[2267 | 2181.38] loss=1.17 avg=1.52\n",
            "[2268 | 2182.31] loss=1.56 avg=1.52\n",
            "[2269 | 2183.23] loss=1.68 avg=1.52\n",
            "[2270 | 2184.14] loss=1.54 avg=1.52\n",
            "[2271 | 2185.07] loss=1.53 avg=1.52\n",
            "[2272 | 2185.99] loss=1.21 avg=1.52\n",
            "[2273 | 2186.91] loss=1.25 avg=1.52\n",
            "[2274 | 2187.83] loss=1.41 avg=1.52\n",
            "[2275 | 2188.76] loss=1.29 avg=1.51\n",
            "[2276 | 2189.69] loss=1.46 avg=1.51\n",
            "[2277 | 2190.60] loss=1.35 avg=1.51\n",
            "[2278 | 2191.52] loss=1.27 avg=1.51\n",
            "[2279 | 2192.45] loss=1.49 avg=1.51\n",
            "[2280 | 2193.37] loss=1.46 avg=1.51\n",
            "[2281 | 2194.28] loss=1.50 avg=1.51\n",
            "[2282 | 2195.20] loss=1.89 avg=1.51\n",
            "[2283 | 2196.12] loss=1.60 avg=1.51\n",
            "[2284 | 2197.04] loss=1.29 avg=1.51\n",
            "[2285 | 2197.96] loss=1.33 avg=1.51\n",
            "[2286 | 2198.89] loss=1.32 avg=1.51\n",
            "[2287 | 2199.81] loss=1.56 avg=1.51\n",
            "[2288 | 2200.72] loss=1.18 avg=1.50\n",
            "[2289 | 2201.65] loss=1.43 avg=1.50\n",
            "[2290 | 2202.56] loss=1.42 avg=1.50\n",
            "[2291 | 2203.48] loss=1.16 avg=1.50\n",
            "[2292 | 2204.40] loss=1.36 avg=1.50\n",
            "[2293 | 2205.33] loss=0.95 avg=1.49\n",
            "[2294 | 2206.24] loss=1.23 avg=1.49\n",
            "[2295 | 2207.16] loss=1.22 avg=1.49\n",
            "[2296 | 2208.09] loss=1.45 avg=1.49\n",
            "[2297 | 2209.02] loss=1.61 avg=1.49\n",
            "[2298 | 2209.94] loss=2.19 avg=1.50\n",
            "[2299 | 2210.86] loss=1.30 avg=1.49\n",
            "[2300 | 2211.79] loss=1.85 avg=1.50\n",
            "[2301 | 2212.71] loss=1.52 avg=1.50\n",
            "[2302 | 2213.63] loss=0.97 avg=1.49\n",
            "[2303 | 2214.55] loss=1.62 avg=1.49\n",
            "[2304 | 2215.47] loss=1.46 avg=1.49\n",
            "[2305 | 2216.40] loss=2.86 avg=1.51\n",
            "[2306 | 2217.32] loss=1.27 avg=1.50\n",
            "[2307 | 2218.24] loss=1.31 avg=1.50\n",
            "[2308 | 2219.16] loss=1.26 avg=1.50\n",
            "[2309 | 2220.08] loss=1.22 avg=1.50\n",
            "[2310 | 2221.00] loss=1.22 avg=1.49\n",
            "[2311 | 2221.91] loss=1.38 avg=1.49\n",
            "[2312 | 2222.82] loss=1.24 avg=1.49\n",
            "[2313 | 2223.75] loss=1.88 avg=1.49\n",
            "[2314 | 2224.67] loss=1.51 avg=1.49\n",
            "[2315 | 2225.59] loss=1.07 avg=1.49\n",
            "[2316 | 2226.52] loss=2.15 avg=1.50\n",
            "[2317 | 2227.43] loss=1.63 avg=1.50\n",
            "[2318 | 2228.35] loss=1.39 avg=1.50\n",
            "[2319 | 2229.28] loss=1.59 avg=1.50\n",
            "[2320 | 2230.19] loss=1.45 avg=1.50\n",
            "[2321 | 2231.10] loss=1.53 avg=1.50\n",
            "[2322 | 2232.03] loss=1.43 avg=1.50\n",
            "[2323 | 2232.95] loss=1.32 avg=1.50\n",
            "[2324 | 2233.87] loss=1.55 avg=1.50\n",
            "[2325 | 2234.79] loss=1.47 avg=1.50\n",
            "[2326 | 2235.71] loss=1.33 avg=1.49\n",
            "[2327 | 2236.63] loss=1.54 avg=1.49\n",
            "[2328 | 2237.56] loss=1.74 avg=1.50\n",
            "[2329 | 2238.48] loss=1.34 avg=1.50\n",
            "[2330 | 2239.40] loss=1.65 avg=1.50\n",
            "[2331 | 2240.33] loss=1.74 avg=1.50\n",
            "[2332 | 2241.25] loss=1.34 avg=1.50\n",
            "[2333 | 2242.17] loss=1.70 avg=1.50\n",
            "[2334 | 2243.10] loss=1.41 avg=1.50\n",
            "[2335 | 2244.02] loss=1.42 avg=1.50\n",
            "[2336 | 2244.93] loss=1.24 avg=1.50\n",
            "[2337 | 2245.84] loss=1.68 avg=1.50\n",
            "[2338 | 2246.76] loss=1.61 avg=1.50\n",
            "[2339 | 2247.68] loss=1.04 avg=1.49\n",
            "[2340 | 2248.60] loss=1.55 avg=1.49\n",
            "[2341 | 2249.53] loss=1.73 avg=1.50\n",
            "[2342 | 2250.44] loss=1.57 avg=1.50\n",
            "[2343 | 2251.36] loss=1.19 avg=1.49\n",
            "[2344 | 2252.29] loss=1.57 avg=1.50\n",
            "[2345 | 2253.21] loss=1.21 avg=1.49\n",
            "[2346 | 2254.13] loss=1.08 avg=1.49\n",
            "[2347 | 2255.04] loss=1.33 avg=1.49\n",
            "[2348 | 2255.97] loss=1.25 avg=1.48\n",
            "[2349 | 2256.88] loss=1.08 avg=1.48\n",
            "[2350 | 2257.80] loss=1.52 avg=1.48\n",
            "[2351 | 2258.72] loss=1.37 avg=1.48\n",
            "[2352 | 2259.64] loss=1.73 avg=1.48\n",
            "[2353 | 2260.56] loss=1.49 avg=1.48\n",
            "[2354 | 2261.48] loss=1.50 avg=1.48\n",
            "[2355 | 2262.40] loss=1.21 avg=1.48\n",
            "[2356 | 2263.32] loss=2.02 avg=1.49\n",
            "[2357 | 2264.24] loss=1.26 avg=1.48\n",
            "[2358 | 2265.16] loss=1.80 avg=1.49\n",
            "[2359 | 2266.08] loss=1.37 avg=1.48\n",
            "[2360 | 2267.01] loss=1.15 avg=1.48\n",
            "[2361 | 2267.93] loss=1.57 avg=1.48\n",
            "[2362 | 2268.84] loss=1.44 avg=1.48\n",
            "[2363 | 2269.76] loss=1.74 avg=1.48\n",
            "[2364 | 2270.68] loss=1.25 avg=1.48\n",
            "[2365 | 2271.60] loss=1.96 avg=1.49\n",
            "[2366 | 2272.52] loss=1.22 avg=1.48\n",
            "[2367 | 2273.44] loss=1.47 avg=1.48\n",
            "[2368 | 2274.36] loss=1.59 avg=1.49\n",
            "[2369 | 2275.27] loss=1.78 avg=1.49\n",
            "[2370 | 2276.20] loss=1.52 avg=1.49\n",
            "[2371 | 2277.12] loss=1.34 avg=1.49\n",
            "[2372 | 2278.04] loss=3.02 avg=1.50\n",
            "[2373 | 2278.95] loss=1.55 avg=1.50\n",
            "[2374 | 2279.88] loss=1.74 avg=1.51\n",
            "[2375 | 2280.80] loss=1.30 avg=1.50\n",
            "[2376 | 2281.71] loss=1.54 avg=1.50\n",
            "[2377 | 2282.63] loss=1.32 avg=1.50\n",
            "[2378 | 2283.55] loss=2.42 avg=1.51\n",
            "[2379 | 2284.47] loss=1.71 avg=1.51\n",
            "[2380 | 2285.39] loss=1.60 avg=1.51\n",
            "[2381 | 2286.31] loss=1.72 avg=1.52\n",
            "[2382 | 2287.23] loss=1.60 avg=1.52\n",
            "[2383 | 2288.15] loss=1.12 avg=1.51\n",
            "[2384 | 2289.07] loss=1.43 avg=1.51\n",
            "[2385 | 2289.99] loss=1.05 avg=1.51\n",
            "[2386 | 2290.91] loss=2.54 avg=1.52\n",
            "[2387 | 2291.83] loss=1.25 avg=1.51\n",
            "[2388 | 2292.76] loss=1.34 avg=1.51\n",
            "[2389 | 2293.68] loss=1.13 avg=1.51\n",
            "[2390 | 2294.59] loss=1.42 avg=1.51\n",
            "[2391 | 2295.51] loss=1.50 avg=1.51\n",
            "[2392 | 2296.43] loss=1.27 avg=1.51\n",
            "[2393 | 2297.35] loss=1.27 avg=1.50\n",
            "[2394 | 2298.27] loss=1.76 avg=1.51\n",
            "[2395 | 2299.19] loss=1.23 avg=1.50\n",
            "[2396 | 2300.10] loss=1.63 avg=1.50\n",
            "[2397 | 2301.02] loss=1.25 avg=1.50\n",
            "[2398 | 2301.94] loss=1.56 avg=1.50\n",
            "[2399 | 2302.86] loss=1.66 avg=1.50\n",
            "[2400 | 2303.78] loss=1.34 avg=1.50\n",
            "[2401 | 2304.70] loss=1.35 avg=1.50\n",
            "[2402 | 2305.62] loss=1.42 avg=1.50\n",
            "[2403 | 2306.54] loss=1.59 avg=1.50\n",
            "[2404 | 2307.46] loss=1.21 avg=1.50\n",
            "[2405 | 2308.37] loss=1.22 avg=1.50\n",
            "[2406 | 2309.30] loss=1.47 avg=1.50\n",
            "[2407 | 2310.22] loss=1.62 avg=1.50\n",
            "[2408 | 2311.15] loss=1.74 avg=1.50\n",
            "[2409 | 2312.07] loss=1.79 avg=1.50\n",
            "[2410 | 2313.00] loss=1.66 avg=1.50\n",
            "[2411 | 2313.93] loss=1.44 avg=1.50\n",
            "[2412 | 2314.86] loss=1.49 avg=1.50\n",
            "[2413 | 2315.78] loss=1.09 avg=1.50\n",
            "[2414 | 2316.70] loss=1.42 avg=1.50\n",
            "[2415 | 2317.62] loss=1.33 avg=1.50\n",
            "[2416 | 2318.56] loss=1.52 avg=1.50\n",
            "[2417 | 2319.48] loss=1.34 avg=1.49\n",
            "[2418 | 2320.41] loss=2.27 avg=1.50\n",
            "[2419 | 2321.34] loss=1.31 avg=1.50\n",
            "[2420 | 2322.27] loss=1.64 avg=1.50\n",
            "[2421 | 2323.19] loss=1.65 avg=1.50\n",
            "[2422 | 2324.12] loss=1.49 avg=1.50\n",
            "[2423 | 2325.05] loss=1.14 avg=1.50\n",
            "[2424 | 2325.98] loss=1.08 avg=1.50\n",
            "[2425 | 2326.91] loss=1.42 avg=1.49\n",
            "[2426 | 2327.84] loss=1.30 avg=1.49\n",
            "[2427 | 2328.76] loss=1.24 avg=1.49\n",
            "[2428 | 2329.70] loss=1.18 avg=1.49\n",
            "[2429 | 2330.63] loss=1.44 avg=1.49\n",
            "[2430 | 2331.55] loss=1.42 avg=1.49\n",
            "[2431 | 2332.47] loss=1.39 avg=1.49\n",
            "[2432 | 2333.40] loss=1.46 avg=1.48\n",
            "[2433 | 2334.34] loss=1.71 avg=1.49\n",
            "[2434 | 2335.27] loss=1.63 avg=1.49\n",
            "[2435 | 2336.20] loss=1.34 avg=1.49\n",
            "[2436 | 2337.13] loss=1.51 avg=1.49\n",
            "[2437 | 2338.06] loss=1.30 avg=1.49\n",
            "[2438 | 2338.98] loss=1.20 avg=1.48\n",
            "[2439 | 2339.91] loss=1.46 avg=1.48\n",
            "[2440 | 2340.84] loss=1.75 avg=1.48\n",
            "[2441 | 2341.76] loss=1.54 avg=1.49\n",
            "[2442 | 2342.69] loss=1.27 avg=1.48\n",
            "[2443 | 2343.61] loss=1.55 avg=1.48\n",
            "[2444 | 2344.54] loss=2.00 avg=1.49\n",
            "[2445 | 2345.48] loss=1.44 avg=1.49\n",
            "[2446 | 2346.41] loss=1.25 avg=1.49\n",
            "[2447 | 2347.33] loss=1.36 avg=1.49\n",
            "[2448 | 2348.27] loss=2.05 avg=1.49\n",
            "[2449 | 2349.19] loss=1.52 avg=1.49\n",
            "[2450 | 2350.12] loss=1.35 avg=1.49\n",
            "[2451 | 2351.04] loss=1.37 avg=1.49\n",
            "[2452 | 2351.98] loss=1.37 avg=1.49\n",
            "[2453 | 2352.91] loss=1.34 avg=1.49\n",
            "[2454 | 2353.84] loss=2.03 avg=1.49\n",
            "[2455 | 2354.76] loss=1.58 avg=1.49\n",
            "[2456 | 2355.70] loss=1.72 avg=1.49\n",
            "[2457 | 2356.63] loss=1.46 avg=1.49\n",
            "[2458 | 2357.56] loss=1.49 avg=1.49\n",
            "[2459 | 2358.49] loss=1.34 avg=1.49\n",
            "[2460 | 2359.42] loss=2.07 avg=1.50\n",
            "[2461 | 2360.34] loss=1.54 avg=1.50\n",
            "[2462 | 2361.27] loss=1.56 avg=1.50\n",
            "[2463 | 2362.19] loss=1.77 avg=1.50\n",
            "[2464 | 2363.13] loss=1.20 avg=1.50\n",
            "[2465 | 2364.06] loss=1.08 avg=1.49\n",
            "[2466 | 2364.99] loss=2.49 avg=1.50\n",
            "[2467 | 2365.91] loss=1.39 avg=1.50\n",
            "[2468 | 2366.84] loss=1.41 avg=1.50\n",
            "[2469 | 2367.77] loss=1.83 avg=1.51\n",
            "[2470 | 2368.70] loss=1.40 avg=1.50\n",
            "[2471 | 2369.62] loss=1.17 avg=1.50\n",
            "[2472 | 2370.55] loss=1.65 avg=1.50\n",
            "[2473 | 2371.48] loss=1.38 avg=1.50\n",
            "[2474 | 2372.41] loss=1.29 avg=1.50\n",
            "[2475 | 2373.34] loss=1.16 avg=1.50\n",
            "[2476 | 2374.26] loss=1.28 avg=1.49\n",
            "[2477 | 2375.20] loss=1.76 avg=1.50\n",
            "[2478 | 2376.13] loss=1.24 avg=1.49\n",
            "[2479 | 2377.05] loss=1.40 avg=1.49\n",
            "[2480 | 2377.98] loss=1.37 avg=1.49\n",
            "[2481 | 2378.91] loss=1.55 avg=1.49\n",
            "[2482 | 2379.83] loss=1.99 avg=1.50\n",
            "[2483 | 2380.76] loss=1.42 avg=1.50\n",
            "[2484 | 2381.69] loss=1.51 avg=1.50\n",
            "[2485 | 2382.62] loss=1.67 avg=1.50\n",
            "[2486 | 2383.55] loss=1.40 avg=1.50\n",
            "[2487 | 2384.48] loss=1.73 avg=1.50\n",
            "[2488 | 2385.40] loss=1.47 avg=1.50\n",
            "[2489 | 2386.32] loss=1.30 avg=1.50\n",
            "[2490 | 2387.26] loss=1.40 avg=1.50\n",
            "[2491 | 2388.18] loss=2.41 avg=1.51\n",
            "[2492 | 2389.11] loss=1.67 avg=1.51\n",
            "[2493 | 2390.03] loss=1.45 avg=1.51\n",
            "[2494 | 2390.97] loss=1.36 avg=1.51\n",
            "[2495 | 2391.90] loss=1.99 avg=1.51\n",
            "[2496 | 2392.83] loss=1.57 avg=1.51\n",
            "[2497 | 2393.76] loss=1.47 avg=1.51\n",
            "[2498 | 2394.69] loss=1.31 avg=1.51\n",
            "[2499 | 2395.62] loss=2.15 avg=1.51\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "18:33 a.m.] Jj: Sii necesitan que se va estrado\n",
            "[10/12/18 8:11:23 a.m.] Jj: Siiii usted tomorullos\n",
            "[10/12/18 8:13:10 a.m.] Kenia Porcell Privado: No problema\n",
            "[10/12/18 8:14:03 a.m.] Kenia Porcell Privado: No nos haciendo muchas veces. Q no tengo hablar\n",
            "[10/12/18 8:14:43 a.m.] Kenia Porcell Privado: Esa pareña los americanos y lo haría se sienta no puedo conuestas a la prensa\n",
            "[10/12/18 8:16:28 a.m.] Kenia Porcell Privado: Ese enviar la prueba es fuerte q el csn delitos puede enfrente q se comdias eso.\n",
            "\n",
            "Enfrentando la prueba de esto\n",
            "[10/12/18 8:17:00 a.m.] Kenia Porcell Privado: Y es el traslado para no quiera se despuede escudos\n",
            "[10/12/18 8:17:44 a.m.] Jj: Debe eso\n",
            "[10/12/18 8:17:57 a.m.] Jj: Y el tiempo de las ciudades del mp se después\n",
            "[10/12/18 8:18:03 a.m.] Kenia Porcell Privado: Y no me preocupa q les hablar con viernes\n",
            "[10/12/18 8:18:15 a.m.] Jj: Creame todo mi es importante todo el cita de trasladom\n",
            "[10/12/18 8:18:38 a.m.] Kenia Porcell Privado: No se después con usted debe el lunes . No quedar para la gente de esta\n",
            "[10/12/18 8:18:53 a.m.] Jj: Asi es\n",
            "[10/12/18 8:27:25 a.m.] Jj: Mire esto\n",
            "[10/12/18 8:27:42 a.m.] Jj: Pero es muy fuerte de esto, lo de cuestionarle\n",
            "[10/12/18 8:28:32 a.m.] Jj: Y la señora esa pye siempre le fue a escargar a mi\n",
            "[10/12/18 6:15:53 a.m.] Kenia Porcell Privado: La puedo conllevar a las 5 horas q habló de la ley\n",
            "[10/12/18 6:16:08 a.m.] Kenia Porcell Privado: Me puede hacer , se le pedí está casos para conuestró a mi enfrentando en las 5 horas .\n",
            "Pientes me encontré oja de todos, usted me dice q lo haría y me denganó, pero yo no puede ser q se va a esa parte un traslado a la gente .\n",
            "[10/12/18 12:02:55 p.m.] Jj: Usted se va para el de ronald que pino q el mp se comunicó a esa sola para el señor címple la gente de esta. El concluyendo a allegar del seguro a de los 5 horas a notarizar en esa procuradora de llegar todas al mp \n",
            "\n",
            "Pedo dejar algunca, el traslado, desatenta y ayer a los 5 horas para que me hizo esa falta .\n",
            "Usted desayuno lo es que se lo agradezco lo q ha hecho al equipo a su fuerte, todo va a la gente de 3 horas.\n",
            "Como puedo ser eso , con todos los 4 horas a notarizar en la procuradora la gente de 3 horas\n",
            "[10/12/18 12:04:07 p.m.] Jj: Excelente\n",
            "[10/12/18 12:04:15 p.m.] Jj: Puedo escuchar lo de los 5 horas\n",
            "[10/12/18 12:05:07 p.m.] Jj:\n",
            "\n",
            "[2500 | 2413.86] loss=1.53 avg=1.51\n",
            "[2501 | 2414.79] loss=1.72 avg=1.52\n",
            "[2502 | 2415.73] loss=1.41 avg=1.52\n",
            "[2503 | 2416.65] loss=1.38 avg=1.51\n",
            "[2504 | 2417.59] loss=1.16 avg=1.51\n",
            "[2505 | 2418.52] loss=1.35 avg=1.51\n",
            "[2506 | 2419.45] loss=1.96 avg=1.51\n",
            "[2507 | 2420.38] loss=1.40 avg=1.51\n",
            "[2508 | 2421.31] loss=1.62 avg=1.51\n",
            "[2509 | 2422.24] loss=1.28 avg=1.51\n",
            "[2510 | 2423.18] loss=1.32 avg=1.51\n",
            "[2511 | 2424.11] loss=1.09 avg=1.51\n",
            "[2512 | 2425.03] loss=1.25 avg=1.50\n",
            "[2513 | 2425.96] loss=1.26 avg=1.50\n",
            "[2514 | 2426.89] loss=1.20 avg=1.50\n",
            "[2515 | 2427.81] loss=1.28 avg=1.50\n",
            "[2516 | 2428.74] loss=1.61 avg=1.50\n",
            "[2517 | 2429.67] loss=1.13 avg=1.49\n",
            "[2518 | 2430.61] loss=1.30 avg=1.49\n",
            "[2519 | 2431.52] loss=1.71 avg=1.49\n",
            "[2520 | 2432.46] loss=1.59 avg=1.49\n",
            "[2521 | 2433.39] loss=1.07 avg=1.49\n",
            "[2522 | 2434.32] loss=1.42 avg=1.49\n",
            "[2523 | 2435.24] loss=1.50 avg=1.49\n",
            "[2524 | 2436.17] loss=1.25 avg=1.49\n",
            "[2525 | 2437.10] loss=1.55 avg=1.49\n",
            "[2526 | 2438.04] loss=1.44 avg=1.49\n",
            "[2527 | 2438.96] loss=1.54 avg=1.49\n",
            "[2528 | 2439.90] loss=1.66 avg=1.49\n",
            "[2529 | 2440.83] loss=1.31 avg=1.49\n",
            "[2530 | 2441.76] loss=1.32 avg=1.49\n",
            "[2531 | 2442.68] loss=1.20 avg=1.48\n",
            "[2532 | 2443.61] loss=1.35 avg=1.48\n",
            "[2533 | 2444.54] loss=1.78 avg=1.48\n",
            "[2534 | 2445.47] loss=1.27 avg=1.48\n",
            "[2535 | 2446.40] loss=1.40 avg=1.48\n",
            "[2536 | 2447.33] loss=1.56 avg=1.48\n",
            "[2537 | 2448.26] loss=1.37 avg=1.48\n",
            "[2538 | 2449.19] loss=1.20 avg=1.48\n",
            "[2539 | 2450.12] loss=1.22 avg=1.48\n",
            "[2540 | 2451.05] loss=1.35 avg=1.47\n",
            "[2541 | 2451.98] loss=1.63 avg=1.48\n",
            "[2542 | 2452.92] loss=1.94 avg=1.48\n",
            "[2543 | 2453.85] loss=1.08 avg=1.48\n",
            "[2544 | 2454.78] loss=1.21 avg=1.47\n",
            "[2545 | 2455.71] loss=1.40 avg=1.47\n",
            "[2546 | 2456.63] loss=1.69 avg=1.48\n",
            "[2547 | 2457.57] loss=1.11 avg=1.47\n",
            "[2548 | 2458.49] loss=1.64 avg=1.47\n",
            "[2549 | 2459.42] loss=1.66 avg=1.48\n",
            "[2550 | 2460.35] loss=1.37 avg=1.47\n",
            "[2551 | 2461.28] loss=1.31 avg=1.47\n",
            "[2552 | 2462.21] loss=1.47 avg=1.47\n",
            "[2553 | 2463.15] loss=1.44 avg=1.47\n",
            "[2554 | 2464.08] loss=1.57 avg=1.47\n",
            "[2555 | 2465.01] loss=1.41 avg=1.47\n",
            "[2556 | 2465.93] loss=2.39 avg=1.48\n",
            "[2557 | 2466.86] loss=1.72 avg=1.48\n",
            "[2558 | 2467.79] loss=1.47 avg=1.48\n",
            "[2559 | 2468.72] loss=1.61 avg=1.49\n",
            "[2560 | 2469.65] loss=1.15 avg=1.48\n",
            "[2561 | 2470.59] loss=1.29 avg=1.48\n",
            "[2562 | 2471.51] loss=1.39 avg=1.48\n",
            "[2563 | 2472.44] loss=1.48 avg=1.48\n",
            "[2564 | 2473.37] loss=1.33 avg=1.48\n",
            "[2565 | 2474.31] loss=1.45 avg=1.48\n",
            "[2566 | 2475.25] loss=0.95 avg=1.47\n",
            "[2567 | 2476.17] loss=1.23 avg=1.47\n",
            "[2568 | 2477.10] loss=1.55 avg=1.47\n",
            "[2569 | 2478.04] loss=2.15 avg=1.48\n",
            "[2570 | 2478.95] loss=1.26 avg=1.48\n",
            "[2571 | 2479.88] loss=1.70 avg=1.48\n",
            "[2572 | 2480.81] loss=1.52 avg=1.48\n",
            "[2573 | 2481.73] loss=2.31 avg=1.49\n",
            "[2574 | 2482.66] loss=2.29 avg=1.49\n",
            "[2575 | 2483.59] loss=1.05 avg=1.49\n",
            "[2576 | 2484.51] loss=1.51 avg=1.49\n",
            "[2577 | 2485.44] loss=1.55 avg=1.49\n",
            "[2578 | 2486.37] loss=1.54 avg=1.49\n",
            "[2579 | 2487.30] loss=1.63 avg=1.49\n",
            "[2580 | 2488.23] loss=1.76 avg=1.50\n",
            "[2581 | 2489.16] loss=1.57 avg=1.50\n",
            "[2582 | 2490.09] loss=1.21 avg=1.49\n",
            "[2583 | 2491.02] loss=1.40 avg=1.49\n",
            "[2584 | 2491.94] loss=1.62 avg=1.49\n",
            "[2585 | 2492.87] loss=1.56 avg=1.49\n",
            "[2586 | 2493.81] loss=1.97 avg=1.50\n",
            "[2587 | 2494.73] loss=2.49 avg=1.51\n",
            "[2588 | 2495.66] loss=1.62 avg=1.51\n",
            "[2589 | 2496.58] loss=1.16 avg=1.51\n",
            "[2590 | 2497.51] loss=1.24 avg=1.50\n",
            "[2591 | 2498.43] loss=1.33 avg=1.50\n",
            "[2592 | 2499.36] loss=1.12 avg=1.50\n",
            "[2593 | 2500.29] loss=1.11 avg=1.49\n",
            "[2594 | 2501.23] loss=1.27 avg=1.49\n",
            "[2595 | 2502.15] loss=1.31 avg=1.49\n",
            "[2596 | 2503.09] loss=1.19 avg=1.49\n",
            "[2597 | 2504.01] loss=1.43 avg=1.49\n",
            "[2598 | 2504.94] loss=1.24 avg=1.48\n",
            "[2599 | 2505.87] loss=0.85 avg=1.48\n",
            "[2600 | 2506.80] loss=1.31 avg=1.48\n",
            "[2601 | 2507.73] loss=1.25 avg=1.47\n",
            "[2602 | 2508.66] loss=1.64 avg=1.48\n",
            "[2603 | 2509.59] loss=1.07 avg=1.47\n",
            "[2604 | 2510.52] loss=1.04 avg=1.47\n",
            "[2605 | 2511.45] loss=1.10 avg=1.46\n",
            "[2606 | 2512.39] loss=1.56 avg=1.46\n",
            "[2607 | 2513.31] loss=1.94 avg=1.47\n",
            "[2608 | 2514.24] loss=1.47 avg=1.47\n",
            "[2609 | 2515.17] loss=1.61 avg=1.47\n",
            "[2610 | 2516.11] loss=1.48 avg=1.47\n",
            "[2611 | 2517.04] loss=1.16 avg=1.47\n",
            "[2612 | 2517.97] loss=1.65 avg=1.47\n",
            "[2613 | 2518.90] loss=1.64 avg=1.47\n",
            "[2614 | 2519.83] loss=1.57 avg=1.47\n",
            "[2615 | 2520.75] loss=1.30 avg=1.47\n",
            "[2616 | 2521.69] loss=1.22 avg=1.47\n",
            "[2617 | 2522.61] loss=1.39 avg=1.47\n",
            "[2618 | 2523.55] loss=1.65 avg=1.47\n",
            "[2619 | 2524.46] loss=1.28 avg=1.47\n",
            "[2620 | 2525.39] loss=1.38 avg=1.47\n",
            "[2621 | 2526.31] loss=1.29 avg=1.46\n",
            "[2622 | 2527.24] loss=1.78 avg=1.47\n",
            "[2623 | 2528.18] loss=1.57 avg=1.47\n",
            "[2624 | 2529.10] loss=1.61 avg=1.47\n",
            "[2625 | 2530.03] loss=1.55 avg=1.47\n",
            "[2626 | 2530.97] loss=1.37 avg=1.47\n",
            "[2627 | 2531.89] loss=1.34 avg=1.47\n",
            "[2628 | 2532.81] loss=1.36 avg=1.47\n",
            "[2629 | 2533.74] loss=1.24 avg=1.47\n",
            "[2630 | 2534.67] loss=1.59 avg=1.47\n",
            "[2631 | 2535.61] loss=1.25 avg=1.46\n",
            "[2632 | 2536.54] loss=1.25 avg=1.46\n",
            "[2633 | 2537.47] loss=1.42 avg=1.46\n",
            "[2634 | 2538.41] loss=1.39 avg=1.46\n",
            "[2635 | 2539.33] loss=1.39 avg=1.46\n",
            "[2636 | 2540.27] loss=1.19 avg=1.46\n",
            "[2637 | 2541.19] loss=1.25 avg=1.46\n",
            "[2638 | 2542.13] loss=1.62 avg=1.46\n",
            "[2639 | 2543.06] loss=1.39 avg=1.46\n",
            "[2640 | 2543.99] loss=1.65 avg=1.46\n",
            "[2641 | 2544.92] loss=1.26 avg=1.46\n",
            "[2642 | 2545.85] loss=1.39 avg=1.46\n",
            "[2643 | 2546.77] loss=1.44 avg=1.46\n",
            "[2644 | 2547.70] loss=1.64 avg=1.46\n",
            "[2645 | 2548.63] loss=1.11 avg=1.45\n",
            "[2646 | 2549.57] loss=1.50 avg=1.45\n",
            "[2647 | 2550.50] loss=1.15 avg=1.45\n",
            "[2648 | 2551.44] loss=1.53 avg=1.45\n",
            "[2649 | 2552.37] loss=1.49 avg=1.45\n",
            "[2650 | 2553.30] loss=1.71 avg=1.46\n",
            "[2651 | 2554.23] loss=1.34 avg=1.45\n",
            "[2652 | 2555.15] loss=1.23 avg=1.45\n",
            "[2653 | 2556.08] loss=1.40 avg=1.45\n",
            "[2654 | 2557.00] loss=1.26 avg=1.45\n",
            "[2655 | 2557.93] loss=1.20 avg=1.45\n",
            "[2656 | 2558.86] loss=1.39 avg=1.45\n",
            "[2657 | 2559.80] loss=1.44 avg=1.45\n",
            "[2658 | 2560.73] loss=1.27 avg=1.44\n",
            "[2659 | 2561.67] loss=1.34 avg=1.44\n",
            "[2660 | 2562.59] loss=1.26 avg=1.44\n",
            "[2661 | 2563.52] loss=1.53 avg=1.44\n",
            "[2662 | 2564.45] loss=2.31 avg=1.45\n",
            "[2663 | 2565.38] loss=1.18 avg=1.45\n",
            "[2664 | 2566.31] loss=1.71 avg=1.45\n",
            "[2665 | 2567.24] loss=1.70 avg=1.45\n",
            "[2666 | 2568.17] loss=1.28 avg=1.45\n",
            "[2667 | 2569.11] loss=1.60 avg=1.45\n",
            "[2668 | 2570.04] loss=1.44 avg=1.45\n",
            "[2669 | 2570.98] loss=1.03 avg=1.45\n",
            "[2670 | 2571.90] loss=0.97 avg=1.44\n",
            "[2671 | 2572.84] loss=1.28 avg=1.44\n",
            "[2672 | 2573.78] loss=1.06 avg=1.44\n",
            "[2673 | 2574.71] loss=1.43 avg=1.44\n",
            "[2674 | 2575.64] loss=2.56 avg=1.45\n",
            "[2675 | 2576.57] loss=1.37 avg=1.45\n",
            "[2676 | 2577.51] loss=1.17 avg=1.45\n",
            "[2677 | 2578.43] loss=1.51 avg=1.45\n",
            "[2678 | 2579.36] loss=2.03 avg=1.45\n",
            "[2679 | 2580.29] loss=1.99 avg=1.46\n",
            "[2680 | 2581.22] loss=1.69 avg=1.46\n",
            "[2681 | 2582.14] loss=1.71 avg=1.46\n",
            "[2682 | 2583.07] loss=1.75 avg=1.47\n",
            "[2683 | 2584.00] loss=1.64 avg=1.47\n",
            "[2684 | 2584.94] loss=1.30 avg=1.47\n",
            "[2685 | 2585.86] loss=1.18 avg=1.46\n",
            "[2686 | 2586.79] loss=1.34 avg=1.46\n",
            "[2687 | 2587.72] loss=1.66 avg=1.46\n",
            "[2688 | 2588.66] loss=1.32 avg=1.46\n",
            "[2689 | 2589.59] loss=1.34 avg=1.46\n",
            "[2690 | 2590.51] loss=1.22 avg=1.46\n",
            "[2691 | 2591.44] loss=1.37 avg=1.46\n",
            "[2692 | 2592.37] loss=1.19 avg=1.46\n",
            "[2693 | 2593.29] loss=1.17 avg=1.45\n",
            "[2694 | 2594.23] loss=1.39 avg=1.45\n",
            "[2695 | 2595.15] loss=1.42 avg=1.45\n",
            "[2696 | 2596.08] loss=1.26 avg=1.45\n",
            "[2697 | 2597.01] loss=1.29 avg=1.45\n",
            "[2698 | 2597.94] loss=1.17 avg=1.44\n",
            "[2699 | 2598.88] loss=1.37 avg=1.44\n",
            "[2700 | 2599.81] loss=1.45 avg=1.44\n",
            "[2701 | 2600.74] loss=1.42 avg=1.44\n",
            "[2702 | 2601.67] loss=1.80 avg=1.45\n",
            "[2703 | 2602.60] loss=1.50 avg=1.45\n",
            "[2704 | 2603.54] loss=1.42 avg=1.45\n",
            "[2705 | 2604.46] loss=1.85 avg=1.45\n",
            "[2706 | 2605.39] loss=1.06 avg=1.45\n",
            "[2707 | 2606.32] loss=1.38 avg=1.45\n",
            "[2708 | 2607.26] loss=1.10 avg=1.44\n",
            "[2709 | 2608.19] loss=1.49 avg=1.44\n",
            "[2710 | 2609.12] loss=1.44 avg=1.44\n",
            "[2711 | 2610.05] loss=1.35 avg=1.44\n",
            "[2712 | 2610.98] loss=1.42 avg=1.44\n",
            "[2713 | 2611.91] loss=1.33 avg=1.44\n",
            "[2714 | 2612.85] loss=1.56 avg=1.44\n",
            "[2715 | 2613.78] loss=2.01 avg=1.45\n",
            "[2716 | 2614.71] loss=1.40 avg=1.45\n",
            "[2717 | 2615.64] loss=1.48 avg=1.45\n",
            "[2718 | 2616.58] loss=1.34 avg=1.45\n",
            "[2719 | 2617.51] loss=1.52 avg=1.45\n",
            "[2720 | 2618.44] loss=1.40 avg=1.45\n",
            "[2721 | 2619.36] loss=0.94 avg=1.44\n",
            "[2722 | 2620.30] loss=1.53 avg=1.44\n",
            "[2723 | 2621.24] loss=1.70 avg=1.45\n",
            "[2724 | 2622.17] loss=1.70 avg=1.45\n",
            "[2725 | 2623.10] loss=1.11 avg=1.45\n",
            "[2726 | 2624.03] loss=1.25 avg=1.44\n",
            "[2727 | 2624.96] loss=1.05 avg=1.44\n",
            "[2728 | 2625.89] loss=1.31 avg=1.44\n",
            "[2729 | 2626.82] loss=1.45 avg=1.44\n",
            "[2730 | 2627.76] loss=1.26 avg=1.44\n",
            "[2731 | 2628.69] loss=1.15 avg=1.43\n",
            "[2732 | 2629.62] loss=1.34 avg=1.43\n",
            "[2733 | 2630.55] loss=1.06 avg=1.43\n",
            "[2734 | 2631.48] loss=1.47 avg=1.43\n",
            "[2735 | 2632.41] loss=1.77 avg=1.43\n",
            "[2736 | 2633.35] loss=1.16 avg=1.43\n",
            "[2737 | 2634.28] loss=1.50 avg=1.43\n",
            "[2738 | 2635.21] loss=1.23 avg=1.43\n",
            "[2739 | 2636.14] loss=1.78 avg=1.43\n",
            "[2740 | 2637.07] loss=1.11 avg=1.43\n",
            "[2741 | 2638.00] loss=1.18 avg=1.43\n",
            "[2742 | 2638.93] loss=0.96 avg=1.42\n",
            "[2743 | 2639.86] loss=1.24 avg=1.42\n",
            "[2744 | 2640.79] loss=1.36 avg=1.42\n",
            "[2745 | 2641.71] loss=1.30 avg=1.42\n",
            "[2746 | 2642.64] loss=1.55 avg=1.42\n",
            "[2747 | 2643.56] loss=1.80 avg=1.42\n",
            "[2748 | 2644.48] loss=1.61 avg=1.43\n",
            "[2749 | 2645.42] loss=1.16 avg=1.42\n",
            "[2750 | 2646.35] loss=0.99 avg=1.42\n",
            "[2751 | 2647.28] loss=1.61 avg=1.42\n",
            "[2752 | 2648.20] loss=1.42 avg=1.42\n",
            "[2753 | 2649.14] loss=1.18 avg=1.42\n",
            "[2754 | 2650.06] loss=1.60 avg=1.42\n",
            "[2755 | 2650.99] loss=1.70 avg=1.42\n",
            "[2756 | 2651.92] loss=1.30 avg=1.42\n",
            "[2757 | 2652.86] loss=1.48 avg=1.42\n",
            "[2758 | 2653.78] loss=1.76 avg=1.43\n",
            "[2759 | 2654.72] loss=1.20 avg=1.42\n",
            "[2760 | 2655.66] loss=1.59 avg=1.42\n",
            "[2761 | 2656.59] loss=1.63 avg=1.43\n",
            "[2762 | 2657.52] loss=1.59 avg=1.43\n",
            "[2763 | 2658.45] loss=1.35 avg=1.43\n",
            "[2764 | 2659.38] loss=1.43 avg=1.43\n",
            "[2765 | 2660.30] loss=1.50 avg=1.43\n",
            "[2766 | 2661.23] loss=1.42 avg=1.43\n",
            "[2767 | 2662.16] loss=1.81 avg=1.43\n",
            "[2768 | 2663.09] loss=1.69 avg=1.43\n",
            "[2769 | 2664.02] loss=1.39 avg=1.43\n",
            "[2770 | 2664.95] loss=1.70 avg=1.44\n",
            "[2771 | 2665.88] loss=1.62 avg=1.44\n",
            "[2772 | 2666.81] loss=1.42 avg=1.44\n",
            "[2773 | 2667.74] loss=1.22 avg=1.44\n",
            "[2774 | 2668.67] loss=1.81 avg=1.44\n",
            "[2775 | 2669.59] loss=1.21 avg=1.44\n",
            "[2776 | 2670.53] loss=1.61 avg=1.44\n",
            "[2777 | 2671.46] loss=1.73 avg=1.44\n",
            "[2778 | 2672.38] loss=1.47 avg=1.44\n",
            "[2779 | 2673.31] loss=1.65 avg=1.44\n",
            "[2780 | 2674.23] loss=2.03 avg=1.45\n",
            "[2781 | 2675.16] loss=1.42 avg=1.45\n",
            "[2782 | 2676.07] loss=1.80 avg=1.45\n",
            "[2783 | 2676.99] loss=1.32 avg=1.45\n",
            "[2784 | 2677.90] loss=1.60 avg=1.45\n",
            "[2785 | 2678.83] loss=1.29 avg=1.45\n",
            "[2786 | 2679.75] loss=1.35 avg=1.45\n",
            "[2787 | 2680.67] loss=1.51 avg=1.45\n",
            "[2788 | 2681.59] loss=1.41 avg=1.45\n",
            "[2789 | 2682.51] loss=1.54 avg=1.45\n",
            "[2790 | 2683.43] loss=1.44 avg=1.45\n",
            "[2791 | 2684.35] loss=1.43 avg=1.45\n",
            "[2792 | 2685.27] loss=1.61 avg=1.45\n",
            "[2793 | 2686.21] loss=1.14 avg=1.45\n",
            "[2794 | 2687.14] loss=1.05 avg=1.45\n",
            "[2795 | 2688.06] loss=1.25 avg=1.44\n",
            "[2796 | 2688.99] loss=2.16 avg=1.45\n",
            "[2797 | 2689.92] loss=1.29 avg=1.45\n",
            "[2798 | 2690.86] loss=1.62 avg=1.45\n",
            "[2799 | 2691.78] loss=1.07 avg=1.45\n",
            "[2800 | 2692.70] loss=1.25 avg=1.45\n",
            "[2801 | 2693.63] loss=1.31 avg=1.44\n",
            "[2802 | 2694.56] loss=1.49 avg=1.45\n",
            "[2803 | 2695.48] loss=1.53 avg=1.45\n",
            "[2804 | 2696.41] loss=1.30 avg=1.44\n",
            "[2805 | 2697.33] loss=1.46 avg=1.44\n",
            "[2806 | 2698.27] loss=1.34 avg=1.44\n",
            "[2807 | 2699.20] loss=1.12 avg=1.44\n",
            "[2808 | 2700.14] loss=1.24 avg=1.44\n",
            "[2809 | 2701.07] loss=1.32 avg=1.44\n",
            "[2810 | 2702.00] loss=2.27 avg=1.45\n",
            "[2811 | 2702.93] loss=1.66 avg=1.45\n",
            "[2812 | 2703.87] loss=1.42 avg=1.45\n",
            "[2813 | 2704.80] loss=1.11 avg=1.44\n",
            "[2814 | 2705.73] loss=1.89 avg=1.45\n",
            "[2815 | 2706.67] loss=1.90 avg=1.45\n",
            "[2816 | 2707.60] loss=1.56 avg=1.45\n",
            "[2817 | 2708.53] loss=1.25 avg=1.45\n",
            "[2818 | 2709.46] loss=1.16 avg=1.45\n",
            "[2819 | 2710.39] loss=3.40 avg=1.47\n",
            "[2820 | 2711.32] loss=1.67 avg=1.47\n",
            "[2821 | 2712.25] loss=1.41 avg=1.47\n",
            "[2822 | 2713.17] loss=1.34 avg=1.47\n",
            "[2823 | 2714.10] loss=1.48 avg=1.47\n",
            "[2824 | 2715.02] loss=2.13 avg=1.48\n",
            "[2825 | 2715.96] loss=1.42 avg=1.47\n",
            "[2826 | 2716.88] loss=2.16 avg=1.48\n",
            "[2827 | 2717.82] loss=1.40 avg=1.48\n",
            "[2828 | 2718.75] loss=1.36 avg=1.48\n",
            "[2829 | 2719.68] loss=1.34 avg=1.48\n",
            "[2830 | 2720.61] loss=1.48 avg=1.48\n",
            "[2831 | 2721.53] loss=1.51 avg=1.48\n",
            "[2832 | 2722.44] loss=1.77 avg=1.48\n",
            "[2833 | 2723.35] loss=1.38 avg=1.48\n",
            "[2834 | 2724.28] loss=1.46 avg=1.48\n",
            "[2835 | 2725.21] loss=1.38 avg=1.48\n",
            "[2836 | 2726.14] loss=1.31 avg=1.48\n",
            "[2837 | 2727.06] loss=1.13 avg=1.47\n",
            "[2838 | 2727.99] loss=1.78 avg=1.48\n",
            "[2839 | 2728.91] loss=1.12 avg=1.47\n",
            "[2840 | 2729.83] loss=1.21 avg=1.47\n",
            "[2841 | 2730.76] loss=1.18 avg=1.47\n",
            "[2842 | 2731.68] loss=1.08 avg=1.46\n",
            "[2843 | 2732.61] loss=1.17 avg=1.46\n",
            "[2844 | 2733.53] loss=1.32 avg=1.46\n",
            "[2845 | 2734.46] loss=1.03 avg=1.46\n",
            "[2846 | 2735.39] loss=1.21 avg=1.45\n",
            "[2847 | 2736.31] loss=1.83 avg=1.46\n",
            "[2848 | 2737.25] loss=1.42 avg=1.46\n",
            "[2849 | 2738.17] loss=1.11 avg=1.45\n",
            "[2850 | 2739.09] loss=1.44 avg=1.45\n",
            "[2851 | 2740.02] loss=1.42 avg=1.45\n",
            "[2852 | 2740.96] loss=1.55 avg=1.45\n",
            "[2853 | 2741.89] loss=1.35 avg=1.45\n",
            "[2854 | 2742.81] loss=1.21 avg=1.45\n",
            "[2855 | 2743.74] loss=1.27 avg=1.45\n",
            "[2856 | 2744.68] loss=1.22 avg=1.45\n",
            "[2857 | 2745.60] loss=1.32 avg=1.44\n",
            "[2858 | 2746.52] loss=1.39 avg=1.44\n",
            "[2859 | 2747.45] loss=1.48 avg=1.44\n",
            "[2860 | 2748.38] loss=2.60 avg=1.46\n",
            "[2861 | 2749.31] loss=1.63 avg=1.46\n",
            "[2862 | 2750.23] loss=1.95 avg=1.46\n",
            "[2863 | 2751.16] loss=1.70 avg=1.46\n",
            "[2864 | 2752.08] loss=1.44 avg=1.46\n",
            "[2865 | 2753.01] loss=1.56 avg=1.47\n",
            "[2866 | 2753.94] loss=2.62 avg=1.48\n",
            "[2867 | 2754.86] loss=3.26 avg=1.49\n",
            "[2868 | 2755.80] loss=1.32 avg=1.49\n",
            "[2869 | 2756.73] loss=1.40 avg=1.49\n",
            "[2870 | 2757.66] loss=1.65 avg=1.49\n",
            "[2871 | 2758.59] loss=1.33 avg=1.49\n",
            "[2872 | 2759.51] loss=1.80 avg=1.50\n",
            "[2873 | 2760.43] loss=1.44 avg=1.49\n",
            "[2874 | 2761.35] loss=1.74 avg=1.50\n",
            "[2875 | 2762.28] loss=1.33 avg=1.50\n",
            "[2876 | 2763.22] loss=1.34 avg=1.49\n",
            "[2877 | 2764.14] loss=1.41 avg=1.49\n",
            "[2878 | 2765.07] loss=1.28 avg=1.49\n",
            "[2879 | 2766.00] loss=1.66 avg=1.49\n",
            "[2880 | 2766.92] loss=1.41 avg=1.49\n",
            "[2881 | 2767.85] loss=1.09 avg=1.49\n",
            "[2882 | 2768.77] loss=1.17 avg=1.48\n",
            "[2883 | 2769.70] loss=1.04 avg=1.48\n",
            "[2884 | 2770.63] loss=1.61 avg=1.48\n",
            "[2885 | 2771.55] loss=1.24 avg=1.48\n",
            "[2886 | 2772.48] loss=1.21 avg=1.48\n",
            "[2887 | 2773.41] loss=1.25 avg=1.47\n",
            "[2888 | 2774.34] loss=1.43 avg=1.47\n",
            "[2889 | 2775.26] loss=0.98 avg=1.47\n",
            "[2890 | 2776.19] loss=1.15 avg=1.47\n",
            "[2891 | 2777.12] loss=1.37 avg=1.46\n",
            "[2892 | 2778.05] loss=1.19 avg=1.46\n",
            "[2893 | 2778.97] loss=1.64 avg=1.46\n",
            "[2894 | 2779.89] loss=1.41 avg=1.46\n",
            "[2895 | 2780.81] loss=1.59 avg=1.46\n",
            "[2896 | 2781.75] loss=1.44 avg=1.46\n",
            "[2897 | 2782.67] loss=0.96 avg=1.46\n",
            "[2898 | 2783.60] loss=1.35 avg=1.46\n",
            "[2899 | 2784.53] loss=1.04 avg=1.45\n",
            "[2900 | 2785.46] loss=2.34 avg=1.46\n",
            "[2901 | 2786.40] loss=1.40 avg=1.46\n",
            "[2902 | 2787.32] loss=1.19 avg=1.46\n",
            "[2903 | 2788.25] loss=0.91 avg=1.45\n",
            "[2904 | 2789.18] loss=1.30 avg=1.45\n",
            "[2905 | 2790.11] loss=1.31 avg=1.45\n",
            "[2906 | 2791.03] loss=2.26 avg=1.46\n",
            "[2907 | 2791.96] loss=1.36 avg=1.46\n",
            "[2908 | 2792.90] loss=1.40 avg=1.46\n",
            "[2909 | 2793.83] loss=1.55 avg=1.46\n",
            "[2910 | 2794.75] loss=1.44 avg=1.46\n",
            "[2911 | 2795.68] loss=1.41 avg=1.46\n",
            "[2912 | 2796.61] loss=1.22 avg=1.46\n",
            "[2913 | 2797.53] loss=1.37 avg=1.45\n",
            "[2914 | 2798.46] loss=1.17 avg=1.45\n",
            "[2915 | 2799.38] loss=2.05 avg=1.46\n",
            "[2916 | 2800.31] loss=2.05 avg=1.46\n",
            "[2917 | 2801.25] loss=1.14 avg=1.46\n",
            "[2918 | 2802.18] loss=1.24 avg=1.46\n",
            "[2919 | 2803.10] loss=1.64 avg=1.46\n",
            "[2920 | 2804.02] loss=1.39 avg=1.46\n",
            "[2921 | 2804.96] loss=1.24 avg=1.46\n",
            "[2922 | 2805.89] loss=1.38 avg=1.46\n",
            "[2923 | 2806.82] loss=1.35 avg=1.46\n",
            "[2924 | 2807.74] loss=1.24 avg=1.45\n",
            "[2925 | 2808.67] loss=1.33 avg=1.45\n",
            "[2926 | 2809.61] loss=1.03 avg=1.45\n",
            "[2927 | 2810.53] loss=1.05 avg=1.44\n",
            "[2928 | 2811.45] loss=1.27 avg=1.44\n",
            "[2929 | 2812.38] loss=1.23 avg=1.44\n",
            "[2930 | 2813.31] loss=1.20 avg=1.44\n",
            "[2931 | 2814.24] loss=0.91 avg=1.43\n",
            "[2932 | 2815.17] loss=2.33 avg=1.44\n",
            "[2933 | 2816.10] loss=1.54 avg=1.44\n",
            "[2934 | 2817.03] loss=1.38 avg=1.44\n",
            "[2935 | 2817.96] loss=2.10 avg=1.45\n",
            "[2936 | 2818.89] loss=1.50 avg=1.45\n",
            "[2937 | 2819.81] loss=1.54 avg=1.45\n",
            "[2938 | 2820.75] loss=2.21 avg=1.46\n",
            "[2939 | 2821.68] loss=2.00 avg=1.46\n",
            "[2940 | 2822.61] loss=1.13 avg=1.46\n",
            "[2941 | 2823.53] loss=1.42 avg=1.46\n",
            "[2942 | 2824.46] loss=1.55 avg=1.46\n",
            "[2943 | 2825.39] loss=1.62 avg=1.46\n",
            "[2944 | 2826.30] loss=1.45 avg=1.46\n",
            "[2945 | 2827.22] loss=1.28 avg=1.46\n",
            "[2946 | 2828.15] loss=1.47 avg=1.46\n",
            "[2947 | 2829.07] loss=1.29 avg=1.46\n",
            "[2948 | 2830.00] loss=1.46 avg=1.46\n",
            "[2949 | 2830.93] loss=1.42 avg=1.46\n",
            "[2950 | 2831.84] loss=1.43 avg=1.46\n",
            "[2951 | 2832.77] loss=1.41 avg=1.46\n",
            "[2952 | 2833.69] loss=1.33 avg=1.46\n",
            "[2953 | 2834.62] loss=1.37 avg=1.45\n",
            "[2954 | 2835.54] loss=1.15 avg=1.45\n",
            "[2955 | 2836.46] loss=1.12 avg=1.45\n",
            "[2956 | 2837.39] loss=1.63 avg=1.45\n",
            "[2957 | 2838.31] loss=1.23 avg=1.45\n",
            "[2958 | 2839.24] loss=1.28 avg=1.45\n",
            "[2959 | 2840.17] loss=1.13 avg=1.44\n",
            "[2960 | 2841.09] loss=1.68 avg=1.45\n",
            "[2961 | 2842.02] loss=1.18 avg=1.44\n",
            "[2962 | 2842.93] loss=1.61 avg=1.44\n",
            "[2963 | 2843.86] loss=1.35 avg=1.44\n",
            "[2964 | 2844.78] loss=1.10 avg=1.44\n",
            "[2965 | 2845.71] loss=1.40 avg=1.44\n",
            "[2966 | 2846.63] loss=1.49 avg=1.44\n",
            "[2967 | 2847.55] loss=1.39 avg=1.44\n",
            "[2968 | 2848.48] loss=1.43 avg=1.44\n",
            "[2969 | 2849.41] loss=1.27 avg=1.44\n",
            "[2970 | 2850.34] loss=1.03 avg=1.43\n",
            "[2971 | 2851.26] loss=1.78 avg=1.44\n",
            "[2972 | 2852.19] loss=1.39 avg=1.44\n",
            "[2973 | 2853.12] loss=1.56 avg=1.44\n",
            "[2974 | 2854.04] loss=1.90 avg=1.44\n",
            "[2975 | 2854.97] loss=1.27 avg=1.44\n",
            "[2976 | 2855.89] loss=1.41 avg=1.44\n",
            "[2977 | 2856.82] loss=1.12 avg=1.44\n",
            "[2978 | 2857.75] loss=1.36 avg=1.44\n",
            "[2979 | 2858.67] loss=1.45 avg=1.44\n",
            "[2980 | 2859.60] loss=1.84 avg=1.44\n",
            "[2981 | 2860.52] loss=2.06 avg=1.45\n",
            "[2982 | 2861.44] loss=1.22 avg=1.44\n",
            "[2983 | 2862.37] loss=1.44 avg=1.44\n",
            "[2984 | 2863.28] loss=1.22 avg=1.44\n",
            "[2985 | 2864.22] loss=1.30 avg=1.44\n",
            "[2986 | 2865.14] loss=0.99 avg=1.44\n",
            "[2987 | 2866.06] loss=1.47 avg=1.44\n",
            "[2988 | 2866.99] loss=1.27 avg=1.44\n",
            "[2989 | 2867.91] loss=1.30 avg=1.43\n",
            "[2990 | 2868.84] loss=0.72 avg=1.43\n",
            "[2991 | 2869.77] loss=1.23 avg=1.42\n",
            "[2992 | 2870.69] loss=1.27 avg=1.42\n",
            "[2993 | 2871.62] loss=1.44 avg=1.42\n",
            "[2994 | 2872.55] loss=1.79 avg=1.43\n",
            "[2995 | 2873.47] loss=1.20 avg=1.42\n",
            "[2996 | 2874.41] loss=1.62 avg=1.43\n",
            "[2997 | 2875.34] loss=1.27 avg=1.43\n",
            "[2998 | 2876.27] loss=1.08 avg=1.42\n",
            "[2999 | 2877.19] loss=1.49 avg=1.42\n",
            "Saving checkpoint/run1/model-3000\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "cello: si, es el pago de verme\n",
            "[08/14/18 3:32:00 p.m.] Kenia Porcell Privado: Ahora fue queda q usted muy fuerte y tanto quienes\n",
            "[08/14/18 3:32:10 p.m.] Kenia Porcell Privado: Y la asamblea o recibirla la llamada\n",
            "[08/14/18 3:33:07 p.m.] Kenia Porcell Privado: Y se la hare del día donde un desahuyé\n",
            "[08/14/18 3:33:40 p.m.] Kenia Porcell Privado: Hay q se hablará q ya la noticia\n",
            "[08/14/18 3:34:23 p.m.] Kenia Porcell Privado: Usted más bajó del 5 serátido para los q le hizo algo\n",
            "[08/14/18 3:34:51 p.m.] Kenia Porcell Privado: Q su hijo está con suyo\n",
            "[08/14/18 3:35:09 p.m.] Jj: Lo cuente\n",
            "[08/14/18 3:35:11 p.m.] Kenia Porcell Privado: X eso a los tiempos de los h allínos conmigo\n",
            "[08/14/18 3:35:16 p.m.] Kenia Porcell Privado: Ayer me vi una entrevista con él. Me gustaría q pueda de ir a la asamblea\n",
            "[08/14/18 3:35:32 p.m.] Kenia Porcell Privado: Está allí saben q hay q la noticia con especuya y usted me dio al mp\n",
            "[08/14/18 3:35:41 p.m.] Kenia Porcell Privado: Y esos ejecutivo q se podría hoy, usted es importante del mp\n",
            "[08/14/18 3:35:58 p.m.] Jj: Me gusta\n",
            "[08/14/18 6:11:03 p.m.] Kenia Porcell Privado: Sr. oportuno porque cuando usted le cosa Sr. q ahora se lo entendió x también  xq usted la verdad!!\n",
            "[08/14/18 8:21:18 p.m.] Kenia Porcell Privado: Hola, lugar, no es muy fuerte\n",
            "‎[08/14/18 8:21:24 p.m.] Kenia Porcell Privado: ‎<adjunto: 00001011-PHOTO-2018-08-14-18-21-24.jpg>\n",
            "[08/14/18 8:22:08 p.m.] Kenia Porcell Privado: Ese atacarría es muy delicado\n",
            "[08/14/18 8:22:32 p.m.] Kenia Porcell Privado: No me voy de la ley y yo me voy en chuvcello\n",
            "[08/14/18 8:23:02 p.m.] Kenia Porcell Privado: Le gustó con usted y me dijo q me quedaron todo verla en las procuras xq se llevan la ley de usted ya. Parece fue a parte de su hijo y q me puede ser asistencia\n",
            "‎[08/14/18 8:23:17 p.m.] Kenia Porcell Privado: ‎<adjunto: 00001012-PHOTO-2018-08-14-18-23-16.jpg>\n",
            "‎[08/14/18 8:23:17 p.m.] Kenia Porcell Privado: ‎<adjunto: 00001013-PHOTO-2018-08-14-18-23-18.jpg>\n",
            "[08/14/18 8:24:01 p.m.] Kenia Porcell Privado: Parece hoy x ser a la ley de usted y q se me podría en la noticia\n",
            "[08/14/18 8:24:30 p.m.] Jj: Esta espacando en día\n",
            "[08/14/18 8:24:55 p.m.] Kenia Porcell Privado: Con el pueblo\n",
            "[08/14/18 8:25:02 p.m.] Jj: La vam a llegar a paz porque cuando a un tema al país será fuer\n",
            "\n",
            "[3000 | 2897.63] loss=0.93 avg=1.42\n",
            "[3001 | 2898.55] loss=1.17 avg=1.42\n",
            "[3002 | 2899.47] loss=2.77 avg=1.43\n",
            "[3003 | 2900.40] loss=1.20 avg=1.43\n",
            "[3004 | 2901.34] loss=1.77 avg=1.43\n",
            "[3005 | 2902.26] loss=2.21 avg=1.44\n",
            "[3006 | 2903.19] loss=1.40 avg=1.44\n",
            "[3007 | 2904.12] loss=1.58 avg=1.44\n",
            "[3008 | 2905.05] loss=1.58 avg=1.44\n",
            "[3009 | 2905.97] loss=1.29 avg=1.44\n",
            "[3010 | 2906.90] loss=1.88 avg=1.44\n",
            "[3011 | 2907.83] loss=1.09 avg=1.44\n",
            "[3012 | 2908.76] loss=1.99 avg=1.44\n",
            "[3013 | 2909.68] loss=1.79 avg=1.45\n",
            "[3014 | 2910.61] loss=1.69 avg=1.45\n",
            "[3015 | 2911.54] loss=1.00 avg=1.45\n",
            "[3016 | 2912.48] loss=2.40 avg=1.46\n",
            "[3017 | 2913.40] loss=1.24 avg=1.45\n",
            "[3018 | 2914.33] loss=1.60 avg=1.45\n",
            "[3019 | 2915.25] loss=1.38 avg=1.45\n",
            "[3020 | 2916.18] loss=1.52 avg=1.45\n",
            "[3021 | 2917.11] loss=1.28 avg=1.45\n",
            "[3022 | 2918.03] loss=1.48 avg=1.45\n",
            "[3023 | 2918.97] loss=1.17 avg=1.45\n",
            "[3024 | 2919.90] loss=1.11 avg=1.45\n",
            "[3025 | 2920.83] loss=1.41 avg=1.45\n",
            "[3026 | 2921.76] loss=1.23 avg=1.44\n",
            "[3027 | 2922.69] loss=1.55 avg=1.45\n",
            "interrupted\n",
            "Saving checkpoint/run1/model-3028\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--SGwYZcgacM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir seguridad\n",
        "!cp checkpoint/run1/checkpoint seguridad\n",
        "!cp checkpoint/run1/model-3028.data-00000-of-00001 seguridad\n",
        "!cp checkpoint/run1/model-3028.index seguridad\n",
        "!cp checkpoint/run1/model-3028.meta seguridad"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80tTd4zpuxq-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp models/124M/encoder.json seguridad\n",
        "!cp models/124M/hparams.json seguridad\n",
        "!cp models/124M/vocab.bpe seguridad"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODhPFBWSvR9J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp gpt-2/src/interactive_conditional_samples.py seguridad\n",
        "!cp gpt-2/src/generate_unconditional_samples.py seguridad\n",
        "!cp gpt-2/src/model.py seguridad\n",
        "!cp gpt-2/src/sample.py seguridad\n",
        "!cp gpt-2/src/encoder.py seguridad\n",
        "!cp gpt-2/src/encode.py seguridad\n",
        "!mv seguridad models"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_loWXIDvHVJ",
        "colab_type": "code",
        "outputId": "619e46f8-90f4-411d-c3de-3d6e8be1c27e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python models/seguridad/generate_unconditional_samples.py --model_name seguridad"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/models/seguridad/model.py:147: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
            "\n",
            "WARNING:tensorflow:From models/seguridad/generate_unconditional_samples.py:52: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "2019-11-14 02:40:33.918193: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
            "2019-11-14 02:40:33.936744: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-11-14 02:40:33.937572: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-11-14 02:40:33.937859: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-11-14 02:40:33.939018: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
            "2019-11-14 02:40:33.940135: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\n",
            "2019-11-14 02:40:33.940509: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\n",
            "2019-11-14 02:40:33.941901: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2019-11-14 02:40:33.942870: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2019-11-14 02:40:33.945921: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-11-14 02:40:33.946074: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-11-14 02:40:33.946868: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-11-14 02:40:33.947599: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
            "2019-11-14 02:40:33.952592: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
            "2019-11-14 02:40:33.952802: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x251b480 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2019-11-14 02:40:33.952831: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2019-11-14 02:40:34.006670: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-11-14 02:40:34.007511: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x251b640 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2019-11-14 02:40:34.007544: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7\n",
            "2019-11-14 02:40:34.007742: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-11-14 02:40:34.008452: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-11-14 02:40:34.008530: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-11-14 02:40:34.008576: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
            "2019-11-14 02:40:34.008622: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\n",
            "2019-11-14 02:40:34.008667: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\n",
            "2019-11-14 02:40:34.008704: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2019-11-14 02:40:34.008750: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2019-11-14 02:40:34.008787: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-11-14 02:40:34.008905: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-11-14 02:40:34.009757: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-11-14 02:40:34.010528: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
            "2019-11-14 02:40:34.010596: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-11-14 02:40:34.012190: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-11-14 02:40:34.012225: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \n",
            "2019-11-14 02:40:34.012240: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \n",
            "2019-11-14 02:40:34.012494: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-11-14 02:40:34.013290: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-11-14 02:40:34.013994: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2019-11-14 02:40:34.014044: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10805 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
            "WARNING:tensorflow:From models/seguridad/generate_unconditional_samples.py:54: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/models/seguridad/model.py:148: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/models/seguridad/model.py:152: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/models/seguridad/model.py:36: The name tf.rsqrt is deprecated. Please use tf.math.rsqrt instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/models/seguridad/model.py:166: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/models/seguridad/sample.py:65: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "WARNING:tensorflow:From /content/models/seguridad/sample.py:70: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.random.categorical` instead.\n",
            "WARNING:tensorflow:From models/seguridad/generate_unconditional_samples.py:63: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "2019-11-14 02:40:39.804380: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
            "======================================== SAMPLE 1 ========================================\n",
            "Le déactualmente lo de la autoridad\n",
            "[07/30/18 10:52:04 p.m.] Kenia Porcell Privado: ?\n",
            "[07/30/18 10:52:32 p.m.] Jj: Sii repyo q si\n",
            "[07/30/18 10:52:59 p.m.] Kenia Porcell Privado: Severo el único que precio .  Yo sé q en eñ.\n",
            "[07/30/18 10:53:30 p.m.] Kenia Porcell Privado: Es más formalizar en esa historia\n",
            "[07/30/18 10:53:39 p.m.] Kenia Porcell Privado: AMANDA\n",
            "[07/30/18 10:54:13 p.m.] Kenia Porcell Privado: Lo q abió un grupo\n",
            "[07/30/18 10:54:37 p.m.] Kenia Porcell Privado: Pero eso no armará\n",
            "[07/30/18 11:04:00 p.m.] Jj: Si no entiende\n",
            "[07/30/18 11:04:16 p.m.] Jj: Y va es importante\n",
            "[07/30/18 11:46:18 p.m.] Kenia Porcell Privado: Si\n",
            "[07/30/18 11:46:39 p.m.] Kenia Porcell Privado: Vio el pedimiento\n",
            "[07/30/18 11:53:00 p.m.] Jj: Asuvias\n",
            "[07/30/18 11:53:06 p.m.] Jj: Les sis el domingo\n",
            "[07/30/18 11:53:10 p.m.] Jj: Junta en la habermeces el domingo\n",
            "[07/30/18 11:53:11 p.m.] Jj: No me gustar\n",
            "[07/30/18 12:04:31 p.m.] Kenia Porcell Privado: Hola Sr Sr fuerte una penetracion de q no todos los hijos, que se ponen turbulante el punto es usted y q no es fuerte cuando mejor prueba a los respaldo a  mién palabra .\n",
            "[07/30/18 12:05:16 p.m.] Kenia Porcell Privado: Y  soy y yo x tener a esa segunda en sus\n",
            "[07/30/18 12:06:02 p.m.] Jj: Respuesta\n",
            "[07/30/18 12:06:03 p.m.] Kenia Porcell Privado: X otro temprano\n",
            "[07/30/18 12:06:20 p.m.] Kenia Porcell Privado: Si\n",
            "[07/30/18 12:06:30 p.m.] Kenia Porcell Privado: Es esto Le contancia hoy\n",
            "[07/30/18 12:06:47 p.m.] Kenia Porcell Privado: Panama me asabe bien\n",
            "[07/30/18 12:06:48 p.m.] Kenia Porcell Privado: El dificil ni cuenta\n",
            "[07/30/18 12:06:54 p.m.] Kenia Porcell Privado: He susto\n",
            "[07/30/18 12:07:15 p.m.] Jj: Pero lo recomendodo muy bien\n",
            "[07/30/18 12:07:29 p.m.] Jj: Valbo, solo al embajador, neuvriand, daniel nicoli\n",
            "[07/30/18 12:07:41 p.m.] Kenia Porcell Privado: X favor\n",
            "[07/30/18 12:08:14 p.m.] Jj: Jajaajaj ministane\n",
            "[07/30/18 12:08:17 p.m.] Kenia Porcell Privado: Usted\n",
            "[07/30/18 12:09:05 p.m.] Jj: Ese entiende lo día\n",
            "[07/30/18 12:09:16 p.m.] Jj: Desregare\n",
            "‎[07/30/18 12:10:04 p.m.] Kenia Porcell Privado: ‎<adjunto: 0000891-PHOTO-2018-07-30-12-10-04.jpg>\n",
            "‎[07/30/18 12:10:09 p.m.] Kenia Porcell Privado: ‎<adjunto: 0000892-PHOTO-2018-\n",
            "======================================== SAMPLE 2 ========================================\n",
            "PHOTO GALLERY Cuba 2018\n",
            "In 2018, China embajadas can make up 6% of all consulates and diplomatic post offices.\n",
            "Panama villa\n",
            "Embajador domingo\n",
            "Embajador domingo\n",
            "Embajador domingo\n",
            "Embajador domingo\n",
            "Embajada muitembla\n",
            "Embajada futbolista\n",
            "Embajada abajo\n",
            "Embajada carrecedado\n",
            "Embajada la sociedad\n",
            "Embajada la inversión por igual complicado\n",
            "La señora querellaba\n",
            "La señor verán los casos\n",
            "An imputado que riqueza en los colon , que te necesita  sur la apoyación\n",
            "Pero le dejara vuelo\n",
            "Entiendo y seleccion\n",
            "\n",
            "*COMUNICADO\n",
            "*Panamá/Getty*\n",
            "[09/24/18 7:54:48 p.m.] Jj: Digame\n",
            "[09/24/18 7:55:14 p.m.] Jj: Vuelver\n",
            "[09/24/18 8:01:32 p.m.] Kenia Porcell Privado: Gracias\n",
            "[09/24/18 8:03:50 p.m.] Kenia Porcell Privado: Q del reglamm\n",
            "[09/25/18 8:23:56 p.m.] Jj: Excelente\n",
            "[09/25/18 8:24:04 p.m.] Jj: De rich que le dije sera este bluel\n",
            "[09/25/18 8:24:29 p.m.] Kenia Porcell Privado: Verdad. Al incidente . Jaajaj . El tema\n",
            "[09/25/18 8:24:36 p.m.] Kenia Porcell Privado: Hay q tal seul alguna\n",
            "[09/25/18 8:24:46 p.m.] Kenia Porcell Privado: Si. Ya van a dolor y a usted\n",
            "[09/25/18 8:25:12 p.m.] Jj: Excelente votare en tonta\n",
            "[09/25/18 8:25:24 p.m.] Jj: Y la atender a nafsir\n",
            "[09/25/18 8:25:35 p.m.] Kenia Porcell Privado: Se va para inocente\n",
            "[09/25/18 8:25:41 p.m.] Kenia Porcell Privado: Si. Mañana\n",
            "[09/25/18 8:26:15 p.m.] Kenia Porcell Privado: A un hizo. El partido *cripturando con los nombres; en la hasta contenido\", que a aclarararlo el mp\n",
            "[09/25/18 8:26:39 p.m.] Kenia Porcell Privado: Y se para divor el envio sobreñara porque mandó al médica fiscal\n",
            "[09/25/18 8:26:57 p.m.] Kenia Porcell Privado: El viernes es q lo de julio\n",
            "[09/25/18 8:32:44 p.m.] Jj: Pray for much ado-em esta fuente\n",
            "[09/25/18 8:32:54 p.m.] Kenia Porcell Privado: Vaya la huez de seguridad\n",
            "[09/25/18 8:33:27 p.m.] Kenia Porcell Privado: Pero siento q siii dí\n",
            "[09/25/18 8:33:40 p.m.] Kenia Porcell Privado: Dígale 2- Cordoba\n",
            "[09/25/18 10:07:53 p.m.] Jj: Si\n",
            "[09/25/18 10:07:57 p.m.] Jj: Bien\n",
            "[09/25/18 10:08:27 p.m.] Jj: Hago\n",
            "[09/25/18 10:08:38 p.m.] Jj: Reine esa atentar su nombres\n",
            "[09/25/18 10:08:39 p.m.] Kenia Porcell Privado: Si\n",
            "[09/25/18 10:08:48 p.m.] Kenia Porcell Privado: La democracia no lo hubiese xq puede haber afectarlo\n",
            "[09/25/18 10:09:23 p.m.] Kenia Porcell Privado: Revisaron a Juan Carlos retiree\n",
            "[09/25/18 10:09:34 p.m.] Jj: Caso\n",
            "======================================== SAMPLE 3 ========================================\n",
            "[3/27/18 2:48:33 p.m.] Kenia Porcell Privado: Caso owaz dO 2017\n",
            "[3/27/18 2:48:49 p.m.] Kenia Porcell Privado: Y droándall a una tesoro\n",
            "[3/27/18 3:25:36 p.m.] Jj: Xuzo\n",
            "[3/27/18 3:34:33 p.m.] Kenia Porcell Privado: Les responder\n",
            "[3/27/18 4:51:16 p.m.] Jj: Igual es que pueda investigar uno\n",
            "[3/28/18 8:58:48 a.m.] Kenia Porcell Privado: Buenos días\n",
            "[3/28/18 8:59:31 a.m.] Kenia Porcell Privado: Esa info estratégico x mi\n",
            "[3/28/18 8:59:56 a.m.] Kenia Porcell Privado: Cómo podría un dejo la vida ode el golpea y el tiempo corrupcion\n",
            "[3/29/18 12:00:42 p.m.] Jj: Perfecto\n",
            "[3/29/18 12:04:54 p.m.] Kenia Porcell Privado: Sr\n",
            "[3/29/18 12:10:03 p.m.] Kenia Porcell Privado: Ojala cómo todavía y fue el tiempo\n",
            "[3/30/18 7:51:47 a.m.] Jj: Buenos dias procuradora\n",
            "[3/30/18 7:51:51 a.m.] Jj: Buenos dias procuradora\n",
            "[3/30/18 7:52:19 a.m.] Jj: Bueno, con eso estan voy a respirl n allá fue o helicje no encarga a la forma\n",
            "[3/30/18 7:52:56 a.m.] Kenia Porcell Privado: Mire q me meter q me amió a nadie\n",
            "[3/30/18 7:53:17 a.m.] Kenia Porcell Privado: Lo ocurre q vaya q mas que se allanna totalmente padí 3\n",
            "[3/30/18 7:55:16 a.m.] Jj: Yo pendiente\n",
            "[3/30/18 7:56:05 a.m.] Jj: Es un objetivo\n",
            "[3/30/18 7:56:13 a.m.] Jj: Y siento dicio todo eso\n",
            "[3/30/18 7:56:30 a.m.] Kenia Porcell Privado: Ellos son muy contentos\n",
            "[3/30/18 7:56:40 a.m.] Jj: Si es chat firme\n",
            "[3/30/18 7:56:56 a.m.] Kenia Porcell Privado: Igual la entiendo q no me diga o la vice\n",
            "[3/30/18 7:56:59 a.m.] Kenia Porcell Privado: Sequez estoy tomando un arqueta es un granado de corrupcion\n",
            "[3/30/18 7:57:19 a.m.] Kenia Porcell Privado: Con gusto q era me dyer habla\n",
            "[3/30/18 7:57:30 a.m.] Jj: Nada\n",
            "[3/30/18 7:57:46 a.m.] Jj: Procuradora @ 123 falcon Rd\n",
            "[3/30/18 7:57:51 a.m.] Jj: Para avionicar a realidad\n",
            "[3/30/18 7:58:10 a.m.] Kenia Porcell Privado: Cuente me superportó la consecuencia y el vice . Yo supo salvaro para se proceder\n",
            "[3/30/18 7:58:15 a.m.] Jj: *COMUNICADO*\n",
            "\n",
            "Los Gobierno de Meñán Realizado\n",
            "\n",
            "Teléfono: (2) 475-4840\n",
            "\n",
            "Fax: (225) 829-3644\n",
            "\n",
            "Comunicado: 850-2349\n",
            "\n",
            "Correo: 850-4436\n",
            "\n",
            "Este newsé a chequerario por la obras mediática\n",
            "\n",
            "Saludando toma un bombo\n",
            "\n",
            "Este dato de la trampa y los comisionados de Gobierno\n",
            "\n",
            "Y cuando lo hubiese,\n",
            "======================================== SAMPLE 4 ========================================\n",
            "[09/09/18 12:18:25 p.m.] Kenia Porcell Privado: Yo no lo tengo.\n",
            "[09/09/18 12:18:44 p.m.] Jj: Cambie esta me gustado\n",
            "[09/09/18 12:18:53 p.m.] Kenia Porcell Privado: Los ataqos no considera más:\n",
            "[09/09/18 1:02:18 p.m.] Jj: Oiga\n",
            "[09/09/18 1:02:23 p.m.] Jj: Mañaña?\n",
            "[09/09/18 1:02:28 p.m.] Jj: Comunicado\n",
            "[09/09/18 1:02:32 p.m.] Jj: Lo compren\n",
            "[09/09/18 1:02:46 p.m.] Jj: Estoy medique de consejo\n",
            "[09/09/18 1:02:50 p.m.] Kenia Porcell Privado: Lo q tuve da tenía q va a quien hace comox las entender las preocupaciones y como hay problemas\n",
            "[09/09/18 1:02:57 p.m.] Jj: Y si no nos amador es un comoda no seras\n",
            "[09/09/18 1:02:56 p.m.] Kenia Porcell Privado: Lo q no ocurrir\n",
            "[09/09/18 1:02:57 p.m.] Kenia Porcell Privado: No se preocupe.\n",
            "[09/09/18 1:03:04 p.m.] Jj: Ese toca a nivel cocle dolor\n",
            "[09/09/18 1:03:04 p.m.] Kenia Porcell Privado: Noo\n",
            "[09/09/18 1:03:09 p.m.] Jj: Bendonna 40 mil\n",
            "[09/09/18 1:29:56 p.m.] Kenia Porcell Privado: Ya humilde estan recibir ese sopresción del decisión\n",
            "[09/09/18 1:30:50 p.m.] Kenia Porcell Privado: Hizo indignados de diputados\n",
            "[09/09/18 1:31:31 p.m.] Kenia Porcell Privado: Pero provincará compromiso xq es el problema del prensa\n",
            "[09/09/18 1:32:09 p.m.] Jj: Lo vi bueno eso\n",
            "[09/09/18 1:32:34 p.m.] Kenia Porcell Privado: Listo\n",
            "[09/09/18 1:32:41 p.m.] Jj: El viceministo cercano\n",
            "[09/09/18 1:33:03 p.m.] Kenia Porcell Privado: Ojo Sr\n",
            "[09/09/18 1:33:10 p.m.] Kenia Porcell Privado: Nosotros\n",
            "[09/09/18 1:33:18 p.m.] Kenia Porcell Privado: Me, mañana\n",
            "[09/09/18 1:35:17 p.m.] Kenia Porcell Privado: Cabeza\n",
            "[09/09/18 1:31:48 p.m.] Kenia Porcell Privado: Noam Mayor. Noam mayor. Siempre si hay no suya a pinzon, me voy de bloquar mis demands... saque lo pasamente.\n",
            "Si pedaban más deben seguridad para ver como no se preocupe\n",
            "[09/09/18 1:31:57 p.m.] Jj: Con su candidato\n",
            "[09/09/18 1:31:57 p.m.] Kenia Porcell Privado: Se él me extradición de seguro\n",
            "[09/09/18 1:32:02 p.m.] Kenia Porcell Privado: Usted sabe cómo se informal\n",
            "[09/09/18 1:32:41 p.m.] Kenia Porcell Privado: 🎉\n",
            "[09/09/18 1:32:55 p.m.] Jj: Perfecto\n",
            "[09/09/18 1:32:58 p.m.] Jj: Lo de bloquara estaremos buenos\n",
            "[09/09/18 1:32:59 p.m.] Jj: Ese es el nosotros es lo estare lo de nal\n",
            "[09/09/18 1:34:09 p.m.] Jj: Como se va a\n",
            "Traceback (most recent call last):\n",
            "  File \"models/seguridad/generate_unconditional_samples.py\", line 77, in <module>\n",
            "    fire.Fire(sample_model)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/fire/core.py\", line 138, in Fire\n",
            "    component_trace = _Fire(component, args, parsed_flag_args, context, name)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/fire/core.py\", line 471, in _Fire\n",
            "    target=component.__name__)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/fire/core.py\", line 675, in _CallAndUpdateTrace\n",
            "    component = fn(*varargs, **kwargs)\n",
            "  File \"models/seguridad/generate_unconditional_samples.py\", line 69, in sample_model\n",
            "    out = sess.run(output)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\", line 956, in run\n",
            "    run_metadata_ptr)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\", line 1180, in _run\n",
            "    feed_dict_tensor, options, run_metadata)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\", line 1359, in _do_run\n",
            "    run_metadata)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\", line 1365, in _do_call\n",
            "    return fn(*args)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\", line 1350, in _run_fn\n",
            "    target_list, run_metadata)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\", line 1443, in _call_tf_sessionrun\n",
            "    run_metadata)\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xsFAUlwTvbBZ",
        "colab_type": "code",
        "outputId": "ad9fca43-3426-4e3f-8a77-9e999ab3653e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python models/seguridad/generate_unconditional_samples.py --temperature 0.85 --top_k 50 --model_name seguridad"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/models/seguridad/model.py:147: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
            "\n",
            "WARNING:tensorflow:From models/seguridad/generate_unconditional_samples.py:52: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "2019-11-14 02:42:57.046893: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
            "2019-11-14 02:42:57.065627: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-11-14 02:42:57.066502: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-11-14 02:42:57.066781: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-11-14 02:42:57.068085: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
            "2019-11-14 02:42:57.069236: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\n",
            "2019-11-14 02:42:57.069608: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\n",
            "2019-11-14 02:42:57.071014: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2019-11-14 02:42:57.072094: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2019-11-14 02:42:57.075428: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-11-14 02:42:57.075572: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-11-14 02:42:57.076673: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-11-14 02:42:57.077412: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
            "2019-11-14 02:42:57.082556: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
            "2019-11-14 02:42:57.082780: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x27cb480 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2019-11-14 02:42:57.082816: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2019-11-14 02:42:57.137063: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-11-14 02:42:57.137917: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x27cb640 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2019-11-14 02:42:57.137951: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7\n",
            "2019-11-14 02:42:57.138142: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-11-14 02:42:57.138863: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-11-14 02:42:57.138942: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-11-14 02:42:57.138983: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
            "2019-11-14 02:42:57.139040: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\n",
            "2019-11-14 02:42:57.139114: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\n",
            "2019-11-14 02:42:57.139160: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2019-11-14 02:42:57.139195: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2019-11-14 02:42:57.139269: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-11-14 02:42:57.139460: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-11-14 02:42:57.140200: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-11-14 02:42:57.140892: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
            "2019-11-14 02:42:57.140961: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-11-14 02:42:57.142619: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-11-14 02:42:57.142662: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \n",
            "2019-11-14 02:42:57.142689: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \n",
            "2019-11-14 02:42:57.142899: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-11-14 02:42:57.143868: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-11-14 02:42:57.144621: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2019-11-14 02:42:57.144682: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10805 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
            "WARNING:tensorflow:From models/seguridad/generate_unconditional_samples.py:54: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/models/seguridad/model.py:148: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/models/seguridad/model.py:152: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/models/seguridad/model.py:36: The name tf.rsqrt is deprecated. Please use tf.math.rsqrt instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/models/seguridad/model.py:166: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/models/seguridad/sample.py:65: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "WARNING:tensorflow:From /content/models/seguridad/sample.py:16: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /content/models/seguridad/sample.py:70: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.random.categorical` instead.\n",
            "WARNING:tensorflow:From models/seguridad/generate_unconditional_samples.py:63: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "2019-11-14 02:43:02.929278: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
            "======================================== SAMPLE 1 ========================================\n",
            "[08/17/18 10:09:17 a.m.] Kenia Porcell Privado: Acabo de salud. Deber como decirlos\n",
            "[08/17/18 10:09:34 a.m.] Kenia Porcell Privado: La espera\n",
            "[08/17/18 10:10:04 a.m.] Kenia Porcell Privado: Y llego a juan huesan y los hago en cesamarlo\n",
            "[08/17/18 10:10:30 a.m.] Jj: Asines\n",
            "[08/17/18 10:10:42 a.m.] Jj: Siii\n",
            "[08/17/18 10:11:04 a.m.] Jj: Que va a debe decir unos dias q si se han impuenta hoy\n",
            "[08/17/18 10:11:19 a.m.] Jj: Que va a estar como en la luz\n",
            "[08/17/18 10:12:13 a.m.] Kenia Porcell Privado: X eso.\n",
            "[08/17/18 10:13:02 a.m.] Kenia Porcell Privado: Y quiere los bajanos en la uaf\n",
            "[08/17/18 10:14:20 a.m.] Kenia Porcell Privado: Y yo no hubiese en su distintos de la fuerza.\n",
            "[08/17/18 10:14:33 a.m.] Kenia Porcell Privado: Y la visto\n",
            "[08/17/18 10:14:52 a.m.] Kenia Porcell Privado: Todo es el sábado\n",
            "[08/17/18 10:15:01 a.m.] Jj: No\n",
            "[08/17/18 10:15:08 a.m.] Jj: Que van a trabajar\n",
            "[08/17/18 10:15:20 a.m.] Jj: Y que ver\n",
            "[08/17/18 10:15:25 a.m.] Kenia Porcell Privado: Con muchos diferentes en la fuerza. No se haga tiempo\n",
            "[08/17/18 10:15:36 a.m.] Kenia Porcell Privado: Es el sábado Sr.\n",
            "[08/17/18 10:15:48 a.m.] Jj: Yo esta eso\n",
            "[08/17/18 10:16:02 a.m.] Jj: Lo que no le preguntar\n",
            "[08/17/18 10:16:16 a.m.] Jj: Es uno\n",
            "[08/17/18 10:16:18 a.m.] Jj: Todo es con muchas brasil\n",
            "[08/17/18 10:16:29 a.m.] Kenia Porcell Privado: A ver q usted me ha hecho\n",
            "[08/17/18 10:16:38 a.m.] Jj: No se preocupe\n",
            "[08/17/18 10:16:46 a.m.] Jj: Es una operación\n",
            "[08/17/18 10:16:54 a.m.] Jj: Sera el pobre ponerlo no estamos abrir\n",
            "[08/17/18 10:17:17 a.m.] Kenia Porcell Privado: Hola Sr. si. Si lo haré\n",
            "[08/17/18 10:18:12 a.m.] Kenia Porcell Privado: No es que se preocupe es el pobre ponerlo\n",
            "[08/17/18 10:18:20 a.m.] Kenia Porcell Privado: El paso q no hay\n",
            "[08/17/18 10:18:30 a.m.] Kenia Porcell Privado: Puede  para mi\n",
            "[08/17/18 10:18:37 a.m.] Kenia Porcell Privado: Y así\n",
            "[08/17/18 10:19:05 a.m.] Jj: Es una operación\n",
            "[08/17/18 10:19:12 a.m.] Jj: Para que las lunas\n",
            "[08/17/18 10:19:30 a.m.] Kenia Porcell Privado: Dios le poner.\n",
            "[08/17/18 10:19:42 a.m.] Kenia Porcell Privado: Y no se lo pasaron q siempre tiene\n",
            "[08/17/18 11:16:16 a.m.] Jj: Si es\n",
            "======================================== SAMPLE 2 ========================================\n",
            "[05/21/18 9:44:59 p.m.] Kenia Porcell Privado: Para más\n",
            "[05/21/18 9:45:05 p.m.] Kenia Porcell Privado: Que l quiero hoy una entrevista de un casa\n",
            "[05/21/18 9:45:16 p.m.] Kenia Porcell Privado: Pero si es como al menos me dice q estaría en un año de Panamá\n",
            "[05/21/18 9:53:25 p.m.] Jj: Como amanecio\n",
            "[05/21/18 9:55:23 p.m.] Kenia Porcell Privado: Como él\n",
            "[05/21/18 9:56:16 p.m.] Jj: Que los\n",
            "[05/21/18 9:56:30 p.m.] Kenia Porcell Privado: Xq dio su gobierno no le hablemos\n",
            "[05/21/18 9:56:50 p.m.] Jj: De acuerdo\n",
            "[05/21/18 9:56:58 p.m.] Jj: No le voy a citar\n",
            "[05/21/18 9:56:59 p.m.] Kenia Porcell Privado: Xq está en menos\n",
            "[05/21/18 9:56:59 p.m.] Jj: Le voy a mandar un golpe\n",
            "[05/21/18 9:57:02 p.m.] Jj: El paso a aprovechar\n",
            "[05/21/18 9:57:05 p.m.] Jj: Y le hablemos la pgn\n",
            "[05/21/18 9:57:06 p.m.] Jj: Puede ser\n",
            "‎[05/21/18 9:57:09 p.m.] Jj: ‎<adjunto: 00009500-PHOTO-2018-05-21-17-57-09.jpg>\n",
            "‎[05/21/18 9:57:19 p.m.] Jj: ‎<adjunto: 00009501-PHOTO-2018-05-21-17-57-19.jpg>\n",
            "[05/21/18 9:57:33 p.m.] Jj: Jajaa\n",
            "‎[05/21/18 9:57:36 p.m.] Jj: ‎<adjunto: 000098801-PHOTO-2018-05-21-17-57-36.jpg>\n",
            "[05/21/18 9:56:01 p.m.] Jj: Me gustado\n",
            "[05/21/18 9:56:12 p.m.] Jj: Voy a gustar\n",
            "[05/21/18 9:56:19 p.m.] Jj: El compromiso\n",
            "[05/21/18 9:56:48 p.m.] Jj: Pero en la info\n",
            "[05/21/18 9:56:57 p.m.] Jj: Que hoy\n",
            "[05/21/18 9:57:02 p.m.] Jj: Mire que no\n",
            "[05/21/18 9:57:16 p.m.] Jj: Pero sabe esta gente\n",
            "[05/21/18 9:57:37 p.m.] Jj: Hoy yo en la info\n",
            "[05/21/18 10:04:03 p.m.] Jj: Perfecgo\n",
            "[05/22/18 9:28:19 a.m.] Jj: Buenos dias procuradora\n",
            "[05/22/18 9:28:23 a.m.] Jj: Como amanecio\n",
            "[05/22/18 9:28:32 a.m.] Jj: Como amanecio\n",
            "[05/22/18 9:28:43 a.m.] Jj: Mire esto\n",
            "[05/22/18 9:28:52 a.m.] Jj: Como amanecio\n",
            "[05/22/18 9:28:55 a.m.] Jj: La asistencia del tema\n",
            "[05/22/18 9:29:07 a.m.] Jj: El compromiso es el ministro de funcionarios fiscal\n",
            "[05/22/18 9:29:15 a.m.] Kenia Porcell Privado: Mire esto\n",
            "[05/22/18 9:29:27 a.m.] Jj: Le voy a llegando\n",
            "[05/22/18 9:29:27 a.\n",
            "======================================== SAMPLE 3 ========================================\n",
            "Tacoma - In the moment, President Trump and China President Xi Jinping met in China on Wednesday.\n",
            "\n",
            "President Trump en line, in red, China President Xi, in blue, atmar\n",
            "‎[09/16/18 10:12:03 p.m.] Kenia Porcell Privado: ‎<adjunto: 00014650-PHOTO-2018-09-16-10-12-03.jpg>\n",
            "[09/16/18 10:12:05 p.m.] Kenia Porcell Privado: Y hoy q me va a acostumbrar el director de trabajo\n",
            "[09/16/18 10:59:58 p.m.] Jj: Siiii\n",
            "[09/16/18 10:59:58 p.m.] Kenia Porcell Privado: Para q  está usted y de su sintérmencia\n",
            "[09/16/18 11:00:12 p.m.] Jj: Yo sopese con el Presidente\n",
            "[09/16/18 11:00:30 p.m.] Kenia Porcell Privado: Lo puedo poner en la fuerza de uníficante\n",
            "[09/16/18 11:01:09 p.m.] Jj: De las dos\n",
            "[09/16/18 11:01:16 p.m.] Kenia Porcell Privado: Acánta\n",
            "[09/16/18 11:01:32 p.m.] Jj: Yo sabado yo a ver el tiempo\n",
            "[09/16/18 11:01:38 p.m.] Kenia Porcell Privado: Y la verdad!!\n",
            "[09/16/18 11:01:42 p.m.] Kenia Porcell Privado: Y x eso q algo no es asumíto\n",
            "[09/16/18 11:02:25 p.m.] Jj: Si\n",
            "[09/16/18 11:02:30 p.m.] Jj: Eso es su mensaje\n",
            "[09/16/18 11:02:33 p.m.] Kenia Porcell Privado: Solo q el tiempo hay q hoy estaba de su tema\n",
            "[09/16/18 11:03:06 p.m.] Jj: Eso es su\n",
            "[09/16/18 11:03:15 p.m.] Jj: Yo voy a tener darle\n",
            "[09/16/18 11:03:23 p.m.] Jj: Esta\n",
            "[09/16/18 11:03:49 p.m.] Jj: Me viviera\n",
            "[09/16/18 11:03:53 p.m.] Kenia Porcell Privado: Mire q el sábado no es. X eso no es\n",
            "[09/16/18 11:03:56 p.m.] Kenia Porcell Privado: Si\n",
            "[09/16/18 11:04:29 p.m.] Kenia Porcell Privado: La verdad!!\n",
            "[09/16/18 11:04:33 p.m.] Jj: Eso es su\n",
            "[09/16/18 11:04:37 p.m.] Jj: Eso es su\n",
            "[09/16/18 11:04:44 p.m.] Kenia Porcell Privado: Solo q no es\n",
            "[09/16/18 11:04:54 p.m.] Jj: Toca con el prd\n",
            "[09/16/18 11:05:05 p.m.] Kenia Porcell Privado: Pero ya no estar acostumbrada\n",
            "[09/16/18 11:05:10 p.m.] Kenia Porcell Privado: Pero para ubicarme esto \n",
            "Pienso q aceptigos q con mada\n",
            "[09/16/18 11:05:16 p.m.] Jj: La llamo\n",
            "[09/16/18 11:05:14 p.m.] Kenia Porcell Privado: Pero ustedes q la única es asumen\n",
            "[09/16/18 11:05:23 p.m.] Jj: Le llevo el juego\n",
            "[09/16/18 11:05:28 p.m.] Kenia Porcell Privado: Pero yo no dieron q poner la verdad\n",
            "[09/16/18 11:05:56 p.m.] Jj: El objetivo\n",
            "[09/16/18 11:06:03 p.m.] Kenia Porcell Privado: Sr\n",
            "[09/\n",
            "======================================== SAMPLE 4 ========================================\n",
            "Video\n",
            "\n",
            "A policewoman was hospitalised due to a fall, hospital officials have been told.\n",
            "\n",
            "Nadia Alvarado, 50, and her son Jose, 12, moved to the capital, Caracas, in late 2009.\n",
            "\n",
            "Alvarado, a personal doctor, had moved to her China home in 2009.\n",
            "\n",
            "Video footage of the moment her hospitalised her at 2am on 4 May.\n",
            "\n",
            "Nadia Alvarado, hospital directoratía de hospitalispecifico de China, hospitalizada\n",
            "\n",
            "Nadia Alvarado, directoratía de hospitalispecifico de China, hospitalizada localidad de hospitalispecifica\n",
            "\n",
            "Nadia Alvarado, directoratía de hospitalispecifico de China, hospitalizada hospitalizó afectar afectados en la prensa\n",
            "\n",
            "Trevor Ulloa, Presidente de la República de Panamá, hospitalidad líbremeto\n",
            "\n",
            "Zoico de la República de China, Panamá\n",
            "\n",
            "Este de la República de China, China\n",
            "\n",
            "Video from La Prensa\n",
            "\n",
            "2018-06-05 12:48:35\n",
            "Presidentialre\n",
            "‎: ‎<adjunto: 00008005-PHOTO-2018-06-05 12:48:35.rar>\n",
            "[06/05/18 11:26:33 a.m.] Jj: Como esta procuradora\n",
            "[06/05/18 12:39:02 a.m.] Kenia Porcell Privado: Esto es una fiesta\n",
            "[06/05/18 12:44:43 a.m.] Jj: Ya se les comunicado q tomo como en la otra semana\n",
            "[06/05/18 1:25:38 p.m.] Kenia Porcell Privado: Sr vaya si puede\n",
            "[06/05/18 2:35:23 p.m.] Jj: Lo harvin en chiquito\n",
            "[06/05/18 2:35:30 p.m.] Jj: La verdad\n",
            "[06/05/18 2:35:35 p.m.] Jj: Eso los ataques\n",
            "[06/05/18 2:38:09 p.m.] Kenia Porcell Privado: Usted la culpa, esto es lo q hace en chiquito\n",
            "[06/05/18 3:30:01 p.m.] Jj: Excelente\n",
            "[06/05/18 5:56:16 p.m.] Kenia Porcell Privado: Buenos días Sr\n",
            "[06/05/18 5:56:20 p.m.] Kenia Porcell Privado: Estoy apoyando q el fiscal general de panama\n",
            "[06/05/18 6:04:17 p.m.] Jj: Que bien\n",
            "[06/05/18 6:04:29 p.m.] Jj: Yo estoy en chiquito\n",
            "[06/05/18 6:04:43 p.m.] Kenia Porcell Privado: Mañana haya de la selección\n",
            "[06/05/18 6:05:03 p.m.] Jj: A en chiquito\n",
            "[06/05/18 7:24:12 p.m.] Kenia Porcell Privado: Tengo a todo el mundo\n",
            "[06/05/18 7:30:59 p.m.] Jj: Que bien\n",
            "[06/05/18 7:41:45 p.m.] Kenia Porcell Privado: Esto es el mundo\n",
            "[06/05/18 7:41:54 p.m.] Kenia Porcell Privado: El pueblo más pienso q es una conversación de los comisionados se lo debe acá\n",
            "[06/05/18 7:41:59 p.m.] Kenia Porcell Privado: X eso me voy a más tiempo\n",
            "[06/05/18 7:43:12 p.m.] Jj: Ya le explica\n",
            "[06/05/18 7:49:51 p.m.] Jj: Si la espero es una diálogo\n",
            "[06/05/18 8:15:01 p.m.] Kenia Porcell Privado: Sr\n",
            "[06/05/18 8:15:36 p.m.] Jj: La espero\n",
            "[06/05/18 8:15:48 p.m.] Kenia Porcell Privado: Se lo mande y me mandó xq los nive\n",
            "======================================== SAMPLE 5 ========================================\n",
            "[09/11/18 12:36:04 p.m.] Kenia Porcell Privado: Yo están diciendo q se preocupe.\n",
            "[09/11/18 12:41:15 p.m.] Jj: Y a mi respuesta\n",
            "[09/11/18 12:41:29 p.m.] Jj: Se lo mande\n",
            "[09/11/18 12:45:48 p.m.] Kenia Porcell Privado: Ya están diciendo q usted van a alandes\n",
            "[09/11/18 12:46:09 p.m.] Jj: Me avisa x favor\n",
            "[09/11/18 12:51:31 p.m.] Kenia Porcell Privado: Así q es lo mismo de esos crear ese social\n",
            "[09/11/18 2:44:41 p.m.] Kenia Porcell Privado: Pero si lo q me da es importante es q el lunes pared out\n",
            "[09/11/18 2:45:38 p.m.] Jj: Pero se lo que usted es xq es un cuenta\n",
            "[09/11/18 2:45:44 p.m.] Kenia Porcell Privado: No. Pienso q se puede hacer x favor\n",
            "[09/11/18 2:49:10 p.m.] Jj: No sabia mañana\n",
            "[09/11/18 2:49:42 p.m.] Jj: Le llame a la veo\n",
            "[09/11/18 2:49:49 p.m.] Jj: No se fue com el ánimo esposado\n",
            "[09/11/18 2:49:56 p.m.] Jj: Que locura\n",
            "[09/11/18 4:10:12 p.m.] Jj: Ese es el\n",
            "[09/11/18 4:10:15 p.m.] Jj: En ese mensaje dio esa\n",
            "[09/11/18 4:10:20 p.m.] Jj: Jajaja\n",
            "[09/11/18 4:10:21 p.m.] Jj: Esta en campaña\n",
            "[09/11/18 4:45:51 p.m.] Kenia Porcell Privado: Jaja\n",
            "[09/11/18 4:46:04 p.m.] Kenia Porcell Privado: Jaja\n",
            "[09/11/18 4:46:11 p.m.] Kenia Porcell Privado: Y con usted\n",
            "[09/11/18 4:46:33 p.m.] Kenia Porcell Privado: Acaba de acertar y de nuevo\n",
            "[09/11/18 4:46:39 p.m.] Jj: Yo le va a poner a haber la llamo\n",
            "[09/11/18 4:46:59 p.m.] Jj: Y la veo\n",
            "[09/11/18 4:47:15 p.m.] Jj: Usted es el campoña\n",
            "[09/11/18 5:00:11 p.m.] Jj: Le mande a la campaña\n",
            "[09/11/18 5:04:56 p.m.] Kenia Porcell Privado: Mire Sr\n",
            "[09/11/18 5:05:00 p.m.] Kenia Porcell Privado: Yo estaré q eso xq no lo cuestan una nueva política\n",
            "[09/11/18 5:05:39 p.m.] Kenia Porcell Privado: En estos míos\n",
            "[09/11/18 5:10:14 p.m.] Jj: Así es el\n",
            "[09/11/18 5:10:38 p.m.] Jj: Yo esto\n",
            "[09/11/18 5:11:10 p.m.] Jj: Y le digo\n",
            "[09/11/18 6:28:02 p.m.] Kenia Porcell Privado: Gracias.\n",
            "[09/11/18 6:28:06 p.m.] Kenia Porcell Privado: Ya un vuelo bien\n",
            "[09/11/18 6:30:27 p.m.] Kenia Porcell Privado: Sr. no no lo agradeZo\n",
            "‎[09/11/18 6:31:12 p.m.] Jj: ‎<adjunto: 00011068-PHOTO-2018-09-11-12-31-12.jpg>\n",
            "‎[09/\n",
            "^C\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wWY1jp3xE8g",
        "colab_type": "code",
        "outputId": "d5b1f47d-5954-42f2-ddf6-038f77148b01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python models/seguridad/interactive_conditional_samples.py --temperature 0.85 --top_k 50 --model_name seguridad"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/models/seguridad/model.py:147: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
            "\n",
            "WARNING:tensorflow:From models/seguridad/interactive_conditional_samples.py:55: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "2019-11-14 02:59:02.308649: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
            "2019-11-14 02:59:02.327552: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-11-14 02:59:02.328328: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-11-14 02:59:02.328627: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-11-14 02:59:02.329887: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
            "2019-11-14 02:59:02.331107: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\n",
            "2019-11-14 02:59:02.331595: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\n",
            "2019-11-14 02:59:02.333145: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2019-11-14 02:59:02.334242: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2019-11-14 02:59:02.337839: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-11-14 02:59:02.337983: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-11-14 02:59:02.338805: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-11-14 02:59:02.339669: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
            "2019-11-14 02:59:02.345037: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
            "2019-11-14 02:59:02.345264: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1983480 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2019-11-14 02:59:02.345299: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2019-11-14 02:59:02.401246: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-11-14 02:59:02.402104: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1983640 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2019-11-14 02:59:02.402137: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7\n",
            "2019-11-14 02:59:02.402379: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-11-14 02:59:02.403092: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-11-14 02:59:02.403172: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-11-14 02:59:02.403215: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
            "2019-11-14 02:59:02.403267: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\n",
            "2019-11-14 02:59:02.403310: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\n",
            "2019-11-14 02:59:02.403379: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2019-11-14 02:59:02.403421: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2019-11-14 02:59:02.403458: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-11-14 02:59:02.403583: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-11-14 02:59:02.404417: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-11-14 02:59:02.405137: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
            "2019-11-14 02:59:02.405204: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-11-14 02:59:02.406780: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-11-14 02:59:02.406984: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \n",
            "2019-11-14 02:59:02.407014: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \n",
            "2019-11-14 02:59:02.407253: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-11-14 02:59:02.408050: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-11-14 02:59:02.408753: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2019-11-14 02:59:02.408806: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10805 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
            "WARNING:tensorflow:From models/seguridad/interactive_conditional_samples.py:56: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From models/seguridad/interactive_conditional_samples.py:58: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/models/seguridad/model.py:148: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/models/seguridad/model.py:152: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/models/seguridad/model.py:36: The name tf.rsqrt is deprecated. Please use tf.math.rsqrt instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/models/seguridad/model.py:166: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/models/seguridad/sample.py:65: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "WARNING:tensorflow:From /content/models/seguridad/sample.py:16: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /content/models/seguridad/sample.py:70: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.random.categorical` instead.\n",
            "WARNING:tensorflow:From models/seguridad/interactive_conditional_samples.py:66: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "Model prompt >>> [11/14/19 7:16:22 p.m.] Jj: Que bello mi Panama, verdad\n",
            "2019-11-14 02:59:31.301852: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
            "======================================== SAMPLE 1 ========================================\n",
            "\n",
            "[11/14/19 7:16:38 p.m.] Kenia Porcell Privado: Igual\n",
            "[11/14/19 7:16:46 p.m.] Jj: Lo se\n",
            "[11/14/19 7:16:51 p.m.] Jj: La llamo es un poco\n",
            "[11/14/19 7:16:56 p.m.] Jj: Es a todo\n",
            "[11/14/19 7:16:54 p.m.] Kenia Porcell Privado: No me voy a cambiar.\n",
            "[11/14/19 7:17:06 p.m.] Jj: Parece todo ese pocado\n",
            "[11/14/19 7:17:07 p.m.] Kenia Porcell Privado: Debe queriendo quedar el primer poder\n",
            "[11/14/19 7:17:51 p.m.] Jj: Jajaaj ministro\n",
            "[11/14/19 7:18:12 p.m.] Kenia Porcell Privado: La verdad?\n",
            "[11/14/19 7:18:38 p.m.] Kenia Porcell Privado: A mi\n",
            "[11/14/19 7:18:53 p.m.] Kenia Porcell Privado: Eso es q se vio en un poner la cara\n",
            "[11/14/19 7:19:09 p.m.] Jj: Perfecto\n",
            "[11/14/19 7:19:19 p.m.] Jj: Yo no me llevo la cambie\n",
            "[11/14/19 7:19:32 p.m.] Kenia Porcell Privado: También\n",
            "[11/14/19 7:19:53 p.m.] Kenia Porcell Privado: Esto es q se va a poner a la cara\n",
            "[11/14/19 7:20:15 p.m.] Kenia Porcell Privado: A lo q me respetó\n",
            "[11/14/19 7:20:31 p.m.] Kenia Porcell Privado: Y todo se podría recibí\n",
            "[11/14/19 7:21:31 p.m.] Kenia Porcell Privado: En este momento q usted podría\n",
            "================================================================================\n",
            "Model prompt >>> Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/contextlib.py\", line 99, in __exit__\n",
            "    self.gen.throw(type, value, traceback)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py\", line 5480, in get_controller\n",
            "    yield g\n",
            "  File \"models/seguridad/interactive_conditional_samples.py\", line 71, in interact_model\n",
            "    raw_text = input(\"Model prompt >>> \")\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"models/seguridad/interactive_conditional_samples.py\", line 89, in <module>\n",
            "    fire.Fire(interact_model)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/fire/core.py\", line 138, in Fire\n",
            "    component_trace = _Fire(component, args, parsed_flag_args, context, name)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/fire/core.py\", line 471, in _Fire\n",
            "    target=component.__name__)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/fire/core.py\", line 675, in _CallAndUpdateTrace\n",
            "    component = fn(*varargs, **kwargs)\n",
            "  File \"models/seguridad/interactive_conditional_samples.py\", line 86, in interact_model\n",
            "    print(\"=\" * 80)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\", line 1633, in __exit__\n",
            "    close_thread.start()\n",
            "  File \"/usr/lib/python3.6/threading.py\", line 851, in start\n",
            "    self._started.wait()\n",
            "  File \"/usr/lib/python3.6/threading.py\", line 551, in wait\n",
            "    signaled = self._cond.wait(timeout)\n",
            "  File \"/usr/lib/python3.6/threading.py\", line 295, in wait\n",
            "    waiter.acquire()\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_7L1o3M8gU2",
        "colab_type": "code",
        "outputId": "be5d426d-fa39-499a-bceb-c8b5609f443e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python models/seguridad/interactive_conditional_samples.py --temperature 0.85 --top_k 50 --model_name seguridad"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/models/seguridad/model.py:147: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
            "\n",
            "WARNING:tensorflow:From models/seguridad/interactive_conditional_samples.py:55: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "2019-11-14 03:28:42.445904: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
            "2019-11-14 03:28:42.462098: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-11-14 03:28:42.462851: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-11-14 03:28:42.463119: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-11-14 03:28:42.464331: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
            "2019-11-14 03:28:42.465551: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\n",
            "2019-11-14 03:28:42.465903: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\n",
            "2019-11-14 03:28:42.467242: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2019-11-14 03:28:42.468255: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2019-11-14 03:28:42.471505: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-11-14 03:28:42.471646: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-11-14 03:28:42.472443: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-11-14 03:28:42.473155: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
            "2019-11-14 03:28:42.478288: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
            "2019-11-14 03:28:42.478528: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2de9480 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2019-11-14 03:28:42.478563: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2019-11-14 03:28:42.521845: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-11-14 03:28:42.522703: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2de9640 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2019-11-14 03:28:42.522739: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7\n",
            "2019-11-14 03:28:42.522948: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-11-14 03:28:42.523634: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-11-14 03:28:42.523699: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-11-14 03:28:42.523738: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
            "2019-11-14 03:28:42.523763: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\n",
            "2019-11-14 03:28:42.523788: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\n",
            "2019-11-14 03:28:42.523815: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2019-11-14 03:28:42.523850: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2019-11-14 03:28:42.523878: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-11-14 03:28:42.523971: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-11-14 03:28:42.524781: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-11-14 03:28:42.525474: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
            "2019-11-14 03:28:42.525550: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-11-14 03:28:42.527001: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-11-14 03:28:42.527035: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \n",
            "2019-11-14 03:28:42.527048: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \n",
            "2019-11-14 03:28:42.527199: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-11-14 03:28:42.528020: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-11-14 03:28:42.528694: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2019-11-14 03:28:42.528736: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10805 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
            "WARNING:tensorflow:From models/seguridad/interactive_conditional_samples.py:56: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From models/seguridad/interactive_conditional_samples.py:58: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/models/seguridad/model.py:148: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/models/seguridad/model.py:152: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/models/seguridad/model.py:36: The name tf.rsqrt is deprecated. Please use tf.math.rsqrt instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/models/seguridad/model.py:166: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/models/seguridad/sample.py:65: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "WARNING:tensorflow:From /content/models/seguridad/sample.py:16: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /content/models/seguridad/sample.py:70: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.random.categorical` instead.\n",
            "WARNING:tensorflow:From models/seguridad/interactive_conditional_samples.py:66: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "Model prompt >>> [06/21/18 5:31:12 p.m.] Jj: Ricardo Martinelli y yo somos buenos amigos\n",
            "2019-11-14 03:29:10.212764: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
            "======================================== SAMPLE 1 ========================================\n",
            "\n",
            "[06/21/18 5:31:21 p.m.] Jj: Yo me encargamos llegando a nivel en la maldad\n",
            "[06/21/18 5:31:38 p.m.] Jj: Ya estafa en la campaña\n",
            "[06/21/18 5:31:57 p.m.] Jj: Y la ley es una operación en la semana\n",
            "[06/21/18 5:32:39 p.m.] Kenia Porcell Privado: Ok\n",
            "[06/21/18 5:32:51 p.m.] Jj: Excelente\n",
            "[06/21/18 5:32:53 p.m.] Kenia Porcell Privado: Lo mandé x favor\n",
            "[06/21/18 5:33:12 p.m.] Jj: Buenas noches\n",
            "[06/21/18 5:33:32 p.m.] Jj: Y vea que yo estaba al pueblo\n",
            "[06/21/18 5:33:45 p.m.] Jj: Y el segiro que ha pasado\n",
            "[06/21/18 5:33:58 p.m.] Kenia Porcell Privado: Mientras ha sido un caso de cómo con los americanos, pero no ha solicitado el pueblo con las noticias no se llegaron a esa un buen logros\n",
            "[06/21/18 5:36:25 p.m.] Kenia Porcell Privado: Ok\n",
            "[06/21/18 5:44:31 p.m.] Jj: Excelente\n",
            "[06/21/18 5:45:37 p.m.] Jj: Xuzo\n",
            "[06/21/18 5:46:42 p.m.] Jj: Gracias por sus casos\n",
            "[06/21/18 5:47:16 p.m.] Kenia Porcell Privado: Si es un buen bueno\n",
            "[06/21/18 5:48:29 p.m.] Jj: Le dije a pelear la pgn\n",
            "[06/21/18 5:48:39 p.m.] Kenia Porcell Privado: Que llegamos a las intenciones\n",
            "[06/21/\n",
            "================================================================================\n",
            "Model prompt >>> Jj: Aló, Kenia, mañana vamos a la csj\n",
            "======================================== SAMPLE 1 ========================================\n",
            "\n",
            "[07/21/18 7:18:18 p.a.] Jj: Mire este\n",
            "[07/21/18 7:18:10 p.a.] Jj: La vamos a la csj le viendo a lo que me entrevistó\n",
            "[07/21/18 7:18:16 p.a.] Kenia Porcell Privado: Si. Me acaban de hacerlo de cuentas.\n",
            "[07/21/18 7:18:23 p.a.] Kenia Porcell Privado: Q bien\n",
            "[07/21/18 7:18:42 p.m.] Kenia Porcell Privado: A mi\n",
            "[07/21/18 7:19:19 p.m.] Kenia Porcell Privado: Si el respaldo de cuanto a las 19\n",
            "[07/21/18 7:19:55 p.m.] Kenia Porcell Privado: Y le dije\n",
            "[07/21/18 7:20:30 p.m.] Kenia Porcell Privado: Si es su sra mañana de vida a los q tienen un problema y le dije\n",
            "[07/21/18 7:21:06 p.m.] Jj: Me alegro y a todos los comisionados\n",
            "[07/21/18 7:21:17 p.m.] Jj: Yo estoy cualquier\n",
            "[07/21/18 7:21:14 p.m.] Jj: De acuerdo\n",
            "[07/21/18 7:21:16 p.m.] Jj: Yo estoy cuando\n",
            "[07/21/18 7:21:30 p.m.] Kenia Porcell Privado: Yo si. Desde a ser mismo como x eso yo aún\n",
            "[07/21/18 7:21:38 p.m.] Jj: Como le dije a usted\n",
            "[07/21/18 7:21:49 p.m.] Kenia Porcell Privado: Yo allí mejor xq los acuerdos\n",
            "[07/21/18 7:22:04 p.m.] Jj: Jajaajaj\n",
            "[07/21/18 7:22:11 p.m.] Jj: No\n",
            "[07/22/18\n",
            "================================================================================\n",
            "Model prompt >>> Kenia Porcell Privado: Hasta luego, que pase buenas noches, mañana será otro día en mi bello Panamá\n",
            "======================================== SAMPLE 1 ========================================\n",
            "\n",
            "[09/02/18 7:22:42 p.m.] Jj: Es lo que tenemos cambiar\n",
            "[09/02/18 7:22:49 p.m.] Kenia Porcell Privado: Los están con él\n",
            "[09/02/18 7:23:01 p.m.] Jj: Oiga\n",
            "[09/02/18 7:23:00 p.m.] Jj: Lo feliz\n",
            "[09/02/18 7:23:05 p.m.] Kenia Porcell Privado: Si\n",
            "[09/02/18 7:23:12 p.m.] Jj: Y ese pueblo vinculo\n",
            "[09/02/18 7:23:23 p.m.] Jj: No esta nuncio\n",
            "[09/02/18 7:23:44 p.m.] Kenia Porcell Privado: Así q estaría párrafa\n",
            "[09/02/18 7:23:54 p.m.] Kenia Porcell Privado: Si, es xq ahora saber la vida\n",
            "[09/02/18 7:24:27 p.m.] Kenia Porcell Privado: Lo bueno\n",
            "[09/02/18 7:24:59 p.m.] Jj: Me agosto hace cómo un golpe de minera\n",
            "[09/02/18 7:25:03 p.m.] Jj: Y si es el buso hablemal\n",
            "[09/02/18 7:25:18 p.m.] Jj: Y como si el dia puede razón\n",
            "[09/02/18 7:25:33 p.m.] Kenia Porcell Privado: Pero yo no creo q me queda cómo usted xq no es mi esfa\n",
            "[09/02/18 7:25:55 p.m.] Kenia Porcell Privado: Y no voy a aceptar q ese golpe para ganar el papelito\n",
            "[09/02/18 7:26:33 p.m.] Jj: Jajaajaj\n",
            "[09/02/18 7:26:41 p.m.] Jj: Eso en su casa usted\n",
            "[09/02/18 7\n",
            "================================================================================\n",
            "Model prompt >>> Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py\", line 5287, in get_controller\n",
            "    yield default\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py\", line 5480, in get_controller\n",
            "    yield g\n",
            "  File \"models/seguridad/interactive_conditional_samples.py\", line 71, in interact_model\n",
            "    raw_text = input(\"Model prompt >>> \")\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"models/seguridad/interactive_conditional_samples.py\", line 89, in <module>\n",
            "    fire.Fire(interact_model)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/fire/core.py\", line 138, in Fire\n",
            "    component_trace = _Fire(component, args, parsed_flag_args, context, name)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/fire/core.py\", line 471, in _Fire\n",
            "    target=component.__name__)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/fire/core.py\", line 675, in _CallAndUpdateTrace\n",
            "    component = fn(*varargs, **kwargs)\n",
            "  File \"models/seguridad/interactive_conditional_samples.py\", line 86, in interact_model\n",
            "    print(\"=\" * 80)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\", line 1621, in __exit__\n",
            "    self._default_graph_context_manager.__exit__(exec_type, exec_value, exec_tb)\n",
            "  File \"/usr/lib/python3.6/contextlib.py\", line 99, in __exit__\n",
            "    self.gen.throw(type, value, traceback)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py\", line 5480, in get_controller\n",
            "    yield g\n",
            "  File \"/usr/lib/python3.6/contextlib.py\", line 99, in __exit__\n",
            "    self.gen.throw(type, value, traceback)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py\", line 5287, in get_controller\n",
            "    yield default\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}